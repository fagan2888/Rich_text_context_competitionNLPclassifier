Original Research Article
What difference does quantity make?
On the epistemology of Big Data
in biology
S Leonelli
Abstract
Is Big Data science a whole new way of doing research? And what difference does data quantity make to knowledge
production strategies and their outputs? I argue that the novelty of Big Data science does not lie in the sheer quantity of
data involved, but rather in (1) the prominence and status acquired by data as commodity and recognised output, both
within and outside of the scientific community and (2) the methods, infrastructures, technologies, skills and knowledge
developed to handle data. These developments generate the impression that data-intensive research is a new mode of
doing science, with its own epistemology and norms. To assess this claim, one needs to consider the ways in which data
are actually disseminated and used to generate knowledge. Accordingly, this article reviews the development of sophis-
ticated ways to disseminate, integrate and re-use data acquired on model organisms over the last three decades of work
in experimental biology. I focus on online databases as prominent infrastructures set up to organise and interpret such
data and examine the wealth and diversity of expertise, resources and conceptual scaffolding that such databases draw
upon. This illuminates some of the conditions under which Big Data needs to be curated to support processes of
discovery across biological subfields, which in turn highlights the difficulties caused by the lack of adequate curation for
the vast majority of data in the life sciences. In closing, I reflect on the difference that data quantity is making to
contemporary biology, the methodological and epistemic challenges of identifying and analysing data given these devel-
opments, and the opportunities and worries associated with Big Data discourse and methods.
Keywords
Big Data epistemology, data-intensive science, biology, databases, data infrastructures, data curation, model organisms
Introduction
Big Data has become a central aspect of contemporary
science and policy, due to a variety of reasons that
include both techno-scientific factors and the political
and economic roles played by this terminology. The
idea that Big Data is ushering in a whole new way of
thinking, particularly within the sciences, is rampant ­
as exemplified by the emergence of dedicated funding,
policies and publication venues (such as this journal).
This is at once fascinating and perplexing to scholars
interested in the history, philosophy and social studies
of science. On the one hand, there seems to be some-
thing interesting and novel happening as a consequence
of Big Data techniques and communication strategies,
which is, however, hard to capture with traditional
notions, such as `induction' and `data-driven' science
(partly because, as philosophers of science have long
shown, there is no such thing as direct inference from
data, and data interpretation typically involves the use
of modelling techniques and various other kinds of con-
ceptual and material scaffolding).1 On the other hand,
many sciences have a long history of dealing with large
quantities of data, whose size and scale vastly outstrip
available strategies and technologies for data collection,
dissemination and analysis (Gitelman, 2013). This is
University of Exeter, UK
Corresponding author:
S Leonelli, University of Exeter, Byrne House, St Germans Road, Exeter
EX4 4PJ, UK.
Email: S.Leonelli@exeter.ac.uk
Big Data & Society
April­June 2014: 1­11
! The Author(s) 2014
DOI: 10.1177/2053951714534395
bds.sagepub.com
Creative Commons CC-BY: This article is distributed under the terms of the Creative Commons Attribution 3.0 License (http://
www.creativecommons.org/licenses/by/3.0/) which permits any use, reproduction and distribution of the work without further
permission provided the original work is attributed as specified on the SAGE and Open Access pages (http://www.uk.sagepub.com/aboutus/open-
access.htm).
particularly evident in the life sciences, where data-
gathering practices in subfields, such as natural history
and taxonomy, have been at the heart of inquiry since
the early modern era, and have generated problems
ever since (e.g. Johnson, 2012; Mu
¨ ller-Wille and
Charmantier, 2012).
So what is actually new here? How does Big Data
science differ from other forms of inquiry, what can and
cannot be learnt from Big Data, and what difference
does quantity make? In this article, I discuss some of
the central characteristics typically associated with Big
Data, as conveniently summarised within the recent
book Big Data by Mayer-Scho
¨ nberger and Cukier
(2013), and I scrutinise their plausibility in the case of
biological research. I then argue that the novelty of Big
Data science does not lie in the sheer quantity of data
involved, though this certainly makes a difference to
research methods and results. Rather, the novelty of
Big Data science lies in (1) the prominence and status
acquired by data as scientific commodity and recog-
nised output both within and beyond the sciences and
(2) the methods, infrastructures, technologies and skills
developed to handle (format, disseminate, retrieve,
model and interpret) data. These developments gener-
ate the impression that data-intensive research is a
whole new mode of doing science, with its own epis-
temology and norms. I here defend the idea that in
order to understand and critically evaluate this
claim, one needs to analyse the ways in which data
are actually disseminated and used to generate know-
ledge, which I refer to as `data journeys'; and I consider
the extent to which the current handling of Big Data
fosters and validates its use as evidence towards new
discoveries.2
Accordingly, the bulk of this article reviews the
development of sophisticated ways to disseminate, inte-
grate and re-use data acquired on model organisms,
such as the small plant Arabidopsis thaliana, the nema-
tode Caenorhabditis elegans and the fruit-fly Drosophila
melanogaster (including data on their ecology, metab-
olism, morphology and relations to other species) over
the last three decades of work in experimental biology.
I focus on online databases as a key example of infra-
structures set up to organise and interpret such data;
and on the wealth and diversity of expertise, resources
and conceptual scaffolding that such databases draw
upon in order to function well. This analysis of data
journeys through model organism databases illumin-
ates some of the conditions under which the evidential
value of data posted online can be assessed and inter-
preted by researchers wishing to use those data to foster
discovery. At the same time, model organism biology
has been one of the best funded scientific areas over the
last three decades, and the curation of data produced
therein has benefited from much more attention and
dedicated investments than data generated in the rest
of the life sciences and biomedicine. Considering the
challenges encountered in disseminating this type of
data thus also highlights the potential problems
involved in assembling data that have not received
comparable levels of care (i.e. the vast majority of bio-
logical data).
In my conclusions, I use these findings to inform a
critique of the supposed revolutionary power of Big
Data science. In its stead, I propose a less sensational,
but arguably more realistic, reflection on the difference
that data quantity is making to contemporary bio-
logical research, which stresses both continuities with
and dissimilarities from previous attempts to handle
large datasets. I also suggest that the natural sciences
may well be the area that is least affected by Big Data,
whose emergence is much more likely to affect the pol-
itical and economic realms ­ though not necessarily for
the better.
The novelty of Big Data
I will start by considering three ideas that, according to
Mayer-Scho
¨ nberger and Cukier (2013) among others,
constitute core innovations brought in by the advent of
Big Data in all realms of human activity, including sci-
ence. The first idea is what I shall label comprehensive-
ness. This is the claim that the accumulation of large
datasets enables scientists to ground their analysis on
several different aspects of the same phenomenon,
documented by different people at different times.
According to Mayer-Scho
¨ nberger and Cukier, data
can become so big as to encompass all the available
data on a phenomenon of interest. As a consequence,
Big Data can provide a comprehensive perspective on
the characteristics of that phenomenon, without need-
ing to focus on specific details.
The second idea is that of messiness. Big Data, it is
argued, pushes researchers to embrace the complex and
multifaceted nature of the real world, rather than pur-
suing exactitude and accuracy in measurement obtained
under controlled conditions. Indeed, it is impossible to
assemble Big Data in ways that are guaranteed to be
accurate and homogeneous. Rather, we should resign
ourselves to the fact that `Big Data is messy, varies in
quality, and is distributed across countless servers
around the world' (Mayer-Scho
¨ nberger and Cukier,
2013: 13) and welcome the advantages of this lack of
exactitude: `With Big Data, we'll often be satisfied with
a sense of general direction rather than knowing a phe-
nomenon down to the inch, the penny, the atom'
(Mayer-Scho
¨ nberger and Cukier, 2013).3
The idea of messiness relates closely to the third
key innovation brought about by Big Data, which
Mayer-Scho
¨ nberger and Cukier call the `triumph of
2 Big Data & Society
correlations'. Correlations, defined as the statistical
relationship between two data values, are notoriously
useful as heuristic devices within the sciences. Spotting
the fact that when one of the data values changes the
other is likely to change too is the starting point for
many discoveries. However, scientists have typically
mistrusted correlations as a source of reliable know-
ledge in and of themselves, chiefly because they may
be spurious ­ either because they result from serendip-
ity rather than specific mechanisms or because they are
due to external factors. Big Data can override those
worries. Mayer-Scho
¨ nberger and Cukier (2013: 52)
give the example of Amazon.com, whose astonishing
expansion over the last few years is at least partly due
to their clever use of statistical correlations among the
myriad of data provided by their consumer base in
order to spot users' preferences and successfully suggest
new items for consumption. In cases such as this, cor-
relations do indeed provide powerful knowledge that
was not available before. Hence, Big Data encourages
a growing respect for correlation, which comes to be
appreciated as not only a more informative and plau-
sible form of knowledge than the more definite but also
a more elusive, causal explanation. In the words of
Mayer-Scho
¨ nberger and Cukier (2013: 14): `the correla-
tions may not tell us precisely why something is hap-
pening, but they alert us that it is happening. And in
many situations this is good enough'.
These three ideas have two important corollaries,
which shall constitute the main target of my analysis
in this article. The first corollary is that Big Data makes
reliance on small sampling, and even debates over sam-
pling, unnecessary. This again seems to make sense
prima facie: if we have all the data about a given phe-
nomenon, what is the point of pondering which types of
data might best document it? Rather, one can now skip
that step and focus instead on assembling and analysing
as much data as possible about the phenomenon of
interest, so as to generate reliable knowledge about it:
`Big Data gives us an especially clear view of the granu-
lar; subcategories and submarkets that samples can't
assess' (Mayer-Scho
¨ nberger and Cukier, 2013: 13).
The second corollary is that Big Data is viewed,
through its mere existence, as countering the risk of
bias in data collection and interpretation. This is
because having access to large datasets makes it more
likely that bias and error will be automatically elimi-
nated from the system, for instance via what sociolo-
gists and philosophers call `triangulation': the tendency
of reliable data to cluster together, so that the more
data one has, the easier it becomes to cross-check
them with each other and eliminate the data that look
like outliers (Denzin, 2006; Wylie, 2002).
Over the next few sections, I show how an empirical
study of how Big Data biology operates puts both of
these corollaries into question, which in turn comprom-
ises the plausibility of the three claims that Mayer-
Scho
¨ nberger and Cukier make about the power of
Big Data ­ at least when they are applied to the
realm of scientific inquiry. Let me immediately state
that I do not intend this analysis to deny the wide-
spread attraction that these three ideas are generating
in many spheres of contemporary society (most obvi-
ously, big government) and which is undoubtedly mir-
rored in the ways in which biological research has been
re-organised since at least the early 2000s (which is
when technologies for the high-throughput production
of genomic data, such as sequencing machines, started
to become widely used). Rather, I wish to shed some
clarity on the gulf that separates the hyperbolic claims
made about the novelty of Big Data science from the
challenges, problems and achievements characterising
data-handling practices in the everyday working life
of biologists ­ and particularly the ways in which new
computational and communication technologies such
as online databases are being developed so as to trans-
form these ideas into reality.
Big Data journeys in biology
For scientists to be able to analyse Big Data, those data
have to be collected and assembled in ways that make it
suitable to consider them as a single body of informa-
tion (O'Malley and Soyer, 2012). This is a particularly
difficult task in the case of biological data, given the
highly fragmented and pluralist history of the field.
For a start, there are myriads of epistemic communities
within the life sciences, each of which uses a different
combination of methods, locations, materials, back-
ground knowledge and interest to produce data.
Furthermore, there are vast differences in the types of
data that can be produced and the phenomena that can
be targeted. And last but not least, the organisms and
ecosystems on which data are being produced are both
highly variable and highly unstable, given their con-
stant exposure to both developmental and evolutionary
change. Given this situation, a crucial question within
Big Data science concerns how one can bring such dif-
ferent data types, coming from a variety of sources,
under the same umbrella.
To address this question, my research over the last
eight years has focused on documenting and analysing
the ways in which biological data ­ and particularly
`omics' data, the quintessential form of `Big Data' in
the life sciences ­ travel across research contexts, and
the significant conceptual and material scaffolding used
by researchers to achieve this. For the purposes of this
article, I shall now focus on one case of Big Data hand-
ling in biology, which is arguably among the most
sophisticated and successful attempts made to integrate
Leonelli 3
vast quantities of data of different types within this
field for the purposes of advancing future knowledge
production. This is the development of model organ-
ism databases between 2000 and 2010.4 These data-
bases were built with the immediate goal of storing
and disseminating genomic data in a formalised
manner, and the long-term vision of (1) incorporating
and integrating any data available on the biology of
the organism in question within a single resource,
including data on physiology, metabolism and even
morphology; (2) allowing and promoting cooperation
with other community databases so that the available
datasets would eventually be comparable across spe-
cies; and (3) gathering information about laboratories
working on each organism and the associated experi-
mental protocols, materials and instruments, thus pro-
viding a platform for community building. Particularly
useful and rich examples include FlyBase, dedicated to
D. melanogaster; WormBase, focused on C. elegans;
and The Arabidopsis Information Resource, gathering
data on A. thaliana. At the turn of the 21st century,
these were arguably among most sophisticated com-
munity databases within biology. They have played
a particularly significant role in the development
of online data infrastructures in this area and continue
to serve as reference points for the construction
of other databases to this day (Leonelli and
Ankeny, 2012). They therefore represent a good
instance of infrastructure explicitly set up to support
and promote Big Data research in experimental
biology.
In order to analyse how these databases enable data
journeys, I will distinguish between three stages of data
travel, and briefly describe the extent to which database
curators are involved in their realisation.
Stage 1: De-contextualisation
One of the main tasks of database curators is to de-
contextualise the data that are included in their
resources, so that they can travel outside of their ori-
ginal production context and become available for inte-
gration with other datasets (thus forming a Big Data
collection). The process of de-contextualisation
involves making sure that data are formatted in ways
that make them compatible with datasets coming from
other sources, so that they are easy to analyse by
researchers who see them for the first time. Given the
above-mentioned fragmentation and diversity of data
production processes to be found within biology, there
tends to be no agreement on formatting standards for
even the most common of data types (such as metabo-
lomics data, for instance; Leonelli et al., 2013). As a
result, database curators often need to assess how to
deal with specific datasets on a one-to-one basis.
Despite constant advances, it is still impossible to auto-
mate the de-contextualisation of most types of bio-
logical data.
Formatting data to ensure that they can all be ana-
lysed as a unique body of evidence is thus exceedingly
labour-intensive, and requires the development of
databases with long-term funding and enough person-
nel to make sure that data submission and formatting
is carried out adequately. Setting up such resources is
an expensive business. Indeed, debate keeps raging
among funding agencies about who is responsible for
maintaining these infrastructures. Many model organ-
ism databases have struggled to attract enough fund-
ing to support their de-contextualisation activities.
Hence, they have resorted to include only data that
had been already published in a scientific journal ­
thus vastly restricting the amount of data hosted by
the database ­ or that were donated by data producers
in a format compatible to the ones supported by the
database (Bastow and Leonelli, 2010). Despite the
increasing pressure to disseminate data in the public
domain, as recently recommended by the Royal
Society (2012) and several funding bodies in the UK
(Levin et al., in preparation), the latter category com-
prises a very small number of researchers. Again, this
is largely due to the labour-intensive nature of de-con-
textualisation processes. Researchers who wish to
submit their data to a database need to make sure
that the format that they use, and the metadata that
they provide, fit existing standards ­ which in turn
means acquiring updated knowledge on what the
standards are and how they can be implemented, if
at all; and taking time out of experiments and grant-
writing. There are presently very few incentives for
researchers to sacrifice research time in this way,
as data donation is not acknowledged as a contribu-
tion to scientific research (Ankeny and Leonelli,
in press).
Stage 2: Re-contextualisation
Once data have been de-contextualised and added to a
database, the next stage of their journey is to be re-
contextualised ­ in other words, to be adopted by a
new research context, in which they can be integrated
with other data and possibly contribute to spotting new
correlations. Within biology, re-contextualisation can
only happen if database users have access not only to
the data themselves but also to the information about
their provenance ­ typically including the specific strain
of organisms on which they were collected, the instru-
ments and procedures used for data collection, and the
composition of the research team who originated them
in the first place. This sort of information, typically
referred to as `metadata' (Edwards et al., 2011;
4 Big Data & Society
Leonelli, 2010), is indispensable to researchers wishing
to evaluate the reliability and quality of data. Even
more importantly, it makes the interpretation of the
scientific significance of the data possible, thus enabling
researchers to extract meaning from their scrutiny of
databases.
Given the challenges already linked to the de-con-
textualisation of data, it will come as no surprise that
re-contextualising them is proving even harder in bio-
logical practice. The selection and annotation of meta-
data is more labour-intensive than the formatting of
data themselves, and involves the establishment of sev-
eral types of standards, each of which is managed by its
own network of funding and institutions. For a start, it
presupposes reliable reference to material specimens of
the model organisms in question. In other words, it is
important to standardise the materials on which data
are produced as much as possible, so that researchers
working on those data in different locations can order
those materials and reasonably assume that they are
indeed the same materials as those from which data
were originally extracted. Within model organism biol-
ogy, the standardisation, coordination and dissemin-
ation of specimens is in the hands of appositely built
stock centres, which collect as many strains of organ-
isms as possible, pair them up with datasets stored in
databases, and make them available for order to
researchers interested in the data. In the best cases,
this happens through the mediation of databases them-
selves; for instance, The Arabidopsis Research
Database has long incorporated the option to order
materials associated with data stored therein at the
same time as one is viewing the data (Rosenthal and
Ashburner, 2002). However, such a well-organised
coordination between databases and stock centres is
rare, particularly in cases where the specimens to be
collected and ordered are not easily transportable
items, such as seeds and worms, but organisms that
are difficult and expensive to keep and disseminate,
such as viruses and mice. Most organisms used for
experimental research do not even have a centralised
stock centre collecting exemplars for further dissemin-
ation. As a result, the data generated from these organ-
isms are hard to incorporate into databases, as
providing them with adequate metadata proves impos-
sible (Leonelli, 2012a).
Another serious challenge to the development of
metadata consists of capturing experimental protocols
and procedures, which in biology are notoriously idio-
syncratic and difficult to capture through any kind of
textual description (let alone standard categories). The
difficulties are exemplified by the recent emergence of a
Journal of Visualized Experiments, whose editors claim
that actually showing a video of how a specific experi-
ment is performed is the only way to credibly
communicate information about research methods
and protocols. Indeed, despite the attempted implemen-
tation of standard descriptions such as the Minimal
Information about Biological and Biomedical
Investigation, standards in this area are very under-
developed and rarely used by biologists (Leonelli,
2012a). This makes the job of curators even more dif-
ficult, as they are then left with the task of selecting
which metadata to insert in their database, and which
format to use in order to provide such information.
Additionally, curators are often asked to provide a pre-
liminary assessment of the quality of data, which can
act as a guideline for researchers interested in large
datasets. Curators achieve this through so-called `evi-
dence codes' and `confidence rankings' which, however,
tend to be based on controversial assumptions (for
instance, the idea that data obtained through physical
interaction with organisms are more trustworthy than
simulation results) which may not fit all scenarios in
which data may be adopted.
Stage 3: Re-use
The final stage of data journeys that I wish to examine
is that of re-use. One of the central themes in Big
Data research is the opportunity to re-use the same
datasets to uncover a large number of different correl-
ations. After having been de-contextualised and re-
contextualised, data are therefore supposed to fulfil
their epistemic role by leading to a variety of new
discoveries. From my observations above, it will
already be clear that very few of the data produced
within experimental biology make it to this stage of
their journeys, due to the lack of standardisation in
their format and production techniques, as well as the
absence of stable reference materials to which data can
be meaningfully associated for re-contextualisation.
Data that cannot be de-contextualised and re-
contextualised are not generally included into model
organism databases, and thus do not become part of a
body of Big Data from which biologically significant
inferences can be made. Remarkably, the data that are
most successfully assembled into big collections are
genomic data, such as genome sequences and micro-
arrays, which are produced through highly standar-
dised technologies and are therefore easier to format
for travel. This is bad news for biological research
focused on understanding higher-level processes, such
as organismal development, behaviour and susceptibil-
ity to environmental factors: data that document these
aspects are typically the least standardised in both
their format and the materials and instruments
through which they are produced, which makes their
integration into large collections into a serious
challenge.
Leonelli 5
This signals a problem with the idea that Big Data
involves unproblematic access to all data about a given
phenomenon ­ or even to at least some data about
several aspects of a phenomenon, such as multiple
data sources concerning different levels of organisation
of an organism. When considering the stage of data
re-use, however, an even more significant challenge
emerges: that of data classification. Whenever data
and metadata are added to a database, curators need
to tag them with keywords that will make them retriev-
able to biologists interested in related phenomena. This
is an extremely hard task, given that curators want to
leave the interpretation of the potential evidential value
of data as open as possible to database users. Ideally,
curators should label data according to the interests
and terminology used by their prospective users, so
that a biologist is able to search for any data connected
to her phenomenon of interest (e.g. `metabolism') and
find what the evidence that she is looking for is. What
makes such a labelling process into a complex and con-
tentious endeavour is the recognition that this classifi-
cation partly determines the ways in which data may be
used in the future ­ which, paradoxically, is exactly
what databases are not supposed to do. In other pub-
lications, I have described at length the functioning of
the most popular system currently used to classify data
in model organism databases, the so-called `bio-ontol-
ogies' (Leonelli, 2012b). Bio-ontologies are standard
vocabularies intended to be intelligible and usable
across all the model organism communities, sub-disci-
plines and cultural locations to which data should
travel in order to be re-used. Given the above-men-
tioned fragmentation of biology into myriads of epi-
stemic communities with their own terminologies,
interests and beliefs, this is a tall order. Consequently,
despite the widespread recognition that model organ-
ism databases are among the best sources of Big Data
within biology, many biologists are suspicious of them,
principally as a result of their mistrust of the categories
under which data are classified and distributed. This
puts into question not only the idea that databases
can successfully collect Big Data on all aspects of
given organisms but also the idea that they succeed in
making such data retrievable to researchers in ways
that foster their re-use towards making new discoveries.
What does it take to assemble Big Data?
Implications for Big Data claims
The above analysis, however brief, clearly points to the
huge amount of manual labour involved in developing
databases for the purpose of assembling Big Data and
making it possible to integrate and analyse them; and to
the many unresolved challenges and failures plaguing
that process.
I have shown how curators have a strong influence
on all three stages of data journeys via model organism
databases. They are tasked with selecting, formatting
and classifying data so as to mediate among the mul-
tiple standards and needs of the disparate epistemic
communities involved in biological research. They
also play a key role in devising and adding metadata,
including information about experimental protocols
and relevant materials, without which it would be
impossible for database users to gauge the reliability
and significance of the data therein. All these activities
require large amounts of funding for manual curation,
which is mostly unavailable even in areas as successful
as model organism biology. They also require the sup-
port and co-operation of the broader biological com-
munity, which is however also rare due to the pressures
and credit systems to which experimental biologists are
subjected. Activities such as data donation and partici-
pation in data curation are not currently rewarded
within the academic system. Therefore, many scientists
who run large laboratories and are responsible for their
scientific success perceive these activities as an inexcus-
able waste of time, despite being aware of their scien-
tific importance in fostering Big Data science.
We thus are confronted with a situation in which
(1) there is still a large gap between the opportunities
offered by cutting-edge technologies for data dissemin-
ation and the realities of biological data production and
re-use; (2) adequate funding to support and develop
online databases is lacking, which greatly limits cur-
ators' ability to make data travel; and (3) data donation
and incorporation into databases is very limited, which
means that only a very small part of the data produced
within biology actually get to be assembled into Big
Data collections. Hence, Big Data collections in biol-
ogy could be viewed as very small indeed, compared to
the quantity and variety of data actually produced
within this area of research. Even more problematic-
ally, such data collections tend to be extremely partial
in the data that they include and make visible. Despite
curators' best efforts, model organism databases mostly
display the outputs of rich, English-speaking labs
within visible and highly reputed research traditions,
which deal with `tractable' data formats. The incorpo-
ration of data produced by poor or unfashionable labs,
whether in developed or developing countries, is very
low ­ also because scientists working in those condi-
tions have an even lesser chance than scientists working
in prestigious locations to be able to contribute to the
development of databases in the first place (the digital
divide is alive and well in Big Data science, though
taking on a new form).
A possible moral to be drawn from this situation is
that what counts as data in the first place should be
defined by the nature of their journeys. According to
6 Big Data & Society
this view, data are whatever can be fitted into highly
visible databases; and results that are hard to dissem-
inate in this way do not count as data at all, since they
are not widely accessible. I regard this view as empiric-
ally unwarranted, as it is clear from my research that
there are many more results produced within the life
sciences which biologists are happy to call and use as
data; and that what biologists consider to be data does
depend on their availability for scrutiny (it has to be
possible to circulate them to at least some peers who
can assess their usefulness as evidence), but not neces-
sarily on the extent to which they are publicly available
­ in other words, data disseminated through paper or
by email can have as much weight as data disseminated
through online databases. Despite these obvious prob-
lems, however, the increasing prominence of databases
as supposedly comprehensive sources of information
may well lead some scientists to use them as bench-
marks for what counts as data in a specific area of
investigation. This tendency is reinforced by wider pol-
itical and economic forces, such as governments, cor-
porations and funding bodies, for whom the prospect
of assembling centralised repositories for all available
evidence on any given topics constitutes a powerful
draw (Leonelli, 2013).
How do these findings compare to the claims made
by Mayer-Scho
¨ nberger and Cukier? For a start, I think
that they cause problems to both of the corollaries to
their views that I listed above. Consider first the ques-
tion of sampling. Rather than disappearing as a scien-
tific concern, looking at the ways in which data travel in
biology highlights the ever-growing significance of sam-
pling methods. Big Data that is made available through
databases for future analysis turns out to represent
highly selected phenomena, materials and contribu-
tions, to the exclusion of the majority of biological
work. What is worse, this selection is not the result of
scientific choices, which can therefore be taken into
account when analysing the data. Rather, it is the ser-
endipitous result of social, political, economic and tech-
nical factors, which determines which data get to travel
in ways that are non-transparent and hard to recon-
struct by biologists at the receiving end. A full account
of factors involved here far transcends the scope of this
article.5 Still, even my brief analysis of data journeys
illustrates how they depend on issues as diverse as
national data donation policies (including privacy
laws, in the case of biomedical data); the good-will
and resources of specific data producers, as well as
the ethos and visibility of the scientific traditions and
environments in which they work (for instance, biolo-
gists working for private industries may not be allowed
to publicly disclose their data); and the availability of
well-curated databases, which in turn depends on the
visibility and value placed upon them (and the data
types therein) by government or relevant public/private
funders. Assuming that Big Data does away with the
need to consider sampling is highly problematic in such
a situation. Unless the scientific system finds a way to
improve the inclusivity of biological databases, they
will continue to incorporate partial datasets that never-
theless play a significant role in shaping future research,
thus encouraging an inherently conservative and irra-
tional system.
This partiality also speaks to the issue of bias in
research, which Mayer-Scho
¨ nberger and Cukier insist
can potentially be superseded in the case of Big Data
science. The ways in which Big Data is assembled for
further analysis clearly introduce numerous biases
related to methods for data collection, storage, dissem-
ination and visualisation. This feature is recognised by
Mayer-Scho
¨ nberger and Cukier, who indeed point to
the fact that the scale of such data collection takes focus
away from the singularity of data points: the ways in
which datasets are arranged, selected, visualised and
analysed become crucial to which trends and patterns
emerge. However, they assume that the diversity and
variability of data thus collected will be enough to
counter the bias incorporated in each of these sources.
In other words, Big Data is self-correcting by virtue of
its very unevenness, which makes it probable that
incorrect or inaccurate data are rooted out of the
system because of their incongruence with other data
sources. I think that my arguments about the inherent
imbalances in the types and sources of data assembled
within big biology casts some doubt as to whether such
data collections, no matter how large, are diverse
enough to counter bias in their sources. If all data
sources share more or less the same biases (for instance,
they all rely on microarrays produced with the same
machines), there is also the chance that bias will be
amplified, rather than reduced, through such Big Data.
These considerations do not make Mayer-
Scho
¨ nberger and Cukier's claims about the power of
Big Data completely implausible, but they certainly
dent the idea that Big Data is revolutionising biological
research. The availability of large datasets does of
course make a difference, as advertised for instance in
the Fourth Paradigm volume issued by Microsoft to
promote the power of data-intensive strategies (Hey
et al., 2009). And yet, as I stressed above, having a
lot of data is not the same as having all of them; and
cultivating such an illusion of completeness is a very
risky and potentially misleading strategy within biology
­ as most researchers whom I have interviewed over the
last few years pointed out to me. The idea that the
advent of Big Data lessens the value of accurate mea-
surements also does not seem to fit these findings. Most
sciences work at a level of sophistication in which one
small error can have very serious consequences (the
Leonelli 7
blatant example being engineering). The constant
worry about the accuracy and reliability of data is
reflected in the care employed by database curators in
enabling database users to assess such properties; and
in the importance given by users themselves to evaluat-
ing the quality of data found on the internet. Indeed,
databases are often valued because they provide means
to triangulate findings coming from different sources,
so as to improve the accuracy of measurement and
determine which data are most reliable. Although
they may often fail to do so, as I just discussed, the
very fact that this is a valued feature of databases
makes the claim that `messiness' triumphs over
accuracy look rather shaky. Finally, considering data
journeys prompts second thoughts about the supposed
primacy of correlations over causal explanations. Big
Data certainly does enable scientists to spot patterns
and trends in new ways, which in turn constitutes an
enormous boost to research. At the same time, biolo-
gists are rarely happy with such correlations, and
instead use them as heuristics that shape the direction
of research without necessarily constituting a discovery
in itself. Being able to predict how an organism or eco-
system may behave is of huge importance, particularly
within fields such as biomedicine or environmental sci-
ence; and yet, within experimental biology, the ability
to explain why certain behaviour obtains is still very
highly valued ­ arguably over and above the ability
to relate two traits to each other.6
Conclusion: An alternative approach
to Big Data science
In closing my discussion, I not only want to consider
its specificity with respect to other parts of Big Data
science but also the general lessons that may be drawn
from such a case study. Biology, and particularly the
study of model organisms, represents a field where
data have been produced long before the advent of
computing and many data types are still generated
in ways that are not digital, but rather rely on physical
and localised interactions between one or more inves-
tigators and a given organic sample. Accordingly, bio-
logical data on model organisms are heterogeneous
both in their content and in their format; are curated
and re-purposed to address the needs of highly dispar-
ate and fragmented epistemic communities; and pre-
sent curators with specific challenges to do with the
wish to faithfully capture and represent complex,
diverse and evolving organismal structures and behav-
iours. Readers with experience in other forms of Big
Data may well be dealing with cases where both data
and their prospective users are much more homoge-
neous, which means that their travel is less contested
and tends to be curated and institutionalised in com-
pletely different ways. I view the fact that my study
bears no obvious similarities to other areas of Big
Data use as a strength of my approach, which
indeed constitutes an invitation to disaggregate the
notion of Big Data science as a homogeneous whole
and instead pay attention to its specific manifestations
across different contexts. At the same time, I maintain
that a close examination of specialised areas can still
yield general lessons, at the very least by drawing
attention to aspects that need to be critically scruti-
nised in all instances of Big Data handling. These
include, for instance, the extent to which data are ­
and need to be ­ curated before being assembled into
common repositories; the decisions and investments
involved in selecting data for travel, and their impli-
cations for which data get to be circulated in the first
place; and the representativeness of data assembled
under the heading of `Big Data' with respect to
other (and/or pre-existing) data collection activities
within the same field.
At the most general level, my analysis can be used
to argue that characterisations of Big Data science as
comprehensive and intrinsically unbiased can be mis-
leading rather than helpful in shaping scientific as
well as public perceptions of the features, opportu-
nities and dangers associated with data-intensive
research. If one admits the plausibility of this pos-
ition, then how can one better understand current
developments? I here want to defend the idea that
Big Data science has specific epistemological and
methodological characteristics, and yet that it does
not constitute a new epistemology for biology. Its
strength lies in the combination of concerns that
have long featured in biological research with oppor-
tunities opened up by novel communication technol-
ogies, as well as the political and economic climate in
which scientific research is currently embedded. Big
Data brings new salience to aspects of scientific prac-
tice which have always been vital to successful empir-
ical research, and yet have often been overlooked by
policy-makers, funders, publishers, philosophers of
science and even scientists themselves, who in the
past have tended to evaluate what counts as `good
science' in terms of its products (e.g. new claims
about phenomena or technologies for intervention
in the world) rather than in terms of the processes
through which such results are eventually achieved.
These aspects include the processes involved in valu-
ing data as a key scientific resource; situating data in
a context within which they can be interpreted reli-
ably; and structuring scientific institutions and credit
mechanisms so that data dissemination is supported
and regulated in ways that are conducive to the
advancement of both science and society.
8 Big Data & Society
More specifically, I want to argue that the novelty of
Big Data science can be located in two key shifts char-
acterising scientific practices over the last two decades.
First is the new prominence attributed to data as com-
modities with high scientific, economic, political and
social value (Leonelli, 2013). This has resulted in the
acknowledgement of data as key scientific components,
outputs in their own right that need to be widely dis-
seminated (for instance, through so-called `data jour-
nals' or repositories such as Figshare or more
specialised databases) ­ which in turn is engendering
significant shifts in the ways in which research is orga-
nised and assessed both within and beyond scientific
institutions. Second is the emergence of a new set of
methods, infrastructures and skills to handle (format,
disseminate, retrieve, model and interpret) data.
Hilgartner (1995) spoke about the introduction of com-
puting and internet technologies in biology as a change
of communication regime. Indeed, my analysis has
emphasised how the introduction of tools such as data-
bases, and the related opportunity to make data
instantly available over the internet, is challenging the
ways in which data are produced and disseminated, as
well as the types of expertise relevant to analysing such
data (which now needs to include computing and cura-
torial skills, in addition to more traditional statistical
and modelling abilities).
When seen through this lens, data quantity can
indeed be said to make a difference to biology, but in
ways that are not as revolutionary as many Big Data
advocates would advocate. There is strong continuity
with practices of large data collection and assemblage
conducted since the early modern period; and the core
methods and epistemic problems of biological research,
including exploratory experimentation, sampling and
the search for causal mechanisms, remain crucial
parts of inquiry in this area of science ­ particularly
given the challenges encountered in developing and
applying curatorial standards for data other than the
high-throughput results of `omics' approaches.
Nevertheless, the novel recognition of the relevance of
data as a research output, and the use of technologies
that greatly facilitate their dissemination and re-use,
provide an opportunity for all areas in biology to rein-
vent the exchange of scientific results and create new
forms of inference and collaboration.
I end this article by suggesting a provocative
explanation for what I argued is a non-revolutionary
role of Big Data in biology. It seems to me that my
scepticism arises because of my choice of domain,
which is much narrower than Mayer-Scho
¨ nberger
and Cukier's commentary on the impacts of Big
Data on society as a whole. Indeed, biological
research may be the domain of human activity that
is least affected by the emergence of Big Data and
related technologies today. This is precisely because,
like many other natural sciences, such as astronomy,
climatology and geology, biology has a long history of
engaging with large datasets; and because deepening
our current understanding of the world continues to
be one of the key goals of inquiry in all areas of sci-
entific investigation. While often striving to take
advantage of any available tool for the investigation
of the world and produce findings of use to society,
biologists are not typically content with establishing
correlations. The quest for causal explanations, often
involving detailed descriptions of the mechanisms and
laws at play in any given situation, is not likely to lose
its appeal any time soon. Whether or not it is plaus-
ible in its implementation, the Big Data epistemology
outlined by Mayer-Scho
¨ nberger and Cukier is thus
unlikely to prove attractive to biologists, for whom
correlations are typically but a starting point to a sci-
entific investigation; and the same argument may well
apply to other areas of the natural sciences.7 The real
revolution seems more likely to centre on other areas
of social life, particularly economics and politics,
where the widespread use of patterns extracted from
large datasets as evidence for decision-making is a
relatively recent phenomenon. It is no coincidence
that most of the examples given by Mayer-
Scho
¨ nberger and Cukier come from the industrial
world, and particularly globalised sales strategies, as
in the case of Amazon.com. Big Data provides new
opportunities for managing goods and resources,
which may be exploited to reflect and engage individ-
ual preferences and desires. By the same token, Big
Data also provides as yet unexplored opportunities
for manipulating and controlling individuals and com-
munities on a large scale ­ a process that Raley (2013)
characterised as `dataveillence'. As demonstrated by
the history of quantification techniques as surveillance
and monitoring tools (Porter, 1995), data have long
functioned as a way to quantify one's actions and
monitor others. `Bigness' in data production, availabil-
ity and use thus needs to be contextualised and ques-
tioned as a political-economic phenomenon as much
as a technical one (Davies et al., 2013).
Acknowledgements
I am grateful to the `Sciences of the Archive' Project at the
Max Planck Institute for the History of Science in Berlin,
whose generous hospitality and lively intellectual atmosphere
in 2014 enabled me to complete this manuscript; and to Brian
Rappert for helpful comments on the manuscript. This
research was funded by the UK Economic and Social
Research Council (ESRC) through the ESRC Centre for
Genomics and Society and grant number ES/F028180/1; the
Leverhulme Trust through grant award RPG-2013-153; and
the European Research Council under the European Union's
Leonelli 9
Seventh Framework Programme (FP7/2007-2013)/ERC grant
agreement number 335925.
Notes
1. For a review of this literature, which includes seminal con-
tributions such as Hacking (1992) and Rheinberger (2011),
see Bogen (2010).
2. This idea, though articulated in a variety of different ways,
broadly underscores also the work of Sharon Traweek
(1998), Geoffrey C. Bowker (2001), Christine Borgman
(2007), Karen Baker and Franc
¸ ois Millerand (2010) and
Paul Edwards (2011).
3. Incidentally, the idea of comprehensiveness may be inter-
preted as clashing with the idea of messiness when formu-
lated in this way. If we can have all the data on a specific
phenomenon, then surely we can focus on understanding it
to a high level of precision, if we so wish. I shall return to
this point below.
4. Investigations of how other types of databases function in
the biological and biomedical sciences, which also point to
the extensive labour required to get these infrastructures to
work as scientific tools, have been carried out by
Hilgartner (1995), Hine (2006), Bauer (2008), Strasser
(2008), Stevens (2013) and Mackenzie and McNally
(2013).
5. While a full investigation has yet to appear in print, STS
scholars have explored several of the non-scientific aspects
affecting data circulation (e.g. Bowker, 2006; Harvey and
McMeekin, 2007; Hilgartner, 2013; Martin, 2001).
6. The value of causal explanations in the life sciences is a key
concern for many philosophers, particularly those inter-
ested in mechanistic explanations as a form of biological
understanding (e.g. Bechtel, 2006; Craver and Darden,
2013).
7. The validity of this claim needs of course to be established
through further empirical and comparative research. Also,
I should note one undisputed way in which Big Data rhet-
oric is affecting biological research: the allocation of fund-
ing to increasingly large data consortia, to the detriment
of more specialised and less data-centric areas of
investigation).
References
Ankeny R and Leonelli S (in press) Valuing data in post-
genomic biology: How data donation and curation prac-
tices challenge the scientific publication system. In: Stevens
H and Richardson S (eds) PostGenomics. Durham: Duke
University Press.
Baker KS and Millerand F (2010) Infrastructuring ecology:
Challenges in achieving data sharing. In: Parker JN,
Vermeulen N and Penders B (eds) Collaboration in the
New Life Sciences. Farnham, UK: Ashgate, pp. 111­138.
Bastow R and Leonelli S (2010) Sustainable digital infrastruc-
ture. EMBO Reports 11(10): 730­735.
Bauer S (2008) Mining data, gathering variables, and recom-
bining information: The flexible architecture of epidemio-
logical studies. Studies in History and Philosophy of
Biological and Biomedical Sciences 39: 415­426.
Bechtel W (2006) Discovering Cell Mechanisms: The Creation
of Modern Cell Biology. Cambridge, UK: Cambridge
University Press.
Bogen J (2013) Theory and observation in science. In: Zalta
EN (ed.) The Stanford Encyclopedia of Philosophy (Spring
2013 Edition). Available at: http://plato.stanford.edu/
archives/spr2013/entries/science-theory-observation/
(accessed 20 February 2014).
Borgman CL (2007) Scholarship in the Digital Age:
Information, Infrastructure, and the Internet. Cambridge,
MA: MIT Press.
Bowker GC (2001) Biodiversity datadiversity. Social Studies
of Science 30(5): 643­684.
Bowker GC (2006) Memory Practices in the Sciences.
Cambridge, MA: MIT Press.
Craver CF and Darden L (2013) In Search of Biological
Mechanisms: Discoveries across the Life Sciences.
Chicago, IL: University of Chicago Press.
Davies G, Frow E and Leonelli S (2013) Bigger, faster, better?
Rhetorics and practices of large-scale research in contem-
porary bioscience. BioSocieties 8(4): 386­396.
Denzin N (2006) Sociological Methods: A Sourcebook.
Chicago, IL: Aldine Transaction.
Dupre
´ J (2012) Processes of Life. Oxford, UK: Oxford
University Press.
Edwards PN (2010) A Vast Machine: Computer Models,
Climate Data, and the Politics of Global Warming.
Cambridge, MA: MIT Press.
Edwards PN, Mayernik MS, Batcheller AL, et al. (2011)
Science friction: Data, metadata, and collaboration.
Social Studies of Science 41(5): 667­690.
Gitelman L (ed.) (2013) `Raw Data' is an Oxymoron.
Cambridge, MA: MIT Press.
Hacking I (1992) The self-vindication of the laboratory sci-
ences. In: Pickering A (ed.) Science as Practice and
Culture. Chicago, IL: University of Chicago Press,
pp. 29­64.
Harvey M and McMeekin A (2007) Public or Private
Economics of Knowledge? Turbulence in the Biological
Sciences. Cheltenham, UK: Edward Elgar Publishing.
Hey T, Tansley S and Tolle K (eds) (2009) The Fourth
Paradigm: Data-Intensive Scientific Discovery. Redmond,
WA: Microsoft Research.
Hilgartner S (1995) Biomolecular databases: New communi-
cation regimes for biology? Science Communication 17:
240­263.
Hilgartner S (2013) Constituting large-scale biology: Building
a regime of governance in the early years of the Human
Genome Project. BioSocieties 8: 397­416.
Hine C (2006) Databases as scientific instruments and their
role in the ordering of scientific work. Social Studies of
Science 36(2): 269­298.
Johnson K (2012) Ordering Life: Karl Jordan and the
Naturalist Tradition. Baltimore, MD: Johns Hopkins
University Press.
Kelty CM (2012) This is not an article: Model organism news-
letters and the question of `open science'. BioSocieties 7(2):
140­168.
Leonelli S and Ankeny RA (2012) Re-thinking organisms:
The impact of databases on model organism biology.
10 Big Data & Society
Studies in History and Philosophy of Biological and
Biomedical Sciences 43(1): 29­36.
Leonelli S (2010) Packaging small facts for re-use: Databases
in model organism biology. In: Howlett P and Morgan MS
(eds) How Well Do Facts Travel?: The Dissemination of
Reliable Knowledge. Cambridge, UK: Cambridge
University Press, pp. 325­348.
Leonelli S (2012a) When humans are the exception: Cross-
species databases at the interface of clinical and biological
research. Social Studies of Science 42(2): 214­236.
Leonelli S (2012b) Classificatory theory in data-intensive sci-
ence: The case of open biomedical ontologies.
International Studies in the Philosophy of Science 26(1):
47­65.
Leonelli S (2013) Why the current insistence on open access to
scientific data? Big Data, knowledge production and the
political economy of contemporary biology. Bulletin of
Science, Technology and Society 33(1/2): 6­11.
Leonelli S, Smirnoff N, Moore J, Cook C and Bastow R
(2013) Making open data work in plant science. Journal
for Experimental Botany 64(14): 4109­4117.
Levin N, Weckoswka D, Castle D, et al. (in preparation) How
Do Scientists Understand Openness? Assessing the Impact
of UK Open Science Policies on Biological Research.
Mackenzie A and McNally R (2013) Living multiples: How
large-scale scientific data-mining pursues identity and dif-
ferences. Theory, Culture and Society 30: 72­91.
Martin P (2001) Genetic governance: The risks, oversight and
regulation of genetic databases in the UK. New Genetics
and Society 20(2): 157­183.
Mayer-Scho
¨ nberger V and Cukier K (2013) Big Data: A
Revolution That Will Transform How We Live, Work and
Think. London: John Murray Publisher.
Mu
¨ ller-Wille S and Charmantier I (2012) Natural history and
information overload: The case of Linnaeus. Studies in
History and Philosophy of Biological and Biomedical
Sciences 43: 4­15.
O'Malley M and Soyer OS (2012) The roles of integration in
molecular systems biology. Studies in the History and the
Philosophy of Biological and Biomedical Sciences 43(1):
58­68.
Porter TM (1995) Trust in Numbers: The Pursuit of
Objectivity in Science and Public Life. Princeton, NJ:
Princeton University Press.
Raley R (2013) Dataveillance and countervailance.
In: Gitelman L (ed.) `Raw Data' is an Oxymoron.
Cambridge, MA: MIT Press, pp. 121­146.
Rheinberger H-J (2011) Infra-experimentality: From traces to
data, from data to patterning facts. History of Science
49(3): 337­348.
Rosenthal N and Ashburner M (2002) Taking stock of our
models: The function and future of stock centres. Nature
Reviews Genetics 3: 711­717.
Royal Society (2012) Science as an Open Enterprise. Available
at: http://royalsociety.org/policy/projects/science-public-
enterprise/report/ (accessed 14 January 2014).
Stein LD (2008) Towards a cyberinfrastructure for the bio-
logical sciences: Progress, visions and challenges. Nature
Reviews Genetics 9(9): 678­688.
Stevens H (2013) Life Out of Sequence: Bioinformatics and the
Introduction of Computers into Biology. Chicago:
University of Chicago Press.
Strasser BJ (2008) GenBank ­ Natural history in the 21st
century? Science 322(5901): 537­538.
Traweek S (1998) Iconic devices: Towards an ethnography of
physical images. In: Downey G and Dumit J (eds) Cyborgs
and Cytadels. Santa Fe, NM: The SAR Press.
Wylie A (2002) Thinking from Things: Essays in the
Philosophy of Archeology. Berkeley, CA: University of
California Press.
Leonelli 11
