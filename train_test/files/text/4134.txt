SAGE Open
July-September 2016: 1
­15
© The Author(s) 2016
DOI: 10.1177/2158244016663800
sgo.sagepub.com
Creative Commons CC-BY: This article is distributed under the terms of the Creative Commons Attribution 3.0 License
(http://www.creativecommons.org/licenses/by/3.0/) which permits any use, reproduction and distribution of
the work without further permission provided the original work is attributed as specified on the SAGE and Open Access pages
(https://us.sagepub.com/en-us/nam/open-access-at-sage).
Article
Introduction
This article describes the establishment and application of a
new research evaluation framework: the Evaluative Action
Research (EvAR) framework which was employed in the
Evaluative Study of Action Research (ESAR). We use italics
throughout the article to illustrate the way in which the EvAR
framework phases and elements were applied and tested on
the ESAR as an example.
We begin with an overview of the EvAR framework which
our seven-strong team of international researchers developed
to provide detail and clarity for the way we would conduct the
ESAR. This beginning section of the article introduces the six
phases and multiple elements of the EvAR framework as well
as its visual representation. The six phases are as follows: (a)
preparation, (b) reconnaissance, (c) implementation, (d)
review of achievement, (e) reporting on achievements/recom-
mendations and knowledge mobilization, and (f) continued
action for improvement. In the overview, we include two of
the features of the EvAR which have been given limited
emphasis in other evaluative frameworks. First is the critical
importance of establishing protocols for an evaluative research
team working together. Second, little mention is made in other
evaluative frameworks of the importance of conducting an ini-
tial deep review of the literature to ensure the evaluative
framework matches the context to be evaluated. The
"Overview: The Evaluative Framework" section concludes
with discussion of the way the EvAR aligns with the underpin-
nings and values of action research (AR).
Six sections follow, where each phase of the framework is
detailed using ESAR as an illustration (shown in italics).
Particular emphasis is placed on the following elements
within the implementation phase: setting purposes, benefits,
indicator establishment, participants/boundary partner
engagement, methods utilized, and analysis of data.
In the "Conclusion" section, we emphasize the impor-
tance of the process of a review and reflective stance when
663800
SGOXXX10.1177/2158244016663800SAGE OpenPiggot-Irvine and Zornes
research-article2016
1Royal Roads University, Victoria, British Columbia, Canada
Corresponding Author:
Eileen Piggot-Irvine, School of Leadership Studies, Royal Roads University,
2005 Sooke Road, Victoria, British Columbia, Canada V9B 5Y2.
Email: Eileen.Piggotirvine@RoyalRoads.ca
Developing a Framework for Research
Evaluation in Complex Contexts Such as
Action Research
Eileen Piggot-Irvine1 and Deborah Zornes1
Abstract
Early investigation led the Evaluative Study of Action Research (ESAR) team to conclude that the complexity of a global,
large scale study (evaluation of more than 100 highly diverse action research [AR] projects) called for an overarching
research evaluation framework that differed from traditional frameworks. This article details the flexible, rigorous, Evaluative
Action Research (EvAR) framework developed to meet the complex demands of the diverse AR projects and the intent to
conduct high engagement research evaluation. The EvAR fulfilled multiple overarching needs to: authentically collaborate,
engage, and enhance ownership from the ESAR team and the AR project participants and boundary partners evaluated;
be informed in decision making via strong reference support; be responsive and flexible yet meet accountability demands
to track, demonstrate, and measure process, outcomes, and impacts of projects; use mixed-method data collection to
enhance rigor of findings; and utilize a highly reflective and reflexive approach to the evaluation. Many of the latter needs
align with underpinning principles and values in AR itself; that is, it is collaborative, consultative, democratic, reflective,
reflexive, dialogical, and improvement oriented. Rationale for the framework is provided alongside full details of phases and
implementation elements using the ESAR as an example. Throughout the article, features are highlighted that distinguish this
new EvAR framework from others. The advantages of adopting a flexible framework, which aims to enhance engagement of
those evaluated, are highly relevant to contexts beyond AR if ownership of evaluation outcomes is a goal.
Keywords
research impact evaluation framework, action research, process, impact, output, impact evaluation
2 SAGE Open
using a research evaluation framework and provide a sum-
mary of the team's reflections throughout the ESAR project
as an example. Such a stance is seldom featured in traditional
evaluative frameworks though it is a strong component of
AR. Extensive knowledge mobilization is discussed next in
the conclusion. Finally, we sum up the effectiveness of the
EvAR as an evaluative framework developed for the ESAR
project.
Overview: The Evaluative Framework
In this overview, we provide an outline of the EvAR frame-
work phases and elements. We want to clarify that our devel-
opment of the framework did not occur until some months
after the ESAR instigation. It was only after the team had
somewhat intuitively engaged in what we describe as Phases
1 and 2 that we realized we were creating an evaluative
framework which differed from many others we explored.
Such a distinctive framework was essential for our work in
evaluating more than 100 complex AR projects globally in
the ESAR. A different framework was also needed to fit our
equally complex international research team who wished to
uphold the principles and values associated with AR itself in
the evaluative research.
The EvAR (Figure 1) begins with the crucial initial step of
a research team creating clarity around the way they will
work together. This preparation phase (Phase 1) includes
establishment of some form of agreement and protocols cov-
ering principles and values. We found little emphasis of this
step in other framework outlines.
The reconnaissance phase (Phase 2) is also rarely men-
tioned in evaluative frameworks. In this phase, two steps are
included which are associated with becoming informed prior
to implementing a research evaluation. First, we recommend
a probing literature review be conducted prior to initiating
evaluative research to gain foundational understanding of the
complexity of research evaluation frameworks. This is fol-
lowed by an investigation of the context such a framework
will be used for. We offer that such an exploration is critical
to becoming sufficiently informed to move to the second step
of selecting and justifying an appropriate framework and
constituting elements.
The implementation phase (Phase 3) of the EvAR incorpo-
rates process elements. Phase 3 begins with setting purposes,
identifying the benefits of the study, articulating objectives
and research questions, and establishing indicators for evalu-
ation. The importance of determining the appropriate partici-
pants in a study is outlined, as is stakeholder (also noted as
boundary partners in this article) engagement. Next, elements
covering the methodology and methods selected and data
analysis used are noted. We do not offer a prescriptive
approach in the EvAR framework but rather allow for delib-
erate flexibility and considerable choice of tools.
Following the articulation of constituting elements, Phase
4, the review of achievement phase is discussed. In this
phase, the focus is on gathering data regarding the effective-
ness of the research evaluation process with meta-reflection
and reflexivity as guiding approaches.
The review of achievement phase is followed by the
reporting achievements, recommendations, and knowledge
mobilization phase (Phase 5). We believe that, ideally, report-
ing out and knowledge mobilization could occur throughout
the entire application of the framework in much the same
way that AR itself often includes iterative reporting to
enhance ownership and further input on findings.
The continuation arrow shown in Figure 1 for the EvAR
indicates that ongoing action is likely to result from Phases 4
and 5. The sixth and final phase of the framework, the con-
tinued action for improvement phase (Phase 6), encourages
the evaluative researchers to be responsive to emergent needs
for further improvement in their evaluation.
The EvAR, as its name suggests, follows an AR philoso-
phy and process. In summary, we developed an AR-based
evaluative framework that could be utilized to evaluate AR
in our ESAR in what could be described as a meta-AR
approach to "function as an umbrella process, a meta-meth-
odology, under which a variety of flexible methods can be
assimilated" (Dick et al., 2015, p. 38). Meta implies that an
AR model is used at a higher level; in the case of the EvAR,
it is AR on AR. We note strongly, however, that the EvAR
could be adopted as an evaluative research framework for
many other types of research.
The framework has all the hallmarks of AR including
combined data collection (systematic research and inquiry)
and change (action) phases (Davison, Martinsons, & Kock,
2004; Dehler & Edmonds, 2006; Gosling & Mintzberg,
2006; Piggot-Irvine et al., 2011). Like AR, the EvAR tran-
scends disciplinary, institutional, and international boundar-
ies with a central focus on research with (and alongside)
boundary partners (all stakeholders) and communities
(Cardno, 2003; Greenwood & Levin, 2007; Reason &
Bradbury, 2001; Stringer, 2014). This inclusive quality urged
in AR and the EvAR is associated with enhancing the capac-
ity of groups and organizations to own and sustain change.
As Greenwood and Levin (2007) indicated, "AR is a set of
self-consciously collaborative and democratic strategies for
generating knowledge and designing action in which trained
experts in social and other forms of research and local stake-
holders work together" (p. 1). The change orientation along-
side the underpinning collaborative and democratic values
and strategies sets AR, and EvAR, apart from most tradi-
tional forms of research and evaluation. We strongly believed
that any evaluative framework which was low in collabora-
tive, participative, democratic and transformative intent
would likely be rejected by those involved in the AR projects
we wanted to investigate, and most importantly that such a
framework would likely lead to low ownership of findings
by those involved in the projects.
The EvAR, like AR, also includes an emphasis on open-
ness to unpredictability and flexibility (Coghlan & Brannick,
Piggot-Irvine and Zornes 3
2010; Stringer, 2014). The EvAR therefore sits well within
broader thinking about complexity where order and predict-
ability are limited (Kurtz & Snowden, 2003).
Such openness matched our needs for the ESAR because
we wanted to be responsive to increasing demands to track,
demonstrate, and measure the impacts and outcomes of
research (Axelrod, 2002; Carroll, 2003; Conteh, 2013;
Giroux & Giroux, 2006a, 2006b, 2009; Popp, Milward,
Mackean, Casebeer, & Lindstrom, 2014), but we also wanted
flexibility in our framework sufficient to deal with the highly
diverse 100 plus AR projects to be evaluated.
Flexibility is also linked to the EvAR pragmatic orienta-
tion to method employment that had previously been
designed by one of the team (Piggot-Irvine, 2012b).
Greenwood (2014) defined such a pragmatic approach in AR
as that which
will use theory and methods from any corner of the sciences,
social sciences and humanities if they offer some hope of helping
a collaborating group move forward. If numbers are needed,
statistical social science, surveys and other formal techniques
can and will be used. (p. 647)
Figure 1. Evaluative AR framework.
Note. AR = action research.
4 SAGE Open
It is also pragmatic in Metcalfe's (2008) terms in that
findings can be created which are meaningful and help those
affected to construct understanding and design actions rele-
vant to their community.
The framework, in keeping with AR, has a cyclic, itera-
tive, depiction that sometimes has spin-off (McNiff, 1988),
or slightly divergent, cycles. This cyclical orientation (itera-
tive planning, acting, reflecting, and evaluating within larger
cycles or phases) is supported by multiple authors (e.g.,
Coghlan & Brannick, 2014; Piggot-Irvine et al., 2011;
Preskill & Torres, 1999; Sankaran, Tay, & Orr, 2009).
The EvAR is also associated with further underpinning
principles that are not so typical of AR, with some indicating
enhanced expectations of rigor. The latter include focusing
on research that evaluates precursors, processes, outcomes
and impacts; establishing clarity of this focus via evaluation
indicators that are both bibliometric and nonbibliometric;
and considering complexity by seeking to understand mean-
ing (largely through Ql data) as well as searching for causal-
ity (through Qn data). To avoid repetition, we note that each
of these principles is covered later in discussion of the indi-
vidual elements of the framework.
Each phase of the EvAR is detailed in the subsequent sec-
tions of this article, using the ESAR as an example (illus-
trated in italics).
Phase 1: Preparation
The inclusion of a preparatory phase as we describe it has not
been mentioned in traditional evaluative research frame-
works we explored. The following outline of Phase 1 there-
fore exclusively describes the employment of the phase in
the ESAR project as an example.
In our initial work together in the ESAR as seven interna-
tionally dispersed researchers with varying levels of under-
standing and experience of either or both AR or evaluation,
we decided that we could not progress without establishing
consensus about commonality of values, principles, and pro-
tocols for working as a cohesive, highly collaborative team.
Such an important element is widely valued in AR itself, and
was a priority for the ESAR team because, as noted in Rowe,
Graf, Agger-Gupta, Piggot-Irvine, and Harris (2013), "the
grounding of a change initiative in early stage elements of
thoughtful inquiry, collaboration, dialogue and reflection
often mitigates resistance and enhances progress on imple-
menting a change agenda" (p. 4).
The team spent two days on this preparatory phase and
the resulting documents developed have continued to provide
reference points throughout our work together. Without
extensively reporting on the content of the documents, we
note briefly that working together authentically in collabora-
tion with each other and with all boundary partners domi-
nated our protocols. The approach to collaboration drew
strongly upon the six preconditions for collaboration out-
lined in Piggot-Irvine (2012a) as "trust; shared goals;
shared language; a desire to participate; openness and lis-
tening; and passion for the process" (p. 2). We also noted the
following advantages of collaboration as stated in Piggot-
Irvine and Bartlett (2008):
. . . the advantages to the participants of collaboration in action
research are cited as many and various (D'Arcy, 1994; Kemmis &
McTaggart, 1990; Tripp, 1990; Wadsworth, 1998). For one thing,
it can allow for public testing of private assumptions and
reflections; that is, it helps to avoid self-limiting reflection (Schön,
1983).Collaborationcanalsoenhanceownershipandcommitment
to change and it can leverage the change to a level frequently
unattainable through individual reflection alone. (p. 25)
As a research team, we felt that just collaborating, as a
principle, would be insufficient and that the collaboration
and democratic values of AR needed to be linked to dialogue
if trust was to be an outcome. Dialogue is associated with
open, nondefensive (Argyris, 2003) interactions where bilat-
eral (considering two sides) and multilateral (considering
multiple sides) conversation dominates. Dialogue is charac-
terized by two essential components. The first is the offering
of openness about perspectives by those collaborating,
alongside provision of evidence and reasoning behind those
perspectives (an advocacy approach). The second compo-
nent is that of receiving, checking, and understanding of oth-
ers' perspectives without prejudgment, or assumptions (an
inquiry approach), so that mutual understanding can be
reached. Preskill and Torres (1999) summed up the dialogue
orientation resulting from this advocacy and inquiry balance
in suggesting: "individuals seek to inquire, share meanings,
understand complex issues, and uncover assumptions"
which facilitates "learning processes of reflection, asking
questions, and identifying and clarifying values, beliefs,
assumptions and knowledge" (p. 53).
Once the ESAR team had created a shared understanding
of the values and protocols for working together, we were
ready to dig deeply into the literature associated with our
framework development task in the reconnaissance phase.
Phase 2: Reconnaissance
As suggested in our introduction, we propose that consid-
erable investigation of evaluative frameworks and existing
knowledge of the context which a framework will be used
within could precede construction of any research evalua-
tion framework. In this statement, there is a premise that
we believe a conceptual framework is necessary despite
the existence of advantages and disadvantages. Baxter and
Jack (2008), for example, suggested that one advantage of
a framework lies in its ability to serve as an anchor for a
study. They also noted, however, that framework construc-
tion may be constraining and limit an inductive approach.
In our experience, such reconnaissance investigation there-
fore can help to clarify why and how any research evalua-
tion might occur.
Piggot-Irvine and Zornes 5
We propose a reconnaissance phase in the framework
which includes literature reviews on the two foundational
topics of (a) research evaluation frameworks themselves and
(b) the context of the specific evaluation research to be con-
ducted to enhance the possibility of a framework-context
match. The following discussion outlines the two founda-
tional topics with illustration via the ESAR.
Foundational Topic 1: Exploring Frameworks
Avast range of research evaluation frameworks exist, includ-
ing the Research Excellence Framework (Parker & van
Teijlingen, 2012); STAR METRICS, which aims to "assess
and understand the performance of research and researchers,
largely for accountability purposes, using data mining and
other novel low burden methods" (Guthrie, Wamae,
Diepeveen, Wooding, & Grant, 2013, p. 2); Excellence in
Research for Australia (ERA); Canadian Academy of Health
Science Payback Framework; National Institutes of Health
Research (NIHR) Dashboard; Productive Interactions;
Evaluation Agency for Research and Higher Education
(AERES) framework; Congressionally Directed Medical
Research Program (CDMRP); Performance-Based Research
Fund (PBRF); and Standard Evaluation Protocol (SEP).
Many of the traditional frameworks we explored focused
largely on measuring impact through a process of external
peer review and frequently emphasized the use of quantifi-
able bibliometric indicators. A recent trend is toward frame-
works incorporating both bibliometric and nonbibliometric
indicators. The Payback framework is an example of the lat-
ter and is used extensively in health research internationally
(Buxton & Hanney, 1996; Buxton, Hanney, & Jones, 2004;
Donovan & Hanney, 2011). The Payback framework incor-
porates both academic outputs and wider societal benefits to
assess outcomes (knowledge production such as journal arti-
cles, etc.), target future research, build capacity, inform poli-
cies and project development, create health and health sector
benefits such as better health and health equity, and enhance
broader economic benefits (Buxton & Hanney, 1996).
Frameworks incorporating both bibliometric and nonbib-
liometric indicators often fall under the cluster of SIAMPI
(Social Impact Assessment Methods for research and fund-
ing instruments through the study of Productive Interactions
between science and society) and have "a central theme of
capturing `productive interactions' between researchers and
stakeholders" (Penfield, Baker, Scoble, & Wykes, 2014, p.
24). The focus is on understanding how research interactions
lead to social impact. The Australian Research Quality
Framework (RQF), for example, uses a case study approach
to demonstrate and justify public expenditure on research
and asks researchers to provide "evidence of economic, soci-
etal, environmental, and cultural impact of their research"
(Penfield et al., 2014, p. 24). Although RQF was never
implemented, it was adapted for the United Kingdom
Research Evaluation Framework (REF), which continued
with the case study approach, adding significance, depth,
spread, and reach as further nonbibliometric criteria for
assessment. Here, depth and spread refer to "the degree to
which the research has influenced or caused change, whereas
spread refers to the extent to which the change has occurred
and influenced end users" (Penfield et al., 2014, p. 24).
In general, as Guthrie et al. (2013) offered, trade-offs are
associated with any framework construction decisions in
evaluation of research. Trade-offs are summarized as
follows:
· Quantitative approaches (those which produce numerical
outputs) tend to produce longitudinal data, can be applied
relative to fixed baselines reducing the need for judgment
and interpretation, and are relatively transparent, but they
have a high initial burden (significant work may be required
at the outset to develop and implement the approach);
· Formative approaches (which focus on learning and
improvement rather than assessing the current status) tend
to be comprehensive, evaluating across a range of areas, and
flexible, but they do not produce comparisons between
institutions;
· Approaches which have a high central burden (requiring
significant work on the part of the body organizing the
evaluation process) tend not to be suitable for frequent use;
· Approaches which have been more fully implemented tend
to have a high level of central ownership (by either the body
organizing the evaluation, or some other body providing
oversight of the process); and
· Frameworks that place a high burden on participants require
those participants to have a high level of expertise (or should
provide capacity building and training to achieve this).
(Adapted from Guthrie et al., 2013, pp. 8-9)
Overall, individual frameworks have specific strengths
and limitations, and each should be weighed up in choosing
a framework. Penfield et al. (2014) suggested the following
limitations that we considered associated with:
· time lag--outcomes and impacts can take years to
materialize and it may be very difficult, if not impossible to
trace them back to the project/research;
· developmental nature of the impact--impact changes and
develops over time and can be temporary or long lasting;
· attribution--over time, it becomes more and more difficult
to tie outcomes, and especially impacts, directly back to the
research and the research findings;
· complementary assets--over time, as various factors and
inputs influence the outcomes, it becomes difficult to
attribute the outcome back to the original research and
findings);
· knowledge creep--typically, new data, discoveries and
information become accepted and absorbed over a long
period of time; and
· gathering evidence--in many cases, the requirement to
collate evidence retrospectively may be difficult as
measures, baselines and evidence itself has not been
collected and may not be available. (adapted from
pp. 25-27)
6 SAGE Open
Despite a less inductive orientation indicated with using a
framework, we decided that we needed an evaluation frame-
work to guide the complex ESAR and we embarked upon an
exploration of the varied frameworks we have described in
this section. Initially, we sought to find a framework which
was a good fit for our planned study, or find aspects of mul-
tiple frameworks that might help guide us.
We considered that both bibliometric and nonbibliomet-
ric indicators were relevant in our research evaluation. We
also decided to adopt considerations from Guthrie et al.
(2013) including that the framework might promote learn-
ing and development and quality improvement, that is, it
could have analysis and accountability purposes; be an
iterative process; draw out wider social, economic, and
policy impacts; minimize administration burden; hold
transparency with rules and processes; include team-based
research; apply collaborative (including cross-disciplin-
ary, cross-institution) research; support capacity building
and development of next generation researchers; and be
helpful if it gathered longitudinal data to support quality
improvement.
Foundational Topic 2: Investigating the AR
Context
In the reconnaissance phase, we consider that a probing lit-
erature review could also cover exploration of the context in
whichtheresearchevaluationwillbeconducted.Furthermore,
such review could include examination of the extent to which
the context has previously been evaluated.
In the ESAR, our literature review of the AR context con-
firmed our knowledge that AR is frequently seen as a popu-
lar developmental research methodology with combined
data collection (research) and change (action) elements
(Piggot-Irvine et al., 2011). Earlier in the "Overview: The
Evaluative Framework" section of this article, we have
summarized many of the other principles of AR including
its pragmatic, responsive, iterative, and flexibly applied
action-orientation with a core element of systematic
research and inquiry processes; ability to transcend disci-
plinary, institutional, and international boundaries; and
focus on research which is inclusive of boundary partners
to democratically enhance the capacity of groups and orga-
nizations to sustain change, develop resilience, and thrive.
We have also reported on the degree of unpredictability,
contextual and cultural specificity of AR, and such charac-
teristics have a consequence of nongeneralizable findings
(Coghlan & Brannick, 2010; Stringer, 2014). AR is also
variably defined (Cardno, 2003; Kemmis, 2010; Meyer,
2000; Piggot-Irvine et al., 2011; Wicks & Reason, 2009)
with subsequent implementation that is also highly
variable.
The principles of AR summarized in our probing litera-
ture review of the evaluation context led us to conclude
that the complexity of the large scale ESAR we
were planning called for an overarching framework which
differed from any of the traditional research evaluation
frameworks we examined. We had dual overarching needs
because we wanted to be responsive to increasing demands
to track, demonstrate, and measure the impacts and out-
comes of research but we also wanted flexibility in our
framework. The framework needed to be pragmatic and
flexible enough to deal with the context and practice
diversity of the 100 plus AR projects to be evaluated but
also needed to match the responsive, collaborative, demo-
cratic, and dialogical underpinnings and values associ-
ated with AR itself if we were to gain ownership, respect,
and credibility from action researchers. We believed that
any evaluative framework which was low in collaborative,
participative, democratic, and transformative intent could
be rejected by those involved in the AR projects we wanted
to evaluate, and most importantly that it could likely lead
to low ownership of findings by those involved in the
projects.
Establishing a rationale for the ESAR framework was
relatively easy because our literature review revealed a gap
in terms of evaluation of AR. The touted high ideals of AR
shown in the literature review, alongside its variable inter-
pretation and implementation, almost set up the approach
for substantial critique with it referred to as "muddled sci-
ence" (Winter, 1987, p. 2), "sloppy research" (Dick, 2004,
p. 16), with reporting as "little more than picturesque jour-
neys of self-indulgent descriptions" (Macpherson & Brooker,
1999, p. 210). Koshy, Koshy, and Waterman (2011) added
that change associated with AR was hard to measure and
there was often poor theory development. As a team, we con-
cluded that such critique prevails because little evaluative
data exist to demonstrate whether the ideals espoused for AR
are widely realized. The paucity of evaluative data was
strongly expressed by Piggot-Irvine and Bartlett (2008) who
stated there was a great deal of literature discussing or iden-
tifying what constitutes good AR, but very little evaluation of
AR outcomes or impact. A strong rationale for the ESAR was
able to be articulated in our framework and our next task in
framework construction was to establish clear direction for
implementation via purpose, objectives, and research
questions.
Phase 3: Implementation
The implementation phase of the EvAR is the most inten-
sively covered in this article. It is during this phase that
purposes and benefits (justification) for the choice of a spe-
cific research evaluation can be outlined. Furthermore, at
this phase, detail of the constituting elements describing
how the research evaluation will be conducted is noted (as
summarized in Figure 2) in the framework. This section of
the article covers description of the constituting elements of
the EvAR alongside illustration with application to the
ESAR.
Piggot-Irvine and Zornes 7
Purpose, Objectives, and Research Questions
Guthrie et al. (2013) noted that the "design of a framework
should depend on the purpose of the evaluation" (p. ix).
These authors described the purposes of research evaluation
as (with our interpretation):
·
· advocacy (demonstrating benefits, enhancing under-
standing of the research process among policymakers
and the public, and making a case for change/
improvement);
·
· accountability (showing efficiency of use of resources
within research);
·
· analysis and learning (demonstrating how and why
research is effective, and how it can be better sup-
ported); and/or
·
· allocation (determining where and how best to allo-
cate resources in the future).
Such purposes, in turn, are linked to whether an evalua-
tion intent is formative (ongoing and learning, developmen-
tal) or summative (endpoint and accountability oriented) as
summarized in Piggot-Irvine and Bartlett (2008). Guthrie
et al. (2013) stressed that purposes have to be clear from the
outset because many other framework decisions are linked to
those purposes.
Furthermore, Aberatne (2010) and more specifically
Durlak and DuPre (2008) have made a solid case for the
need to understand purposes and process implementation in
evaluating outcomes. For example, Durlak and DuPre
(2008), in their own research, asked, "1) Does implementa-
tion affect outcomes?; and 2) What factors affect implemen-
tation?" (p. 328).
In the ESAR project, Guthrie et al.'s (2013) primary pur-
poses of advocacy, and analysis and learning predominated.
Advocacy was strong because we wanted to demonstrate
benefits, effective processes, and improvement impacts of
AR. Analysis and learning also dominated as purposes due to
our intent to showcase how and why AR led to different types
of impacts. Both purposes have formative intent, but because
we wanted to evaluate the efficiency of resource use within
AR projects we studied, there was also a secondary account-
ability (summative) purpose.
Purpose decisions led to clarification of the overall
objective for the ESAR as to explore, via an examination
of process and outcomes of approximately 100 AR proj-
ects implemented in varied contexts globally; whether and
how the often touted espousals of individual, community,
organizational, and/or societal impact of AR are actually
realized; and to advance knowledge and understanding of
the elements of AR enhancing outputs, outcomes, and
impact.
Further focus in the ESAR was articulated through the
clarification of the key research question:
Research Question 1: In what ways can AR be validated
as a contributor to meaningful individual, community,
organizational, and societal change?
The overall objective and question shows that the ESAR
had a focus on both process and outcomes. Findings were
also intended to provide clarity about validity claims for
AR as an approach to change. In addition, more general
outcomes associated with advancing knowledge were
hoped for from the ESAR. These outcomes included build-
ing on current research from Piggot-Irvine and Bartlett
(2008) on evaluation of AR, establishing evaluative indi-
cators for AR, and creating a publicly accessible AR
repository as a directory for AR project reports and
research findings.
Establishing Benefits
Intended benefits of any study should be strongly articulated
in a research evaluation framework (de Jong, van
Arensbergen, Daemen, van der Meulen, & van den Besselaar,
2011; Guthrie et al., 2013; Hemlin, 2006; Klein, 2006, 2008;
Spaapen, Dijstelbloem, & Wamelink, 2007; Spaapen & van
Drooge, 2011). Such benefits can be articulated as justifica-
tion for a research evaluation study.
Key benefits of the ESAR included that it was conducted
in multicontextual, nonacademic, communities (e.g., health,
sport, development aid, education, agriculture, environ-
mental, management, and leadership, to name but a few),
Figure 2. Constituting elements in the EvAR framework.
Note. EvAR = Evaluative Action Research.
8 SAGE Open
and findings of the ESAR study were to be of interest to a
variety of disciplines (sometimes transdisciplinary), aca-
demic fields, and research areas such as philosophy, sociol-
ogy, science, arts, and so on. A further benefit was reported
as enhanced AR credibility. We believed that the current
perception of limited impact of AR was substantially due to
the minimal examination of outputs, outcomes, and impact.
In our framework, we recorded that the ESAR findings
could not only address this limitation but also add recom-
mendations on processes that enhance effective outcomes
for action researchers. If outcomes, outputs, and impact
were validated, there could be reduction of criticism of low
credibility of AR. We stated in our "Establishing Benefits"
section of the EvAR framework that, at the least, recom-
mendations for improved AR process/practice could be
established to demonstrate how AR might be designed to
genuinely create thinking and behavior leading to improve-
ments in economic, social, cultural, and intellectual
well-being.
Indicator Establishment
Guthrie et al. (2013) emphasized that a framework "requires
careful selection of units of aggregation for the collection,
analysis and reporting of data" (p. x). Units of aggregation
are most often referred to as indicators. Indicators can be
discussed from varying perspectives, including scope (meth-
ods, dimensions of indicators) and establishment (extent of
collaboration in development, etc.).
In terms of scope, Penfield et al. (2014) offered specifi-
cally that in data collection methods there should be a focus
on metrics, narratives, surveys, and citations (within and out-
side of academia) as indicators for evaluating the success of
research. A broader, dimensions oriented, emphasis proposed
by Wickson and Carew (2014) included that indicators should
focus on whether a project/research is/was socially relevant
and solutions oriented, sustainability and future scanning,
diverse and deliberative, reflexive and responsive, rigorous
and robust, creative and elegant, and honest and accountable.
Jahn and Keil (2015) noted similar dimensions focusing on
the quality of the research problem (considering different spa-
tial, temporal, and social scales), research process (level of
integration and epistemic, social organization, and communi-
cative levels), and research results (maintaining the viability
of society, and the attention to current and future issues of
justice).
There has, however, been growing acknowledgment that
traditional bibliometrics (including citations, number of
patents, licenses, spin-off firms, revenue generated, etc.)
are insufficient for measuring the impact of research
(Universities Canada, 2008; Butler, 2008; Donovan, 2006,
2008; Duryea, Hochman, & Parfitt, 2007; Rasmussen,
2008). Donovan (2008), instead, suggested including non-
bibliometric indicators. Those with relevance to the EvAR
include the following:
Honours and awards, election to and roles within learned
societies, journal editing, editorial board membership, editing
special issues of journals, special journal editions dedicated to
one's research, invited lectures at conferences (particularly
keynote addresses), organising conferences or workshops,
activities in providing academic advice (e.g., assessing research
applications,manuscriptrefereeing,supervisionandexamination
of PhD theses), contributions to dissemination/popularization of
research in the media, policy preparation research . . . visiting
professorships or fellowships and conferences dedicated to
specific research. (p. 30)
The approach adopted for establishing indicators is possibly
as, if not more, important than scope in a framework.
Furthermore, Defila and Di Giulio (1999), Huutoneimi (2010),
Huutoneimi and Tapio (2014), and Spaapen and van Drooge
(2011) all emphasized the importance of joint development of
indicators by the researchers and stakeholders involved.
In the ESAR, we were mindful of AR as a complex system
and that in such systems, outcomes and end states are not
known with any degree of certainty, only probability. We
drew upon the work of multiple authors who had established
indicators with any relevance to AR. Included were ideas
from Bryman and Bell's (2011) indicators for authenticity;
Meyer's (2000) consideration of change and knowledge;
Piggot-Irvine's (2008) indicators for meta-evaluating AR;
Earl, Carden, and Smutylo's (2001) definition of outcomes
from change; and Wadsworth's (2011) indicators for success.
We favored indicators that not only evaluated the quality of
outcomes of the AR project but also the extent to which the
project made a difference and a difference that is ongoing--
the sort of sustainability referred to by Wickson and Carew
(2014) and Jahn and Keil (2015). We were particularly con-
scious of the fact that a very wide range of impacts could be
associated with projects in the ESAR and our categorization
of indicators would likely be complex and extensive. Care
was also taken to ensure the indicators could be easily ana-
lyzed given our mixed-method design.
For indicator organization, we developed subsections of
"Precursors/Preconditions," "Process and Activities," and
"Postaction Research Outputs, Outcomes, and Impacts."
The organization of indicators formed part of a conceptual
explanatory model which was based on a logic model
(Kellogg Foundation, 2001) showing a research to impact
progression (for a comprehensive outline, see Piggot-Irvine,
Rowe, & Ferkins, 2015).
The early indicator establishment and confirmation task
developed for the ESAR was probably the most intensive and
time-consuming of all activities in our framework construc-
tion. We upheld the inclusive orientation through our com-
mitment to, and fierce enactment of, the collaborative and
dialogical intent of AR. We spent months jointly creating the
indicators and then over a year extensively seeking feedback
on these from the wider AR community. We believe that the
time spent was invaluable in creating clarity for development
of data collection tools, analysis, and subsequent reporting.
Piggot-Irvine and Zornes 9
Participants and Stakeholder Engagement
Defining who will respond and participate in a research evalua-
tion is the next constitutional element of a framework. In the
EvAR, we consider that determination of selection criteria is an
important step prior to participant selection. Criteria establish-
ment is usually followed by sampling and choice of participants
which, as in all research, is strongly linked to method selection,
with the latter often preceding the former. In the EvAR frame-
work, because collaboration and ownership are valued highly
(Cardno,2003;Greenwood&Levin,2007;Reason&Bradbury,
2001; Stringer, 2014), recording the approach to participant
selection takes on even greater significance.
We articulated in the ESAR that we were evaluating mul-
tiple projects at a meta-level, so the definition of participants
also included projects. We established the selection criteria
as projects having (a) clear articulation as AR (including
participatory AR); (b) a change emphasis arising out of an
issue, concern, or need; (c) articulation of espousal of
improvement or capacity building which may have been, in
turn, linked to goals of personal, team, organization, or soci-
ety improvement; (d) the usual characteristics of collabora-
tion and iterative phases of action and reflection; (e)
outcomes of publication or reporting dissemination post-
2008; and (f) availability of a project lead and other team
members and stakeholders (i.e., all boundary partners).
More than 100 projects from several countries and varied
contexts met the criteria. No sampling was required other
than meeting the criteria. Similarly, because all project team
members were included in a large scale online survey in the
ESAR, no sampling within projects was involved. The case
studies examined in the ESAR, however, were purposefully
(Adams, Khan, Raeside, & White, 2007) selected because we
drew upon projects that were able to be accessed with relative
ease and in reasonably close proximity to the research team.
Clarity in a framework is also needed about whether (and
which) boundary partners or stakeholders will be involved. The
importance of including stakeholders in research is reinforced
by Bergmann et al. (2005), Spaapen et al. (2007), Mitchell and
Willetts (2009), Carew and Wickson (2010), Smudde and
Courtright (2011), Tremblay and Hall (2014), and Dick et al.
(2015).As Tremblay and Hall suggested, impactful knowledge
creation and mobilization occurs when communities and stake-
holders are authentically engaged. Wickson and Carew (2014)
also emphasized the importance of stakeholder inclusion in
their proposed four central characteristics in responsible
research and innovation (RRI), the second of which is "a com-
mitment to actively engaging a range of stakeholders for the
purpose of substantively better decision-making and mutual
learning" (p. 255). Furthermore, Ackermann and Eden (2011)
noted the importance of "identifying who the stakeholders
really are in the specific situation . . . ; exploring the impact of
stakeholders' dynamics . . . ; and development stakeholder
management strategies" (p. 180) as being critical to the success
of an endeavor. As Phillipson, Lowe, Proctor, and Ruto (2012)
suggested, "effective research uptake in policy and practice
may be built upon a foundation of active knowledge exchange
and stakeholder engagement during the process of knowledge
production itself" (p. 57).
Once a decision to engage participants is decided upon,
then clarifying how they will be involved is also critical.
There are many research studies which claim to include or
involve communities, and indeed start out with the premise
that all stakeholders are equal. Somewhere along the way,
however, these studies often deteriorate to a researcher
knows best model, where participants, subjects, and stake-
holders are secondary (Zornes, 2012). As we reported in
Zornes, Ferkins, and Piggot-Irvine (2016),
in AR, building relationships is a defining feature and their
importance is paramount. The relationships developed, in turn,
spawn a variety of networks, among teams, with stakeholders,
and with the larger community . . . Stakeholders, often referred
to as boundary partners, can, and should, include a wide cross-
section of individuals and partnerships. (p. 6)
Piggot-Irvine (2012a) identified preconditions necessary
for how to engage participants when she stated that collabo-
ration needed to include shared goals and language alongside
a desire to participate and openness. Such collaboration cre-
ates an outcome of trust and trust, in turn, is "the lubricant
that makes cooperation possible between these actors and
higher levels of trust are believed to lead to increasing net-
work effectiveness" (Popp et al., 2014, p. 10).
In the ESAR project, participants were involved throughout
the research process wherever possible. The emphasis was on
creating an authentic collaborative relationship with partici-
pants. A prerequisite for creating such collaboration with oth-
ers was to establish our own research team approach to
collaboration and engagement at the initial stages of the study
and to model this throughout the study. Furthermore, to
enhance engagement with participants, we focused on methods
for data collection which aimed to enhance dialogue, including
focus groups and goal attainment scaling (GAS). We also com-
mitted to quickly sharing findings with participants to clarify
our interpretations and enhance ownership of recommenda-
tions for change. Our goal was to ensure that all boundary
partners had ownership of any improvements implied in the
findings. Ownership, in our eyes, could only be assured if those
who led the organizations and communities impacted by the AR
projects we studied were involved as early as possible in con-
firmation of indicators, as well as discussion of findings and
recommendations (the latter point was also particularly
emphasized by Brown & Isaacs, 2005).
Methodology and Methods
All frameworks usually include a description of the over-
arching methodology and methods used. The importance of
transparency and systematicity in the description has been
10 SAGE Open
noted by Meyrick (2006). As Meyrick (2006) pointed out, a
framework should "communicate enough knowledge about
the process to enable readers to make a value judgement
about rigour and quality" (p. 804). Knowledge includes
ensuring there are clear details regarding what data are to be
collected as well as how the collection will occur.
In the EvAR framework, we have encouraged extension of
the usual AR multimethod approach for enhancing data cred-
ibility (Yin, 2003) to a mixed-method methodology (Creswell,
2009; Ivankova, 2015; Ivankova, Creswell, & Stick, 2006)
falling under triangulation and convergence typologies
(Creswell & Plano Clark, 2011). The indicators are used to
inform construction of the methods used for data collection
within the mixed-method methodology. We accepted
Creswell's (2009) interpretation of mixed methods where the
Qn component uses statistical formulae "so that numbered
data can be analyzed using statistical procedures . . ." (p. 4).
We add that Qn combined with Ql offers a more holistic
understanding of perceptions. The rationale for using mixed-
method methodology in the EvAR has been based on the
assumption that quantitative (Qn) or qualitative (Ql) alone is
insufficient, Qn and Ql complement each other, and such a
mix allows for more robust analysis (Youngs & Piggot-Irvine,
2012, 2014). In the EvAR framework, the methodology with
wide choice of method selection aligned with the intent ofAR
for flexibility and responsiveness.
In the ESAR, we considered that a mixed-method method-
ology would meet our need to add insight and understanding
while recognizing the influence of context and perception
alongside identifying strength of relationships between indi-
cators and the ability to generalize findings. An electronic
survey was first piloted by five experienced action research-
ers who were not participants in the study. Findings from the
pilot survey helped inform tool development for seven pilot
case studies (which later became the seven further full case
studies) using documentary analysis, further surveys, focus
groups, semistructured interviews, and GAS as data sources.
Almost all of these methods are well known with the excep-
tion of GAS. Molyneux et al. (2012), Latham and Locke
(2006), and Roach and Elliott (2005) provided detail on
GAS, but briefly it is a tool for ranking and quantifying indi-
cators. Further articles on the methodology and methods are
forthcoming.
Data Analysis
Following (and connected to) indicator, methodology and
method decisions in a research evaluation framework is the
determination of data analysis techniques. Meyrick (2006)
noted the importance of this element at the end of compo-
nents of study conduct. However, as recommended by Baxter
and Jack (2008), in the EvAR, we favored data collection
and analysis occurring concurrently where feasible.
For the ESAR, we recorded that we would closely link our
indicators to analysis and ensure that both Qn statistical
analysis and Ql thematic analysis could be carried out given
our mixed-method design. For Qn analysis, we converted
many of the indicators into questions for the survey which
had a 5-point Likert-type scale indicating levels of agree-
ment. Qn survey data analysis was then conducted using the
online program Fluid Survey with the results imported into
SPSS 13. We will use scale analyses of both discrete (logistic
regression) and continuous data (multiple regression) to
show associational/causal analyses. The latter was designed
to enable derivation of relationships between preconditions,
process, and postproject outcomes and impacts. From here,
degree of satisfaction or completeness of reaching project
espousals will be used to identify the strength of relationship,
or a predictive path for future studies. In this way, we could
propose that if condition X was in place, the impact on proj-
ect success was more likely to be Y. We also acknowledged
that while causal analysis is not a fundamental part of AR,
we considered that the Qn analysis based on prediction mod-
els of outcomes was an important component of the ESAR.
We were cognizant that the specific sustainable change out-
comes and impacts could vary wildly from context to context in
AR projects and therefore decided the collection of Ql data
would be more appropriate for these components. Varied anal-
ysis tools were used for ESAR Ql data including descriptive
and thematic analysis (using predetermined coding criteria
and NVivo software) of the existing documentation on case
studies, open-ended survey responses, interview transcripts,
and focus group responses. We looked for patterns, linkages,
explanations, and synthesis of ideas in this analysis. Our intent
was akin to that of Srivastava and Hopwood (2009) to "provide
the best explanation of what's going on" (p. 77).
Overall in our analysis, the convergence type of mixed-
method methodology (Creswell & Plano Clark, 2011)
enabled us to compare the findings from both Qn and Ql
analysis to understand an overall case. Member-checking
was intended with all Qn and Ql data.
Phase 4: Review of Achievement
In the review of achievement phase in the EvAR, we have
noted the importance, at a meta-level, of gathering data on
effectiveness of the evaluation conducted. Such a phase is
infrequently mentioned in other frameworks yet it is widely
noted as a vital component of AR. In the EvAR, we have
encouraged review on three areas that are loosely derived
from Coghlan and Brannick's (2014) thinking: premise
(reflection on underlying assumptions and perspectives,
whether unstated or even unconscious; content (reflection on
what was constructed or planned); and process (reflection on
how it was implemented and evaluated). We believe that
reflexivity has to be at the core of this meta-level reflective
review with evaluators consciously attempting to question
their own actions and thoughts, reflecting upon what, why,
and how they have been doing things (as supported by Tolich
& Davidson, 2011).
Piggot-Irvine and Zornes 11
In the ESAR, we continuously recorded own perspectives
on the effectiveness of our implementation of the EvAR
framework elements. Such meta-reflection was critical to us
as action researchers. Just as one example, we appointed
one ESAR team member to coordinate the recording of
reflections among the group at the end of every team meeting
over the period of three years and these reflections are cur-
rently being summarized as part of our reporting phase.
Furthermore, because authentic collaboration with stake-
holders was critical for the study, in all interactions we con-
sciously attempted to question our own actions and thoughts,
reflecting upon what, why, and how we were doing things by
seeking feedback continuously from the AR community.
Phase 5: Report Achievements,
Recommendations, Knowledge
Mobilization
Generally, the approach to communicating findings is
planned early in research and elaborated in the research eval-
uation framework. The communication plan in the frame-
work needs to consider where, how, and from whom attention
is sought; be attention grabbing by clarifying what is distinc-
tive, or what problem is being explored and why it is impor-
tant; clarify the context of the message; be open about the
beliefs and purpose of the research and the beliefs of those
receiving the message; explain the effort that has gone into
the research; clarify how dialogue can be created around the
topic; use multiple channels for delivering the message; and
note whether the results can be shown to be trustworthy
(Minto, 1987). Both logical and rational messages, as well as
emotional, should be considered. Increasingly, in social sci-
ence research, opportunities to "tell the story" (Universities
Canada, 2008) are outlined in a knowledge mobilization
plan.
If collaborative activity is featured in a framework, the
following caution about communication from Boyd, Buizer,
Schibeci, and Baudains (2015) might be considered:
. . . in spite of ubiquitous participation-rhetoric, in the ways that
researchers perform communication about their projects, a
common normalized expression like "knowledge transfer"
implies a role for the researcher as the "holder" of knowledge,
and a role for publics as receivers of knowledge. Also, in this
view of knowledge as something that can be transferred,
knowledge is sitting out there, waiting to be discovered and
distributed, rather than being relational, and evolving in
interaction between different actors. (p. 177)
In the ESAR, the approach to early and continuous, rather
than just endpoint, reporting and knowledge mobilization
was articulated. We noted that the study itself reflected
knowledge mobilization at two levels. First, the collabora-
tion among researchers was designed to ensure enhanced
accessibility, flow, and exchange of knowledge. Second, the
dialogical approach to engaging respondent/participant
input was deliberate in terms of practitioner­researcher­
organization­community flow of information and enhance-
ment of ownership of improvement rather than us "holding"
the knowledge. As a result of the collaboration and the dia-
logical approach to engagement, we have already been able
to report that varied levels of networks have developed
(Zornes et al., 2016), including those among the ESAR team,
within each of the individual projects studied, between and
among the leads and participants of projects in the study,
and in the larger AR community outside of the team and proj-
ect participants.
We also articulated how results from the ESAR on validity
claims for AR as an approach to change could be reported
out to a wide audience via journals, books, conferences, and
forums such as press releases and nonacademic media
(newsletters, podcasts, listservs, webinars, and blogs).
Furthermore, we noted the specific annual research dissemi-
nation workshops we could offer to our respective universi-
ties. Knowledge mobilization was also planned to occur
through incorporation of process and findings in curriculum
material for postgraduate courses taught, and theses super-
vised, by members of the research team. We have already met
a considerable number of our planned targets for mobiliza-
tion with multiple journal articles, conference presentations,
and workshops presented.
Phase 6: Continued Action for
Improvement
We found no mention in traditional frameworks of evaluators
encouraging enhanced or ongoing actions for improvement
associated with the activity of the evaluators themselves, the
process steps used in the framework, or the context evaluated.
Like AR, at almost every stage of the EvAR, there are implica-
tions and expectations of continuing action for improvement.
For example, the preparatory phase activities associated with
establishing values and protocols have been designed as a
guide for the evaluation team to continuously improve their
own practice. In the reconnaissance phase, the probing litera-
ture review has not been articulated as a one-off early evalua-
tive activity, but rather the literature could be continuously
updated throughout the research evaluation. In the implementa-
tion phase, the flexibility underpinning ofAR has been deliber-
ately inserted as part of the framework to create an imperative
for ongoing, iterative, piloting, checking, reviewing, and updat-
ing of all the constituting elements of the framework. Reflection
and reflexivity have been stipulated as central features of the
EvAR and the continuation arrow shown in Figure 1 indicates
that ongoing action is likely to result.
All of the noted ongoing actions for improvement occurred in
the ESAR project and we have discussed many of those actions in
previous sections of this article. In particular, flexibility has been
strongly present in our emphasis on communicating with, and
seeking feedback from, stakeholders to improve our approaches;
continually piloting and updating methods; employment of
12 SAGE Open
reflective and reflexive processes for checking progress and cre-
ating new thinking; openly mobilizing and sharing our findings
throughout the study; and actively encouraging dialogue about
findings. This article is an example of the latter.
Conclusion
In this article, we have presented EvAR as a research evalu-
ation framework that has the flexibility to meet the challenge
of the complex needs of evaluation of a large applied research
study. The framework has been designed to meet a dual over-
arching need to engage stakeholders in a responsive,
improvement oriented, evaluation process, and to be respon-
sive to increasing accountability demands to track, demon-
strate, and measure the impacts and outcomes of research
(Conteh, 2013; Giroux & Giroux, 2009; Popp et al., 2014).
We have argued that a flexible model such as EvAR could
ensure the responsivity required of the varied boundary part-
ners in a context such as the ESAR. Furthermore, we have
suggested that a framework underpinned by values and prin-
ciples ofAR (Greenwood & Levin, 2007; Reason & Bradbury,
2001; Stringer, 2014) could align well with a context of
research evaluation where engagement and ownership of find-
ings are important. The authentic collaboration (Piggot-Irvine,
2012a) approach based on nondefensive (Argyris, 2003), dia-
logical, strategies is central to such engagement and genuinely
open interactions. We hope that we have demonstrated that the
collaborative approach must also be deeply embedded within
the practice of the research evaluators themselves and we have
attempted to illustrate such practice within the ESAR.
In keeping with Phase 6 of the EvAR, we have written this
article to not only share our thinking but also, most impor-
tantly, to invite response. We welcome your input.
Acknowledgments
With thanks to the ESAR team: Phil Cady, Lesley Ferkins, Wendy
Rowe, Shankar Sankaran, Judith Kearney, Bernard Schissel, Maria
Anderson.
Declaration of Conflicting Interests
The author(s) declared no potential conflicts of interest with respect
to the research, authorship, and/or publication of this article.
Funding
The author(s) disclosed receipt of the following financial support for
the research and/or authorship of this article: This work was supported
by the Social Science and Humanities Research Council of Canada,
SSHRC [grant number 611-2012-0274].
References
Aberatne, A. M. (2010). Using the most significant change sto-
ries: A learning history from COMPAS Sri Lanka. Retrieved
from http://www.seachangecop.org/sites/default/files/docu-
ments/2010%2010%20MSC%20Stories%20-%20a%20
Learning%20History.pdf
Ackermann, F., & Eden, C. (2011). Strategic management of stake-
holders: Theory and practice. Long Range Planning, 44, 179-196.
Adams, J., Khan, H. T. A., Raeside, R., & White, D. (2007).
Research methods for graduate business and social science
students. New Delhi, India: SAGE. Retrieved from http://site.
ebrary.com/id/10272446
Argyris, C. (2003). A life full of learning. Organization Studies, 24,
1178-1192. doi:10.1177/01708406030247009
Axelrod, P. (2002). Values in conflict: The university, the market-
place, and the trials of liberal education. Montreal, Quebec,
Canada: McGill-Queen's University Press.
Baxter, P., & Jack, S. (2008). Qualitative case study methodology:
Study design and implementation for novice researchers. The
Qualitative Report, 13, 544-599.
Bergmann, M., Brohmann, B., Hoffmann, E., Loibl, M. C., Rehaag,
R., Schramm, E., & Voss, J.-P. (2005). Quality criteria of trans-
disciplinary research: A guide for the formative evaluation of
research projects (Central report of Evalunet--Evaluation
Network for Transdisciplinary Research). Frankfurt am Main,
Germany: Institute for Social-Ecological Research. Retrieved
from http://www.isoe.de/ftp/evalunet_guide.pdf
Boyd, D., Buizer, M., Schibeci, R., & Baudains, C. (2015).
Prompting transdisciplinary research: Promising futures for
using the performance metaphor in research. Futures, 65,
175-184.
Brown, J., & Isaacs, D. (2005). The World Café: Shaping our
futures through conversations that matter. San Francisco, CA:
Berrett-Koehler.
Bryman, A., & Bell, E. (2011). Business research methods (2nd
ed.). Oxford, UK: Oxford University Press.
Butler, L. (2008). Using a balanced approach to bibliometrics:
Quantitative performance measures in the Australian Research
Quality Framework. Ethics in Science and Environmental
Politics, 8, 83-92.
Buxton, M., & Hanney, S. (1996). How can payback from health
services research be assessed? Journal of Health Services
Research & Policy, 1, 35-43.
Buxton, M., Hanney, S., & Jones, T. (2004). Estimating the eco-
nomic value to societies of the impact of health research: A
critical review. Bulletin of the World Health Organization, 82,
733-739.
Cardno, C. (2003). Action research: A developmental approach.
Wellington: New Zealand Council for Educational Research.
Carew, A. L., & Wickson, F. (2010). The TD wheel: A heuris-
tic to shape, support and evaluate transdisciplinary research.
Futures, 42, 1146-1155.
Carroll, W. K. (2003). Undoing the end of history: Canada-centred
reflections on the challenge of globalisation. In Y. Atasoy & W.
K. Carroll (Eds.), Global shaping and its alternatives (pp. 33-56).
Toronto, Ontario, Canada: Garamond Press.
Coghlan, D., & Brannick, T. (2010). Doing action research in your
own organization (3rd ed.). Thousand Oaks, CA: SAGE.
Coghlan, D., & Brannick, T. (2014). Doing action research in your
own organization (4th ed.). Thousand Oaks, CA: SAGE.
Conteh, C. (2013). Strategic inter-organizational cooperation in
complex environments. Public Management Review, 15,
501-521.
Creswell, J. W. (2009). Research design: Qualitative, quantitative
and mixed methods approaches (2nd ed). Thousand Oaks, CA:
SAGE.
Piggot-Irvine and Zornes 13
Creswell, J. W., & Plano Clark, V. L. (2011). Designing and con-
ducting mixed methods research (2nd ed.). Thousand Oaks,
CA: SAGE.
D'Arcy, P. (1994, Spring). On becoming an action researcher--
Who qualifies? Plus ca change? Action Researcher, 1, 1-3.
Davison, R., Martinsons, M. G., & Kock, N. (2004). Principles of
canonical action research. Information Systems Journal, 14,
65-86. doi:10.1111/j.1365-2575.2004.00162.x
Defila, R., & Di Giulio, A. (1999). Evaluating transdisciplinary
research. Newsletter of the Swiss Priority Programme environ-
ment. Berne: Swiss National Science Foundation.
Dehler, G. E., & Edmonds, R. K. (2006). Using action research
to connect practice to learning: A course project for working
management students. Journal of Management Education, 5,
636-669. doi:10.1177/1052562905277302
de Jong, S. P. L., van Arensbergen, P., Daemen, F., van der Meulen,
B., & van den Besselaar, P. (2011). Evaluation of research in
context: An approach and two cases. Research Evaluation, 20,
61-72.
Dick, B. (2004). Action research literature: Themes and trends.
Action Research, 2, 425-444. doi:10.1177/1476750304047985
Dick, B., Sankaran, S., Shaw, K., Kelly, J., Soar, J., Davies, A.,
& Banbury, A. (2015). Value co-creation with stakehold-
ers using action research as a meta-methodology in a funded
research project. Project Management Journal, 46(2), 36-46.
doi:10.1002/pmj.21483
Donovan, C. (2006, November). Visible gains from research.
The Australian. Retrieved from http://www.theaustralian.
com.au/higher-education/visible-gains-from-research/story-
e6frgcjx-1111112447265
Donovan, C. (2008). The Australian Research Quality Framework:
A live experiment in capturing the social, economic, envi-
ronmental, and cultural returns of publicly funded research
[Special issue]. New Directions for Evaluation, 2008, 47-60.
doi:10.1002/ev.260
Donovan, C., & Hanney, S. (2011). The "payback framework"
explained. Research Evaluation, 20, 181-183.
Durlak, J. A., & DuPre, E. P. (2008). Implementation matters:
A review of research on the influence of implementation on
program outcomes and the factors affecting implementation.
American Journal of Community Psychology, 41, 327-350.
doi:10.1007/s10464-008-9165-0
Duryea, M., Hochman, M., & Parfitt, A. (2007, February).
Measuring the impact of research. Research Global, pp. 8-9.
Earl, S., Carden, F., & Smutylo, T. (2001). Outcome mapping--
Building learning and reflection into development programs.
Ottawa, Ontario, Canada: International Development Research
Centre.
Giroux, H. A., & Giroux, S. S. (2006a). Challenging neoliberal-
ism's new world order: The promise of critical pedagogy.
Cultural Studies  Cultural Methodologies, 6, 21-32.
Giroux, H. A., & Giroux, S. S. (2006b). Take back higher educa-
tion. New York, NY: Palgrave Macmillan.
Giroux, H. A., & Giroux, S. S. (2009). Beyond bailouts: On the
politics of education after neoliberalism. Policy Futures in
Education, 7, 1-4.
Gosling, J., & Mintzberg, H. (2006). Management educa-
tion as if both matter. Management Learning, 37, 419-428.
doi:10.1177/1350507606070214
Greenwood, D. (2014). Pragmatic action research. In D.
Coghlan & M. Brydon-Miller (Eds.), SAGE encyclopedia
of action research (pp. 645-648). London, England: SAGE.
doi:10.4135/9781446294406.n286
Greenwood, D., & Levin, D. (2007). Introduction to action research
(2nd ed.). Thousand Oaks, CA: SAGE.
Guthrie, S., Wamae, W., Diepeveen, S., Wooding, S., & Grant, J.
(2013). Measuring research: A guide to research evaluation
frameworks and tools (Report prepared for RAND Europe).
Retrieved from http://www.rand.org/pubs/monographs/
MG1217.html
Hemlin, S. (2006). The shift in academic quality control. Science,
Technology & Human Values, 31, 173-198.
Huutoneimi, K. (2010). Evaluating interdisciplinary research.
In R. Frodermann, J. T. Klein, & C. Mitcham (Eds.), The
Oxford handbook of interdisciplinarity (pp. 309-320). London,
England: Oxford University Press.
Huutoneimi, K., & Tapio, P. (Eds.). (2014). Transdisciplinary sus-
tainability studies: A heuristic approach. London, England:
Routledge.
Ivankova, N. V. (2015). Mixed methods application in action
research. Thousand Oaks, CA: SAGE.
Ivankova, N. V., Creswell, J. W., & Stick, S. L. (2006). Using
mixed-methods sequential explanatory design: From theory
to practice. Field Methods, 18, 3-20. doi:10.1177/15258
22X05282260
Jahn, T., & Keil, F. (2015). An actor-specific guideline for
quality assurance in transdisciplinary research. Futures, 65,
195-208.
Kellogg Foundation (2001). W.K. Kellogg Foundation logic model
development guide. Retrieved from http://www.exinfm.com/
training/pdfiles/logicModel.pdf
Kemmis, S. (2010). What is to be done? The place of action
research. Educational Action Research, 18, 417-427. doi:10.10
80/09650792.2010.524745
Kemmis, S., & McTaggart, R. (Eds.). (1990). Action research
reader. Geelong, Australia: Deakin University Press.
Klein, J. T. (2006). Afterword: The emergent literature on interdis-
ciplinary and transdisciplinary research evaluation. Research
Evaluation, 15, 75-80.
Klein, J. T. (2008). Evaluation of interdisciplinary and transdis-
ciplinary research: A literature review. American Journal of
Preventive Medicine, 35(Suppl. 2), S116-S123. doi:10.1016/j.
amepre.2008.05.010
Koshy, E., Koshy, V., & Waterman, H. (2011). Action research in
healthcare. London, England: SAGE.
Kurtz, C., & Snowden, D. (2003). The new dynamics of strategy:
Sense-making in a complex and complicated world. IBM
Systems Journal, 42, 462-483. doi:10.1147/sj.423.0462
Latham, G. P., & Locke, E. A. (2006). Enhancing the benefits
and overcoming the pitfalls of goal setting. Organizational
Dynamics, 35, 332-340. doi:10.1016/j.orgdyn.2006.08.008
Macpherson, I., & Brooker, R. (1999). Communicating the
processes and outcomes of practitioner research: An
opportunity for self-indulgence or a serious professional
responsibility. Educational Action Research, 7, 207-221.
doi:10.1080/09650799900200091
McNiff, J. (1988). Action research: Principles and practice.
Basingstoke, UK: Macmillan.
14 SAGE Open
Metcalfe, M. (2008). Pragmatic inquiry. Journal of the Operational
Research Society, 59, 1091-1099. doi:10.1057/palgrave.
jors.2602443
Meyer, J. (2000). Evaluating action research. Age and Aging, 29,
8-10.
Meyrick, J. (2006). What is good qualitative research? A first step
towards a comprehensive approach to judging rigour/quality.
Journal of Health Psychology, 11, 799-808.
Minto, B. (1987). The pyramid principle: Logic in writing and
thinking. London, England: Prentice Hall.
Mitchell, C. A., & Willetts, J. R. (2009, September). Quality
criteria for inter and trans-disciplinary doctoral research
outcomes. Prepared for ALTC Fellowship: Zen and the
Art of Transdisciplinary Postgraduate Studies, Institute for
Sustainable Futures, University of Technology Sydney,
Australia.
Molyneux, C., Koo, N., Piggot-Irvine, E., Talmage, A., Travaglia,
R., & Willis, M. (2012). Doing it together--Collaborative
research on goal-setting and review in a music therapy centre.
New Zealand Journal of Music Therapy, 10, 6-38.
Parker, J., & van Teijlingen, E. (2012). The Research Excellence
Framework (REF): Assessing the impact of social work
research on society. Practice, 24, 41-52. doi:10.1080/095031
53.2011.647682
Penfield, T., Baker, M. J., Scoble, R., & Wykes, M. C. (2014).
Assessment, evaluations, and definitions of research impact: A
review. Research Evaluation, 23, 21-32.
Phillipson, J., Lowe, P., Proctor, A., & Ruto, E. (2012). Stakeholder
engagement and knowledge exchange in environmental
research. Journal of Environmental Management, 95, 56-65.
Piggot-Irvine, E. (2008). Meta-evaluation of action research in a
school leadership programme. In E. Piggot-Irvine & B. Bartlett
(Eds.), Evaluating action research (pp. 147-166). Wellington:
New Zealand Council For Educational Research.
Piggot-Irvine, E. (2012a). Creating authentic collaboration: A
central feature of effectiveness. In O. Zuber-Skerritt (Ed.),
Action research for sustainable development in a turbulent
world (pp. 89-107). Bingley, UK: Emerald.
Piggot-Irvine, E. (2012b, October). Evaluation using a collab-
orative action research approach. Presentation to British
Columbia Canadian Evaluation Society AGM, Vancouver,
British Columbia, Canada.
Piggot-Irvine, E., & Bartlett, B. (2008). Evaluating action research.
Wellington: New Zealand Council For Educational Research.
Piggot-Irvine, E., Connelly, D., Curry, R., Hanna, J., Moodie,
M., Palmer, M., . . . Thompson, A. (2011). Building leader-
ship capacity--Sustainable leadership [Monograph]. Action
Research Action Learning Association, 2, 1-40.
Piggot-Irvine, E., Rowe, W., & Ferkins, L. (2015). Conceptualizing
indicator domains for evaluating action research. Educational
Action Research, 23, 545-566. doi:10.1080/09650792.2015.1
042984
Popp, J. K., Milward, H. B., Mackean, G., Casebeer, A., &
Lindstrom, R. (2014). Inter-organizational networks: A review
of the literature to inform practice (Report for IBM Center for
the Business of Government). Washington, DC: IBM Center
for the Business of Government.
Preskill, H., & Torres, R. (1999). Evaluative inquiry for learning in
organizations. Thousand Oaks, CA: SAGE.
Rasmussen, E. (2008). Government instruments to support the com-
mercialization of university research: Lessons from Canada.
Technovation, 28, 506-517.
Reason, P., & Bradbury, H. (2001). Introduction: Inquiry and par-
ticipation in search of a world worthy of human aspiration. In
P. Reason & H. Bradbury (Eds.), Handbook of action research
(pp. 1-14). Thousand Oaks: CA: SAGE.
Roach, A. T., & Elliott, S. N. (2005). Goal attainment scaling: An
efficient and effective approach to monitoring student prog-
ress. Teaching Exceptional Children, 37(4), 8-17.
Rowe, W. E., Graf, M., Agger-Gupta, N., Piggot-Irvine, E., &
Harris, B. (2013). Action research engagement: Creating the
foundations for organizational change [Monograph]. Action
Research Action Learning Association, 5, 1-46.
Sankaran, S., Tay, B. H., & Orr, M. (2009). Managing organiza-
tional change by using soft systems thinking in action research
projects. International Journal of Managing Projects in
Business, 2, 179-197. doi:10.1108/17538370910949257
Schön, D. (1983). The reflective practitioner: How professionals
think in action. New York, NY: Basic Books.
Smudde, P. M., & Courtright, J. L. (2011). A holistic approach
to stakeholder management: A rhetorical foundation. Public
Relations Review, 37, 137-144.
Spaapen, J., Dijstelbloem, H., & Wamelink, F. (2007). Evaluating
research in context: A method for comprehensive assessment.
The Netherlands: Consultative Committee of Sector Councils
for Research and Development.
Spaapen, J., & van Drooge, L. (2011). Introducing "productive
interactions" in social impact assessment. Research Evaluation,
20, 211-218.
Srivastava, P., & Hopwood, N. (2009). A practical iterative frame-
work for qualitative data analysis. International Journal of
Qualitative Methods, 8, 76-84.
Stringer, E. T. (2014). Action research (3rd ed.). Thousand Oaks,
CA: SAGE.
Tolich, M., & Davidson, C. (2011). Getting started: An introduc-
tion to research methods. Auckland, New Zealand: Pearson
Education.
Tremblay, C., & Hall, B. L. (2014). Learning from community-
university research partnerships: A Canadian study on commu-
nity impact and conditions for success. International Journal
of Action Research, 10, 376-404.
Tripp, D. (1990). Socially critical action research. Theory Into
Practice, 29, 158-166.
Universities Canada. (2008). Momentum: The 2008 report on uni-
versity research and knowledge mobilization. Retrieved from
http://www.univcan.ca
Wadsworth, Y. (1998). What is participatory action research?
Action Research International, Paper 2. http://www.scu.edu.
au/schools/gcm/ar/ari/p-ywadsworth98.html
Wadsworth, Y. (2011). Building in research and evaluation: Human
inquiry for living systems. Walnut Creek, CA: Left Coast Press.
Wicks, P., & Reason, P. (2009). Initiating action research:
Challenges and paradoxes of opening communicative space.
Action Research, 7, 243-262.
Wickson, F., & Carew, A. L. (2014). Quality criteria and indica-
tors for responsible research and innovation: Learning from
transdisciplinarity. Journal of Responsible Innovation, 1,
254-273.
Piggot-Irvine and Zornes 15
Winter, R. (1987). Action-research and the nature of social inquiry:
Professional innovation and educational work. Aldershot, UK:
Gower.
Yin, R. K. (2003). Case study research: Design and methods (3rd
ed.). Thousand Oaks, CA: SAGE.
Youngs, H., & Piggot-Irvine, E. (2012). The Application of a
multiphase triangulation approach to mixed methods: The
research of an aspiring school principal development pro-
gram. Journal of Mixed Methods Research, 6, 184-198.
doi:10.1177/1558689811420696
Youngs, H., & Piggot-Irvine, E. (2014). The merits of triangula-
tion: The evaluation of a New Zealand school leadership
development program using mixed-methods research. Method
in Action Case Studies. SAGE Research Methods Cases.
doi:10.1177/1558689811420696
Zornes, D. (2012). The business of the university: Research, its place in
the "business," and the role of the university in society (Master's
thesis). University of Victoria, British Columbia, Canada.
Zornes, D., Ferkins, L., & Piggot-Irvine, E. (2016). Action
research networks: Role and purpose in the evaluation
of research outcomes and impacts. Educational Action
Research, 24, 97-114. doi:10.1080/09650792.2015.1045538
Author Biographies
Eileen Piggot-Irvine is professor of leadership at Royal Roads
University (Victoria, Canada) and an adjunct professor at
Griffith University (Brisbane). Among other things, she was for-
merly vice president of the international association Action
Learning Action Research Association (ALARA), established
the New Zealand Action Research Network, and is the director
of the New Zealand Action Research and Review Centre
(NZARRC). She has published six books, multiple book chap-
ters, 62 refereed journal articles, and presented too many key-
notes, and so on to count.
Deborah Zornes is the director of Research Services at Royal
Roads University and holds a PhD in interdisciplinary studies.
Her research interests include the role of the university it society,
research impacts, research outcomes, research evaluation, and
research measures.
