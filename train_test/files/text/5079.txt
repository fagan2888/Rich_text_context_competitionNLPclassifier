SAGE Open
1
­9
© The Author(s) 2011
DOI: 10.1177/2158244011408618
http://sgo.sagepub.com
Explanation and Expert Systems
Introduction
Much had been written in the 1980s about the role of expla-
nation facilities in medical diagnostic expert systems:
MYCIN (Clancey, 1983; Shortliffe, 1981) and PUFF (Aikins,
Kunz, Shortliffe, & Fallat, 1983) were both prototype medi-
cal expert systems of interest because of their explanation.
However, despite being widely recognized as a useful
adjunct to expert systems, explanation facilities have been
largely ignored in the health care literature in recent years.
This is partly because explanation facilities were first used
in diagnostic applications, such as the MYCIN expert sys-
tem and its derivatives in the 1980s, but the clinical tasks
served by expert systems have changed considerably since
this time (Coeira, 2003; Friedman & Wyatt, 1997; Hanson,
2006). Currently, expert system tasks are more likely to be
used in the determination of drug dosing and drug prescrib-
ing or in reminding clinicians to engage in preventive inter-
ventions through inoculations. Indeed, Hunt, Haynes, Hanna,
and Smith (1998) published a systematic review of clinical
trials of clinical expert systems, which found clear evidence
for the effectiveness of such systems with 43 out of 65 trials
showing an improvement of clinician performance. These
trials involved a variety of expert system tasks that included
diagnostic aid, preventive care, and reminder systems.
However, only 20% of the diagnostic aid trials in this study
were effective, as compared with74% of trials based on
preventive care and reminder systems. A more recent study
by Garg et al. (2005) shows that clinical expert systems
improved practitioner performance in 62 (64%) of the 97
studies assessing this outcome. Of these trials, 21 were
based on reminder systems, with 16 of them (76%) deemed
to be successful, and 10 of these trials were diagnostic based
with 4 (40%) successful. This improvement in the use of
diagnostic expert systems reflects an increasing trend in
recent years as will be shown later in this article.
Benefits of Explanation Facilities in
Clinical Expert Systems
Very little attention seems to have been given to the ways in
which explanation facilities could support modern expert
system medical tasks. This is surprising, for Walton (1996)
presents evidence to suggest that advice from a computer
will be more convincing if supported with explanation
facilities. In their evaluation of CAPSULE, an expert system
giving advice to general practitioners about prescribing
408618
XXXXXX10.1177/2158
244011408618DarlingtonSAGE Open
1London South Bank University, London, England
Corresponding Author:
Keith W. Darlington, Senior Lecturer in Artificial Intelligence and
Computing, Knowledge Based Systems Centre, Department of Informatics,
London South Bank University, 103 Borough Rd., London, SE1 OAA,
England
Email: keithd@lsbu.ac.uk
Designing for Explanation in Health Care
Applications of Expert Systems
Keith W. Darlington1
Abstract
A great deal has been written about the role of clinical decision support systems in medicine in recent years--an important
category of which are expert systems. Expert systems would normally contain an explanation module--the subject of a
great deal of research interest in the 1980s when the main problem-solving task for medical expert systems was diagnostics.
However, expert systems nowadays are more likely to perform tasks other than diagnosis, yet the role of explanation in
expert systems has been largely ignored in the health care literature since this time. Furthermore, user requirements can
vary considerably in the health care domain and may include physicians, medical researchers, administrators, and patients.
Such user groups would have differing levels of knowledge and goals,which would impact on the type of explanatory support
provided by the system.This article examines the potential benefits of explanation facilities for a range of clinical tasks and
also considers the ways in which explanation facilities may be delivered so as to be of benefit to these categories of health
care user for these tasks.
Keywords
expert systems, explanation, decision support, stakeholders
2 SAGE Open
drugs, they say that "finding the most effective way of pre-
senting the explanation is an important goal for future studies
of computer support for prescribing drugs." Many other stud-
ies have demonstrated the importance of a system being able
to explain its own reasoning. For example, in a study of physi-
cian's expectations and demands for computer-based consul-
tation systems, it was found that explanation was the single
most important requirement for advice giving systems in
medicine (Buchanan & Shortliffe, 1984). Moreover, accord-
ing to Berry, Gillie, and Banbury (1995), explanation is seen
as a vital feature of expert systems--particularly in high-risk
domains, such as medicine, where users need to be convinced
that a system's recommendations are based on sound and
appropriate reasoning. The inclusion of explanation facilities
can enhance the quality of the decision making. Gregor and
Benbasat (1999) have shown that well-designed explanation
facilities could improve user acceptance and performance in
terms of speed and accuracy of judgments. Furthermore,
explanation facilities can lead to greater adherence to the rec-
ommendations of the expert system (Darlington, 2008;
Gregor & Benbasat, 1999). In addition, according to Friedman
and Wyatt (1997), expert systems raise complex and profes-
sional issues, in that, to avoid exposure to liability, every
expert system must treat its users as a "learned intermediary."
Consequently, opaque programs such as neural networks are
clinically doubtful as compared with the transparency offered
by explanation facilities.
This article briefly examines the technical means by
which explanation facilities can be implemented in expert
systems and the components of explanation. Explanation
facilities are then considered with regard to a range of
problem-solving tasks in the health care domain as well as
discussing user requirements for explanation with regard to
the range of users in this domain. These may include physi-
cians, medical researchers, administrators, and patients. The
next section looks at the reasons for the low usage of expla-
nation facilities and suggests ways to increase usage.
Usage of Explanation Facilities in
Expert Systems
Explanation facilities can be viewed as an optional feature
for expert systems without interfering with the operation of
the underlying system, and although potentially very useful,
research indicates that usage of explanation facilities is fre-
quently very low. For example, a study by Dhaliwal (1996)
found that only 25% to 30% of available explanations were
requested during consultations with an expert system,
whereas Everett (1994) did a study that found only about
half of the participants chose to view explanations at all.
Some ways in which the developer can improve the relative
inattention given to explanation facilities are described in
more detail later in this article and include the access mech-
anism (see section titled "The Communication"), the content
of the explanation delivery (see section titled "Explanatory
Content"), and the adaptation to the user by delivering the
appropriate level of abstraction (see section titled "User
Adaptation"; Berry et al., 1995). Moreover, according to
Clancey (1983) and Gregor and Benbasat (1999), the default
explanation types in rule-based expert systems are inade-
quate and should be enhanced with other explanatory con-
tent (see section titled "Explanatory Content"). These
shortcomings can be improved with some effort from the
system developer without necessarily restructuring the
expert system knowledge base--which could be very time-
consuming. However, Berry et al. (1995) provide another
reason for the low frequency of explanation use, in that,
system developers often fail to involve users in the specifi-
cation and evaluation of explanation facilities in the early
stages of project development, but explanation facilities are
strongly correlated with the structure and knowledge content
in an expert system (Wick, 1992b). This means that the
knowledge sources adopted for developing an expert system
will affect the quality of the explanation facilities provided
by that system. As Wyatt (1997) says, "All too often in the
past the knowledge used for decision support expert systems
has been acquired by a knowledge engineer from a single
expert--or even by browsing out of date narrative text-
books, with all their defects" (p. 167). Understanding the
difficulties of implementing explanation facilities in expert
systems requires knowledge of the way that both compo-
nents work together as described in the next sections.
Explanation in Clinical
Expert Systems
Characteristics of Clinical Expert systems
The sections that follow will briefly examine some of the
ways that knowledge is represented in expert systems fol-
lowed by a description of the way in which explanation
facilities would normally integrate with components of an
expert system, before considering the types of explanations
suitable in the health care domain.
Representing Knowledge. Several knowledge representa-
tional models have been used to construct clinical expert sys-
tems, including symbolic methods, such as simple decision
trees, statistical/probabilistic methods, and rule-based and
frame (descriptive logic) based expert systems (Peleg & Tu,
2006; Van Bemmel & Musen, 1997). Probabilistic methods
are also quite common and include certainty factors (Short-
liffe, 1981) and belief networks, based on probability theory,
such as Bayesian networks (Korver & Lucas, 1993; Lacave
& Diez, 2004). Machine learning methods using rule induc-
tion, case-based reasoning, genetic algorithms, or even a
combination of these techniques can also be used to develop
the knowledge bases used by medical expert systems (Coeira,
2003; Cunningham, Doyle, & Loughrey, 2003). These tech-
niques can be used in the medical domain by, for example,
using a set of clinical cases that act as examples from which
Darlington 3
a machine learning system can then produce a systematic
description of those clinical features that uniquely character-
ize the clinical conditions. This knowledge can be expressed
in the form of rules to represent the knowledge in an expert
system (Coeira, 2003). Neural networks provide another
machine learning technique suited to expert system develop-
ment but are not well suited to provide explanations because
of their "black-box" opaque nature of operation (Cunning-
ham et al., 2003).
Rule-based expert systems and frame (descriptive logic)
based are better-suited representations for explanation as
their inferences can provide transparency because of the
explicit way in which the knowledge is represented.
A Typical Rule-Based Expert System Architecture. Fig-
ure 1 describes an archetypal expert system component
structure. An end user would communicate with the system
via a user interface and an explanation facility that would
interact with an expert system inference engine. The infer-
ence engine would use medical domain knowledge stored in
the knowledge base and control the consultation by deter-
mining the questions that are to be asked to achieve its goals
and derive conclusions or specify actions to be taken--such
as proposed therapies and so on. The explanation component
would combine with the inference engine to provide expla-
nations that could follow a consultation and provide postad-
vice explanations--called feedback. Explanations before
advice--that is, during the question input phase--could also
be generated, called feedforward explanations. The latter
provides the user with a means to find out why a question is
being asked during a consultation (i.e., during the data input
stage). Feedforward explanations would frequently take the
form of a description of technical terms (terminological) to
enable the user to answer the question(s) in a meaningful
way. Feedforward explanations are general in that they are
not dependent on any particular output case. By contrast,
feedback explanations are case specific in that they will
normally present a trace of the rules that were invoked dur-
ing a consultation and display intermediate inferences to
arrive at a particular conclusion. Feedback explanations pro-
vide the user with a record of problem-solving action during
a consultation to that the user can see how a conclusion was
reached when the data have been completely input. Figure 1
also shows how the explanation facility is dependent on the
expert system knowledge base.
Explanation Properties
There are three main components of an explanation that are
to be considered by the designer. They are the communica-
tion, the adaptation to the user, and the content of an expla-
nation (Lacave & Diez, 2004).
The Communication. As Figure 1 shows, the explanation
facility is a component of a rule-based expert system inter-
face. The designer will therefore need to consider issues
such as the language dialogue and style of presentation--
that is, graphical, textual, audio, or some combination as
well as the access provision mechanism of the explanatory
component. Three types of provision mechanisms are pos-
sible: embedded, automatic, and user invoked. Embedded
explanations would be permanently visible on the display
but can result in confusion and consume valuable screen
space. However, automatic explanations are automatically
invoked as determined by the system (Gregor & Benbasat,
1999). They could be invoked as a result of some action
taken by the user during a consultation. User-invoked
explanations are the most commonly used in practice for
they appear when they are explicitly requested by the user
and could be implemented using hypertext links or other
interface commands (Mao & Benbasat, 2000). Payne, Bet-
tman, and Johnson (1993) have conducted research in the
usage of user-invoked explanations and from their studies
have proposed the cognitive effort perspective theory. This
suggests that expert system users will only invoke explana-
tions voluntarily if the perceived benefits in accessing them
are outweighed by the amount of mental effort in doing so.
This theory suggests good reasons for the relative inatten-
tion to explanation facilities in the absence of any specific
trigger for their use. Consequently, careful thought has to
be given to the design of the access mechanism (Gregor,
1996a). The main triggers for user-invoked explanations
(Dhaliwal, 1996; Gregor & Benbasat, 1999) have been
identified as follows:
1. A desire for long-term learning--normally trig-
gered by novice users. In the health care domain,
this category might include patients.
2. A strong disagreement with the advice given by the
system--this type of trigger is normally invoked
by experts whose views may conflict with that of
the system advice.
Clinical
Expert
Clinical
User User Interface
Inference
Mechanism
Explanaon
Facility
Knowledge
acquision
facility
Knowledge
base
Figure 1. Relationship between an expert system and
explanation component
4 SAGE Open
3. The complexity of the task itself--normally trig-
gered by novice users who will often need explan-
atory support with tasks involving perhaps more
complicated procedures or the use of more com-
plex technical jargon.
User Adaptation. An explanation designer has to consider
the recipient of an explanation, in terms of his or her knowl-
edge and expectations. Unfortunately, default expert system
explanations do not take into account the variability of
knowledge between different users. User modeling refers to
the process of managing these differences. One possible
solution to this problem of user modeling has been advo-
cated by (Cawsey, 1993). The technique involves a dialog
planning method for the generation of interactive explana-
tions. The method consists of not only planning the text to be
presented to the user but also planning the dialog with the
user, based on the retaining of a model of the user's knowl-
edge, which would be updated during the explanatory inter-
action between the system and the user. One prominent
project incorporating a user model was OPADE (Carolis
et al., 1996). This was a European Community Project­
funded expert system for generating beneficiary-centered
explanations about drug prescriptions that takes into account
the user requirements. The main objective of OPADE was to
improve the quality of drug treatment by supporting the phy-
sician in their prescription process and by increasing compli-
ance with the clinical guidelines (Berry et al., 1995). OPADE
supports two types of users: those who directly interact with
the system such as general practitioners and nurses, and
those who receive a report of results--that is, the patients.
The explanations that are generated are dynamic (unlike
static canned text explanations) in that a "user model" is
maintained containing the characteristics of the user. A "text
planner" component plans the discourse during a consulta-
tion. The text planner will build a tree containing the dis-
course plan, which will depend on the objectives that are to
be met by the user model. The explanation is then delivered
in natural language by taking the tree generated by the text
planner as input and transforming it using text phrases into
the appropriate format.
Explanatory Content. Early (first generation) expert sys-
tem explanation facilities were characterized by design
incorporating one or more of the following types of explana-
tion (Chandrasekaran, Tanner, & Josephson, 1989).
Rule-trace explanations. As its name suggests, the rule-
trace explanation links problem-solving knowledge with an
explanation of a trace of rules that were invoked during a
consultation. Most of the early expert systems were honed
for diagnostic support and were predominantly rule-based
expert systems (Darlington, 2000). The explanation facilities
provided in these systems, such as MYCIN (Shortliffe,
1981), would have been predominantly problem solving
knowledge via a rule trace. This is, essentially, a record of
the system's run-time rule invocation history during a
consultation--frequently syntactically doctored to present
the explanation in natural language form (Binsted, Cawsey,
& Jones, 1995). The rule-trace facility would enable users to
find out Why a system is asking a question or How, following
a consultation, the system arrived at its conclusions by dis-
playing the trace of rules invoked. This is what sets explana-
tion facilities in expert systems apart from "help" facilities
found in conventional software systems: They can provide a
rule trace of the problem-solving mechanism of the inference
engine during a consultation so that case-specific explana-
tions can be delivered, whereas help facilities would nor-
mally be provided by the developer preparing text in advance
to explain the different outcomes. This is known as canned
text and is often used with rule-trace or other explanation
facilities to enhance or supplement explanations. However,
there are problems associated with canned text for the sys-
tem builder has to anticipate all the possible user questions in
advance to invoke the appropriate response, which can result
on a lengthy development overhead.
Justification explanations. The rule-trace explanation can
reconstruct a trace from what problem-solving knowledge is
contained in the expert system knowledge base. A rule trace
can be executed without access to any rules that justify the
existence of this knowledge. If the builder has not included
the knowledge to justify the knowledge in the rule base, then
the system will not be able to justify the existence of the
knowledge. The importance of this justification knowledge
was recognized by Clancey (1983) when attempting to
extend the MYCIN system to support the training of junior
physicians. He found that MYCIN failed to do this because
it did not contain justification knowledge as the rules that
model the domain did not capture all the forms of knowledge
used by experts in their reasoning. Expert physicians would,
of course, use rules of thumb themselves in solving prob-
lems, but they would also--as a result of their training and
experience--possess a deep theoretical understanding of
their subject domain called "deep knowledge." They may,
for example, use "rules of thumb" or heuristic knowledge
when performing a diagnosis but would need to draw on
their deep knowledge if an explanation that justifies such
knowledge is required. In the same way, justification knowl-
edge would have to be explicitly captured by the system
designer if it was required for explanation. Justification
knowledge may be captured by using deep knowledge mod-
els. Empirical research has consistently shown that user
acceptance of expert systems increases for nonexpert users
when this justification knowledge is present and that justifi-
cation is the most effective type of explanation to bring about
positive changes in user attitudes toward the advice-giving
system (Ye & Johnson, 1995).
Strategic explanations. Strategic explanations provide
knowledge about how to approach a problem by choosing an
ordering for finding subgoals to minimize effort in the search
for a solution. For example, the rule of thumb that alcoholics
Darlington 5
are likely to have an unusual etiology can lead the expert to
focus on less common causes of infection first--thereby
pruning the search space to find a solution. In rule-based
expert systems, the strategic knowledge is frequently implic-
itly incorporated in the problem-solving rules. This is accept-
able if the designer wants the system to provide a
problem-solving role only. However, Clancey (1983) real-
ized that this knowledge needed to be explicitly represented
in the MYCIN system so that it could become transparent to
students training to use the system--rather like the justifica-
tion knowledge discussed earlier. A follow-up system called
NEOMYCIN was developed by Clancey (Clancey, 1983;
Clancey & Letsinger, 1981): This was a consultation system
whose medical knowledge base contained the strategic
knowledge explicitly available for training purposes. Sys-
tems containing explicit strategic knowledge could be better
able to answer questions such as "Why not pursue this line of
reasoning instead of that" as the solution planning would be
explicitly available.
Terminological explanations. Terminological explanations
provide knowledge of concepts and relationships of a domain
that domain experts use to communicate with each other. The
inclusion of terminological explanations is sometimes neces-
sary because in order for one to understand a domain, one
must understand the terms used to describe the domain.
Terminological explanations are a category of explanations
that are frequently used with feedforward explanations and
would frequently be implemented using canned text. They
are more likely to be used by novice or nonclinical users
such as patients, rather than the more knowledgeable users.
Terminological explanations provide generic rather than
case-specific knowledge (Mao & Benbasat, 2000; Swartout
& Smoliar, 1987).
Some approaches to second generation explanation. One of
the problems with explanation in first generation expert sys-
tems incorporating the above explanation types was the
unnatural and inflexible dialogues that often resulted during
consultations--often as a consequence of the restrictions of
the interrogatives Why and How described earlier. One
approach to resolving this problem was advocated by Wick
and Slagle (1989), who developed a system called Journalis-
tic Explanation Facility (JOE). JOE delivers explanation
based on a journalistic analogy for news writing reported
events. It does this by extending the Why and How inter-
rogatives to include the interrogatives Who, What, Where,
and When providing scope for extending explanation queries
to answer in past, present, and future text. Despite being lim-
ited by the absence of explicit domain knowledge, JOE is an
example of a prototype that can make the most of the inbuilt
explanation facilities and lead to low construction overheads.
However, Wick (1992a) recognizes shortcomings in JOE in
its inability to present the "big picture" in its explanations.
Thus, different research directions were advocated, which
focused more on understandability of explanations--taking
into account such issues as abstraction into different levels,
linguistic competence, and summarization (Swartout &
Moore, 1993). One prototype system (Wick, 1992b) used a
technique called Reconstructive Explanation. This technique
uses one knowledge base for the expert system and another
for the explanation component. The rationale for this is that
an expert's line of reasoning is not necessarily the same as
the explanation provided, yet the basis for default explana-
tions has been the rule trace of the line of reasoning. The
Reconstructive Explanation system can improve on under-
standability but at a cost of higher construction overheads.
The techniques described above can be used to tailor
explanation facilities to suit different categories of users, or
stakeholders, without necessarily altering the reasoning of
the underlying expert system.
The Stakeholders
Classification of Health Care Domain
User-Groups
According to Leroy and Chen (2007), modern medical
Decision Support Systems (DSS) are developed for four
different groups of decision makers--two of whom have
a medical background (clinicians and medical research-
ers), the other two (administrators and patients) may not.
Clinicians could include physicians (including junior
physicians) and nurses to use their knowledge and expert
systems to make decisions on behalf of others by, for
example, diagnosing diseases or evaluating drug interac-
tions from a treatment or adopting a nursing strategy to
alleviate pain. General research has shown that expert
physicians do make use of explanation facilities, but their
requirements are very different to that of other users.
Experts tend to use feedback rule-trace explanations and
are more likely--than nonexperts--to use explanations
for resolving anomalies, such as disagreements with sys-
tem advice, exploring alternative diagnoses, and verifica-
tion of assumptions (Arnold, Clark, Collier, Leech, &
Sutton, 2006; Mao & Benbasat, 2000). However, nonex-
perts such as trainee physicians are more likely to use a
range of explanations types for short- and long-term
learning. For example, Arnold et al. (2006) have shown
that nonexperts tend to use both feedback and feedfor-
ward justification explanations as well as terminological
feedforward explanations.
Administrators do not have direct clinical interaction with
patients but are responsible for the management of health
care options and facilities. They may therefore use expert
systems to manage health care options and could benefit
from explanation facilities by aiding clinicians, through, for
example, the managing of patient referrals by finding out
whether a patient is suitable for a referral. All of the explana-
tion types discussed previously may benefit administrators
depending on the nature of the application task. Furthermore,
administrators may use expert system for patient care
6 SAGE Open
management, that is, automatically scheduling follow-up
appointments or treatments, or automatically generating
reminders relative to preventive care (i.e., inoculations) or
tracking adherence to research protocols. Explanation facili-
ties which enable time-dependent queries such as through
using the interrogative When, as described in the JOE proto-
type in the previous section may be useful in this context.
Patients are the largest group of decision makers with the
least amount of medical training. However, chronic patients
often become expert patients because in many cases of
chronic illness, patients are likely to acquire skills to help
them manage their illness better (Bury, 2003) because of the
severity of their discomfort, and they may have a much
greater desire to understand and access to web-based expert
systems could help. However, Berland, Elliot, Morales, and
Algazy (2001) have shown that the general public have dif-
ficulty with wordy medical jargon. Patients would therefore
clearly benefit from terminological explanations in some
health care systems.
Expert System MedicalTasks
Amenable to Explanation
Medical Expert SystemTaskTypes
Expert systems are now in routine clinical use in several task
areas, including those described below (Coeira, 2003;
Taylor, 2006).
Generating Alerts and Reminders. Alerts inform clini-
cians of changes in a person's condition, perhaps by attach-
ing an expert system to a monitor. Alerts could also be used
to scan lab test results and send reminders or warnings. For
example, a patient could be attached to an electrocardiogram
(ECG) or pulse oximeter, and the alert expert system could
warn of changes in a patient's condition. In less acute cir-
cumstances, it might scan laboratory test results, or drug or
test orders, and then send alerts or warnings--either via
immediate on-screen feedback or through a messaging sys-
tem like email. Reminder systems serve a slightly different
purpose in that they are used to notify clinicians of important
tasks that need to be done before an event occurs. An exam-
ple of a reminder system could be the generation of a list of
immunizations that is required by each patient on the daily
schedule in an outpatient clinic (Randolph, Guyatt, Calvin,
Doig, & Richardson, 1998). According to Bindels, De
Clercq, Winkens, and Hasman (2000), reminder systems are
far more successful than diagnostic expert systems, for the
so-called Greek Oracle approach is not very popular among
physicians because it reduces their role. Reminder systems
are successful because they leave the physician in control
and only provide feedback when the physician does not obey
the rules. Explanation facilities could be important for these
tasks for as Hanson (2006) says, the clinician wants access to
the knowledge rules associated with the condition and the
interventions proposed by the expert systems. The clinician
may want to evaluate the evidence that underpins the expert
systems rules, suggesting that justification and problem-
solving explanation types may be appropriate for clinicians
using alert systems. Moreover, as the purpose of alert and
reminder systems is to provide interventions to clinicians
(junior and senior), explanations may only be appropriate for
these user categories.
Therapy Critiquing and Planning. These are expert system
tasks that may look for inconsistencies and omissions in an
existing treatment plan but would not necessarily assist in
the generation of the plan. Some examples of their uses could
be applied to physician order entry systems. For example, on
entering an order for a blood transfusion, a clinician may
receive a message stating that the patient's hemoglobin level
is above the transfusion threshold, and the clinician must jus-
tify the order by stating an indication, such as active bleeding
(Randolph et al., 1998). However, planning systems have
more knowledge about the structure of treatment protocols
and can be used to formulate a treatment based on a data on
patient's specific condition from the Electronic Medical
Record (EMR) and accepted treatment guidelines. Therapy
critiquing explanations could benefit clinicians or adminis-
trators in that they could allow the user to enter a proposed
solution to a problem and then explain flaws in the solution.
Strategic explanations might be appropriate for administra-
tors, enabling them to make resource or allocation decisions.
Clinicians could also benefit from strategic explanations so
that "why not" scenarios may be explored (Martincic, 2003).
Clinicians (junior and senior) may benefit from access to
rule trace and justification knowledge to understand the
rationale underlying the possible flaws in the solution.
Drug Advisory Systems. Drug advisory systems such as
CAPSULE (Walton, 1996) are used to assist clinicians with
the prescription and medications and the selection of the
most cost-effective treatments. There is much evidence to
show that computer-based clinical decision support systems
have been shown to improve physician performance in rela-
tion to drug treatment and adherence to protocols (Hunt
et al., 1998). Furthermore, some expert systems can improve
physician performance for drug dosing and preventive care
(Hunt et al., 1998) without reducing drug expenditure
(Vested, Nielsen, & Olesen, 1997). Explanation facilities can
serve an important role in drug prescribing and dosing. An
example of an expert system incorporating explanation for
drug dispensing is OPADE (Carolis et al., 1996), which was
described earlier. The system generates beneficiary centered
explanations about drug prescriptions that take into account
the user requirements (Berry et al., 1995). Patients could
benefit from this type of task with the provision of termino-
logical explanations--enabling them to understand medical
terms and jargon. More generally, justification explanations
could be appropriate for other categories of users for
this type of medical task, so that, for example, users can see
Darlington 7
theoretical reasons for why a treatment has side effects
(Berry et al., 1995) and/or be aware of interactions with other
drugs. Strategic explanations could also be useful to these
users so that they could understand why alternative drugs have
been ruled out for consideration with a particular patient.
Diagnostic Systems. Early medical expert systems were
mainly used for diagnosis and many of these have fallen by
the wayside (Coeira, 2003). According to Taylor (2006),
expert systems have proved to be least required where they
were thought to be most required: in diagnosis applications.
Delaney, Fitzmaurice, Riaz, and Hobbs (1999) believe that
computerized diagnostic systems have not yet been devel-
oped to the stage where they can significantly aid diagnostic
accuracy and also claimed that there was no groundswell of
interest in diagnostic systems because physicians do not per-
ceive that they have a deficiency of expertise within their
own area of speciality. However, a number of focused diag-
nostic systems are still in use, and new uses are emerging, for
example, ECG interpretation and laboratory test interpreta-
tions (Warner, Sorenson, & Bouhaddou, 1997). In particular,
when a patient's case is complex or rare, or the person mak-
ing the diagnosis is simply inexperienced, an expert system
can help in the formulation of likely diagnoses based on
patient data presented to it, and the system's understanding
of illness, stored in its knowledge base. For example, diag-
nostic assistance may be useful with complex data, such as
those provided by an ECG analysis, where most clinicians
can make straightforward diagnoses but may miss rare pre-
sentations of common illnesses like myocardial infarction or
may struggle with formulating diagnoses, which typically
require specialized expertise. Explanation facilities could
play an important part here for, as noted earlier, experts tend
to use explanation facilities when there is disagreement with
the advice given by the system (Gregor & Benbasat, 1999).
Computer-aided decision making could also speed diagno-
sis, especially for difficult cases, thus providing the physi-
cian with time for other matters--such as interacting with
patients. A number of web-based diagnostic systems, such as
ISABEL (www.isabelhealthcare.com)--a dynamic diagnos-
tic checklist system--are also gaining in popularity. The
ISABEL diagnostic aid has been shown to be of potential use
in reminding junior doctors of key diagnoses in the
emergency department (Ramnarayan et al., 2007). The
effects of its widespread use on decision making and diag-
nostic error can be clarified by evaluating its impact on
routine clinical decision making. Another study by Graber
and Ashlie (2008) shows that the ISABEL clinical deci-
sion support system quickly suggested the correct diagno-
sis in almost all complex cases. However, ISABEL
provides user-invoked canned text explanations at various
levels. For example, when ISABEL ranks likely diagnoses
to symptoms, it can provide textual explanations with
hyperlinks to lower level explanations if required. In the
case of diagnostic systems, rule trace, justification, and
strategic explanations could be appropriate to clinicians
(junior and senior), although patients who use web-based
diagnostic systems could benefit again from the provision
of terminological explanations possibly delivered through
canned text or other means activated by hyperlinks.
Patients may find this type of explanatory support particu-
larly important during question answering phase where
they may not fully understand the meaning of a question
or the relationship that the question has on the overall
consultation.
Summary
Table 1 provides a summary based on suggestions for
explanation facilities for the tasks described in the previous
sections. These are only suggestions because in practice,
much would depend on the characteristics of a particular
domain.
Conclusions
This article recognizes the changing role of health care
expert systems tasks over the past 30 years. However, one of
their main features--explanation facilities--has been largely
ignored in health care systems--despite the potential bene-
fits that can be derived from their inclusion. This article has
shown that there is much potential for the use of explanation
facilities for problem-solving tasks other than diagnostics--
the only problem-solving task that was used in the early
medical expert systems.
Table 1. Summary Explanation Types,Tasks, and Users
ES task/user category Clinicians Junior clinicians Administrators Patients
Generating alerts and
reminders
Rule trace, justification Rule trace, justification Not applicable Not applicable
Therapy critiquing and
planning
Rule trace, justification, strategy Rule trace, justification,
strategy
Strategy Not applicable
Drug advisory systems Justification, strategy justification, strategy Justification, strategy Terminological
Diagnostic systems Rule trace, justification, strategy Rule trace, justification,
strategy
Not applicable Terminological
Note: ES = expert system.
8 SAGE Open
Another trend emerging, especially with the growth
of web-based medical expert systems, is the range of
stakeholders--which can vary from clinicians to patients.
This article has described a range of techniques that can be
used to tailor and enhance explanation facilities to suit dif-
ferent stakeholders without necessarily altering the underly-
ing system. However, the developer will determine the way
in which the knowledge is represented and this will affect the
way in which these techniques could be implemented in
terms of the construction overheads--how difficult and
time-consuming it is to build the explanations.
The implications of this research is then that builders of
expert systems should no longer consider explanation facili-
ties an unnecessary adjunct but should give careful consider-
ation to the way that they might help support users of the
system. In doing so, builders of health care expert systems
should canvass the views of the stakeholders to gauge what
explanation content, type of interaction, and access mecha-
nism may be suitable. The low usage of explanation facilities
discussed earlier in the article could be improved substan-
tially, for this article has shown that users are more likely to
adhere to expert system recommendations when quality
explanation facilities are available as well as improve perfor-
mance and result in more positive user perceptions about the
system.
However, further empirical research is necessary to inves-
tigate the potential benefits and changes in performance aris-
ing from using explanation facilities in different medical task
settings as well as expand on the general results exploring
the likely benefits and performance when applied to different
stakeholders in the health care domain.
Declaration of Conflicting Interests
The author(s) declared no potential conflicts of interest with
respect to the research, authorship, and/or publication of this
article.
Funding
The author(s) received no financial support for the research,
authorship, and/or publication of this article.
References
Aikins, J. S., Kunz, J. C., Shortliffe, E. H., & Fallat, R. J. (1983).
PUFF: An expert system for interpretation of pulmonary func-
tion data. Computers and Biomedical Research, 16, 199-208.
Arnold, V., Clark, N., Collier, P. A., Leech, S. A., & Sutton, S. G.
(2006). The differential use and effect of knowledge-based sys-
tem explanations. MIS Quarterly, 30, 79-97.
Berland, G. K., Elliot, M., Morales, L., & Algazy, J. (2001). Health
information on the Internet: Accessibility, quality and readabil-
ity in English and Spanish. Journal of the American Medical
Association, 285, 2612-2621.
Berry, D., Gillie, D. T., & Banbury, S. (1995).What do patients want
to know: An empirical approach to explanation generation and
validation. Expert Systems With Applications, 8, 419-428.
Bindels, R., De Clercq, P. A., Winkens, R. A. G., & Hasman, A.
(2000). A test ordering system with automated reminders for
primary care based on practice guidelines. International Jour-
nal of Medical Informatics, 58-59, 219-233.
Binsted, K., Cawsey, A., & Jones, R. B. (1995). Generating person-
alized information using the medical record. In Artificial intel-
ligence in medicine: Proceedings of AIME 95 (pp. 29-41). New
York, NY: Springer Verlag.
Buchanan, B. G., Shortliffe, E. H. (1984). Rule-based expert sys-
tems: The MYCIN experiments of the Stanford heuristic pro-
gramming project. Reading, MA: Addison-Wesley.
Bury, M. (2003). Perspectives on the expert patient. London, UK:
Royal Pharmaceutical Society of Great Britain.
Carolis, B. D., Rosis, F. D., Grasso, F., Rossiello, A., Berry, D.,
& Gillie, T. (1996). Generating recipient-centered explanations
about drug prescription. Artificial Intelligence in Medicine, 8,
123-145.
Cawsey, A. (1993). User modeling in interactive explanations.
Journal of User Modeling and User Adapted Interaction, 3,
1-25.
Chandrasekaran, B., Tanner, M. C., & Josephson, J. R. (1989,
Spring). Explaining control strategies in problem solving. IEEE
Expert, 4, 9-24.
Clancey, W. J. (1983). Epistemology of a rule-based expert system:
A framework for explanation. AI Magazine, 20, 215-251.
Clancey, W. J., & Letsinger, R. (1981). NEOMYCIN: Reconfigur-
ing a rule-based expert system for application to teaching. In
W. J. Clancey & E. H. Shortliffe (Eds.), Readings in medical
artificial intelligence: The first decade (pp. 361-381). Reading,
MA: Addison-Wesley.
Coeira, E. (2003). Guide to health informatics. London, England:
Hodder Arnold.
Cunningham, P., Doyle, D., & Loughrey, J. (2003). An evalua-
tion of the usefulness of case-based explanation. Proceedings
of the 5th International Conference on Case-Based Reasoning
(ICCBR 2003) (pp. 122-130). Trondheim, Norway: Springer.
Darlington, K. (2000). The essence of expert systems. London,
England: Prentice Hall.
Darlington, K. (2008). Using explanation facilities in healthcare
expert systems. HEALTHINF 2008 Conference, Funchal,
Madeira, Portugal.
Delaney, B. C., Fitzmaurice, D. A., Riaz, D., & Hobbs, F. D. (1999).
Can computerised decision support systems deliver improved
quality in primary care? British Medical Journal, 319, 1281.
Dhaliwal, J. S. (1996). An experimental investigation of the use of
explanations provided by knowledge-based systems (Unpublished
doctoral dissertation). University of British Columbia, Canada.
Everett, A. M. (1994). An empirical investigation of the effect of
variations in expert system explanation presentation on users'
acquisition of expertise and perceptions of the system (Unpub-
lished doctoral dissertation). University of Nebraska, Lincoln.
Friedman, C., & Wyatt, J. (1997). Evaluation methods in medical
informatics. New York, NY: Springer.
Garg, A. X., Adhikari, N. K., McDonald, H., Rosas-Arellano, M. P.,
Devereaux, P. J., Beyene, J., . . . Haynes, R. B. (2005). Effects
Darlington 9
of computerized clinical decision support systems on practitio-
ner performance and patient outcomes: A systematic review.
Journal of the American Medical Association, 293, 1223-1238.
Graber, M. L., & Ashlie, M. (2008). Performance of a web-based
clinical diagnosis support system for internists. Journal of Gen-
eral Internal Medicine, 23(Suppl. 1), 37-40.
Gregor, S., & Benbasat, I. (1999). Explanations from intelligent
systems: Theoretical foundations and implications for practice.
Management Information Systems Quarterly, 23, 497-530.
Gregor, S. D. (1996). Explanations from knowledge-based systems
for human learning and problem solving (Unpublished doctoral
dissertation). University of Queensland, Brisbane, Australia.
Hanson, C. W. (2006). Healthcare informatics. New York, NY:
McGraw-Hill.
Hunt, D., Haynes, R., Hanna, S., & Smith, K. (1998). Effects of
computer-based clinical decision support systems on physician
performance and patient outcomes: A systematic review. Jour-
nal of the American Medical Association, 280, 1339-1346.
Korver, M., & Lucas, P. (1993). Converting a rule-based system
into a belief network. Medical Informatics, 18, 219-241.
Lacave, C., & Diez, F. J. (2004). A review of explanation methods
for heuristic expert systems. Knowledge Engineering Review,
19, 133-146.
Leroy, G., & Chen, H. (2007). Introduction to the special issue on
decision support in medicine. Decision Support Systems, 43,
1203-1206.
Mao, J., & Benbasat, I. (2000). The use of explanations in knowl-
edge-based systems: Cognitive perspectives and a process-trac-
ing analysis. Journal of Management Information Systems, 17,
153-180.
Martincic, C. J. (2003). QUE: An expert system explanation facility
that answers "Why Not" types of questions. Journal of Comput-
ing Sciences in Colleges, 19, 335-346.
Payne, J. W., Bettman, J. R., & Johnson, E. J. (1993). The adaptive
decision maker. Cambridge, UK: Cambridge University Press.
Peleg, M., & Tu, S. (2006). Decision support knowledge represen-
tation and management in medicine. In Reinhold Haux & Casi-
mir Kulikowski (Eds.), IMIA Yearbook of Medical Informatics
(pp. 72-80). Stuttgart, Germany: Schattauer.
Ramnarayan, P., Cronje, N., Brown, R., Negus, R., Coode, B., Moss,
P., . . . Britto, J. (2007). Validation of a diagnostic reminder sys-
tem in emergency medicine: A multi-centre study. Emergency
Medicine Journal, 24, 619-624. doi:10.1136/emj. 044107
Randolph, A., Guyatt, G., Calvin, J., Doig, G., & Richardson, W.
(1998). Understanding articles describing clinical prediction
tools. Critical Care Medicine, 26, 1603-1612.
Shortliffe, E. H. (1981). Computer based medical consultations:
MYCIN. New York, NY: Elsevier.
Swartout, W. R., & Moore, J. (1993). Explanation in second gen-
eration expert systems. In J. David, J. Krivine, & R. Simmons
(Eds.), Second generation expert systems (pp. 543-585). Berlin,
Germany: Springer.
Swartout, W. R., & Smoliar, S. W. (1987). On making expert sys-
tems more like experts. Expert Systems, 4, 196-207.
Taylor, P. (2006). From patient data to medical knowledge:
The principles and practice of health informatics. London,
England: Blackwell.
Van Bemmel, J., & Musen, M. (1997). Handbook of medical infor-
matics. Houten, Netherlands: Springer.
Vested, P., Nielsen, J. N., & Olesen, F. (1997). Does a computerized
price comparison module reduce prescribing costs in general
practice? Family Practice, 14, 199-203.
Walton, R. (1996). An evaluation of CAPSULE, a computer system
giving advice to general practitioners about prescribing drugs.
Journal of Informatics in Primary Care, March 2006, 2-7.
Warner, H., Sorenson, D., & Bouhaddou, O. (1997). Knowledge
engineering in health informatics. New York, NY: Springer.
Wick, M. (1992a). Expert system explanation in retrospect: A case
study in the evolution of expert system explanation. Journal of
Systems and Software, 19, 159-169.
Wick, M. (1992b). Explanation as a primary task in problem solv-
ing. Journal of Systems and Software, 19, 159-169.
Wick, M. R., & Slagle, J. R. (1989). An explanation facility for
today's expert systems. IEEE Expert, 4(1), 26-36.
Wyatt, J. R. (1997). Evaluation of clinical information systems. In
J. H. van Bemmel & M. A. Musen (Eds.), Handbook of medical
informatics (pp. 165-173). New York: Springer.
Ye, L. R., & Johnson, P. E. (1995). The impact of explanation facili-
ties on user acceptance of expert systems advice. MIS Quar-
terly, 19, 157-172.
Bio
Keith W. Darlington is a senior lecturer in Computing and
Artificial Intelligence at London South Bank University. He spe-
cialises in Expert Systems and has published a book on the subject
called The Essence of Expert Systems.
