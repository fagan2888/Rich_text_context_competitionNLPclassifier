Methodological Innovations Online (2009) 4(2) 12-20
Correspondence: F. Wiseman, Professor of Statistics and Marketing Research, College of Business Administration,
Northeastern University, Boston, MA 02115 USA. Email: f.wiseman@neu.edu
ISSN 1748-0612online
DOI:
The Effects of the Initial Mode of Contact on the Response Rate and
Data Quality in an Internet-Based College Satisfaction Survey
Frederick Wiseman*
*College of Business Administration, Northeastern University
Abstract
This paper reports on the results of an experiment that investigated the response rate and data quality effects of two
alternative methods for making initial contact with potential respondents in a sample survey. The study was conducted in a
university setting and selected students were initially contacted about their participation in the survey either thru an e-mail
message or thru their own customised university portal. Survey findings revealed that the e-mail alternative resulted in a
significantly higher overall response rate. However, the findings also revealed that there were significant differences in the
responses of the two groups with respect to the evaluations given by students regarding their academic and social well-being
at the university. Specifically, sample members in the portal group gave significantly more positive evaluations regarding
their academic and social well-being than did members of the e-mail group. However, there were no significant differences
between the responses of the two groups in terms of their evaluations of either the universitys food court or the campus
recreational centre. Additionally, despite efforts within both groups to achieve a high response rate and a representative
sample, neither alternative was able to produce a sample that had a high response rate or a sample that was representative of
the population in terms of either students gender or grade point average. Implications of these findings for conducting
surveys, especially those of student populations, are also discussed.
Key words: Internet, survey, methodology, non-sampling error, representativeness, data quality
Introduction
In recent years, marketing and survey researchers have investigated how various data collection strategies have
affected response rates and data quality in online surveys (see, for example, Marcus, et al., 2007; Taylor et al.,
2005; Deutskens, et al., 2004; Manfreda, et al., 2002; Sheehan, 2001; Taylor, 2000). While the number of such
studies is relatively small compared to those involving other frequently used modes of data collection, the
internets speed, efficiency, and low cost have all spurred researchers in a variety of disciplines to search for
improved data collection strategies. This paper reports on an experimental study conducted in a university setting
where it was possible to survey students online either by notifying them about the survey through an e-mail
message or by having them receive notice about the survey on their customised university portal. These
Wiseman/Methodological Innovations Online (2009) 4(2) 12-20
13
customised portals allow the university to communicate with designated students on numerous topics and issues,
one of which being their selection in a sample for a university sponsored survey.
The study was undertaken in response to the universitys decision to greatly reduce the use of e-mail messages
(including requests for survey participation) as a means for communicating with their students. Members of the
student body had complained about the wide spread use of what they perceived to be "junk e-mail" messages that
were being sent by university officials. Because this decision impacted requests for survey participation and
because this decision was made without prior consideration of response rate or data quality implications, it was
subsequently decided to conduct an experimental study in order to determine whether this change in procedure
would, in turn, impact the quality of sample data obtained in future online surveys conducted by the university.
Specifically, the objectives of the experimental study were (i) to determine whether there was a significant
difference in the response rate between the group that received a request for participation through an e-mail
message and the group that received a request for participation through their customised portal, (ii) to determine
whether each alternative (e-mail and portal) was able to produce a sample of respondents that was representative
of the population from which it was drawn, and (iii) to determine whether survey results varied depending upon
which method was used to initially contact potential respondents.
Porter and Whitcomb (2003), in their review of the literature, found that there had been a paucity of research that
examined the effects of contact type on web survey response rates. Noting the importance of this area as
evidenced by the rich literature on this topic for other modes of data collection, the authors conducted an
experimental investigation that examined how various factors such as personalisation of the salutation, setting a
deadline for participation and the statement of sample selectivity affected survey response rates. Their research
findings showed that while personalisation had little impact on the response rate, setting a deadline for
participation and informing sample members of their selectivity did have a positive effect. The finding on
personalisation contradicted results of a previous study based upon a meta-analysis of sixty-eight web surveys
which found that the use of personalisation in the initial contact did increase the response rate (Cook et al., 2000).
Ilieva et al. (2002), following up on the work of Schaefer and Dillman (1998), strongly advocated the use of
mixed modes of data collection (for example, online and mail) especially when conducting international
marketing research surveys. They argued that researchers using such an approach were much more likely to obtain
a representative sample than they would be if only a single mode of data collection were used. More recently,
Porter and Whitcomb (2005, 2007) extended their research to examine the effects of mixed mode contacts and
alternative e-mail subject lines on response rates. Here the authors found no significant differences if survey pre-
notification and reminders were sent to sample members by either paper or an e-mail message. Past research,
conducted by Schaefer and Dillman (1998) and Kaplowitz, et al. (2004), had produced contradictory evidence on
this subject with the former finding e-mail correspondences preferred to paper correspondences, while the latter
reaching an opposite conclusion. For e-mail subject lines, the findings revealed that the effects were a function of
the level of involvement that sample members had with the survey sponsor.
The present research study examines another area related to how the nature of the initial contact affects response
rates and data quality. The study also looks at other factors that must be investigated before researchers will be
able to have a high degree of confidence that their survey findings are representative of the population from which
their sample was drawn.
Methods
In the present study, the population of 7,220 students at a large American university was randomly divided into
two groups (portal and e-mail) of 3,610 students each. Prior to sample selection, the population was stratified by
gender and class year so that each of the two experimental groups mirrored the population in terms of these two
Wiseman/Methodological Innovations Online (2009) 4(2) 12-20
14
characteristics. Once the sample selection had been made, a subsequent analysis revealed that the groups were also
identical in terms of their mean grade point average (GPA).
The survey questionnaire was comprised of four sections. The first asked participants to evaluate various items
that were directly related to their academic and social well-being on campus. This section was followed by one in
which students evaluated the universitys food court and its recreational centre on a number of different
dimensions. The next section asked students to indicate the extent to which they believed that five potential
problems areas (academic dishonesty, alcohol abuse, drug abuse, racism and sexual harassment) were present on
their campus. The last section contained classification questions. Sample members were told in the introductory
remarks to the survey that their participation would help university officials to improve students overall
educational experience.
Students received the actual survey announcement either through an e-mail message which included a direct link
to the questionnaire or by a notice with an accompanying survey link posted on their customised portal. Two
follow-up reminders were also sent by similar methods to increase the surveys response rate within each group.
Further, as an incentive for participation, all students who completed the survey questionnaire were entered into a
raffle for book store gift certificates.
Hypotheses
It was hypothesised that the student group which received an e-mail message that introduced them to the
survey would have a higher overall response rate than the student group that received notice of the survey
directly from their portal. There are two reasons for this. First, the e-mail message sent to each student
increases the perceived importance of the survey compared to the portal alternative. This is analogous to the
use of first class postage in a mail survey compared to that of a bulk mailing of survey questionnaires. Second,
with the e-mail message, students are more likely to become aware of the survey than they would be from the
universitys portal. This is because students are more likely to check their e-mail messages than they are to
check the portal alternative and, even if they were to check their portal, there would be other items competing
for their attention.
It was further hypothesised that neither the obtained e-mail sample nor the obtained portal sample would be
representative of the population from which it was drawn on three characteristics Â­ gender, class year and
GPA. Past studies, have shown that (i) females and (ii) freshmen and sophomores were more likely to
participate in internet surveys than their male and upper-class counterparts (Porter, 2004). In addition, it was
hypothesised that the GPA of participating students would be higher than the non-respondents since those
with the most positive academic experiences and better grades would be the ones most likely to participate in
the survey due to these positive factors.
Finally, it was believed that student participation in the survey would be correlated with the ratings given to
the academic and social life items with those students who evaluated these items most positively also being
the ones most likely to complete and return the questionnaire. Because of this, it was further hypothesised that
the smaller sample of respondents in the portal group would give higher and more positive ratings to these
items compared to the larger number of sample respondents in the e-mail group. However, this was
hypothesised only for the academic and social life items and not for the food court, recreational centre items
and problem area items which were hypothesised to have little, if any, correlation to survey participation.
Wiseman/Methodological Innovations Online (2009) 4(2) 12-20
15
Results
Response rate
The student group that received the request for participation directly from an e-mail message had a response
rate of 28% compared to a response rate of 21% for the group that received its request directly from their
customised portal. As shown in Table 1., this difference of seven percentage points was significant at the .01
level and provided confirmation of the previously stated hypothesis. In percentage terms, the e-mail group's
response rate was one-third higher than the portal group's response rate. On the other hand, these rather low
response rates occurred despite using data collection strategies in both groups that were designed to increase
survey participation.
Table 1. Survey Response Rate by Experimental Group
Number Number Response
Group completed selected ________ ratea
Portal 747 3610 21%
E-mail 1001 3610 28_
a p-value=.000.
Sample representativeness
One critical consideration in any survey is whether the obtained sample of respondents is representative of the
population from which it was drawn. In this study, the population characteristics with respect to three
variables (gender, class year and grade point average) were known. These known characteristics were
compared to the obtained sample characteristics of those in both the e-mail and portal groups. The results of
this comparison, shown in Table 2., revealed that neither sample was representative of the population from
which it was drawn on either gender or GPA. The groups were, however, representative of the population with
respect to class year.
The differences between sample and population characteristics were large in the portal group for both gender
and grade point average. While the overall population was evenly split between males and females, the
females in the portal group were far more likely to participate in the survey compared to their male
counterparts (64% vs. 36%). Additionally, for grade point average, the GPAs for sample members in the
portal group were significantly higher than the actual GPAs of students in the population. For example, 58%
of the survey participants claimed to have a GPA of at least 3.25 compared to a known population percentage
of 46%. Further, only 11% of the participants in the portal sample claimed to have a GPA under 2.75
compared to the known population percentage of 23%. One note of caution, since the sample participant
GPAs were self-reported, the differentials that were found between sample participants and the population
could also be due to measurement error rather than to non-response error. In terms of class year, there was a
slightly higher rate of participation among freshmen and sophomore students compared to those in their upper
class years. These differences, though, were too small to be statistically significant.
Wiseman/Methodological Innovations Online (2009) 4(2) 12-20
16
Table 2. Sample versus Population Characteristics of Portal and E-mail Groups
Portal E-mail ___
Characteristic Sample Population Difference Sample Population Difference
Gender
Male 36% 50% -14% 43% 50% -7%
Female 64 50 14 57 50 7
(Chi-square=20.5, df=1, p=.000) (Chi-square=53.6, df=1, p=.000)
Grade point average
3.50 or above 37% 27% 10% 38% 28% 10%
3.25-3.49 21 19 2 23 19 4
3.00-3.24 21 18 3 18 18 0
2.75-2.99 10 13 -3 12 14 -2
2.50-2.74 6 10 -4 4 9 -5
Below 2.50 5 13 -8 5 12 -7
(Chi-square=138.5, df=5, p=.000) (Chi-square=82.8, df=1, p=.000)
Class year
Freshman/sophomore 45% 43 2 44 43 1
Upperclass 55 57 -2 56 57 -1
(Chi-square=1.47, df=1, p=.225) (Chi-square=1.24, df=1, p=.266)_
Significant differences were also found for gender and GPA between the e-mail sample of participants and the
population. However, the difference for gender was considerably smaller. For the e-mail group, the sample
was overrepresented by females by just fourteen percentage points (57% vs. 43%) compared to a twenty-eight
percentage point differential in the portal group. Again, for class year, statistically significant differences did
not exist between the sample respondents and the population.
The previous analysis compared each groups sample of participants to their respective populations. We now
look at this problem in a slightly different fashion in order to determine whether the actual respondents in the
e-mail group differed from the actual respondents in the portal group in terms of gender, class year and GPA.
Results are presented in Table 3. and indicate that the two groups differed only with respect to gender. There
was a significantly highly percentage of female respondents in the portal group compared to the percentage of
female respondents in the e-mail group. This difference was significant at the .01 level. Significant differences
did not exist for either GPA or class year.
Wiseman/Methodological Innovations Online (2009) 4(2) 12-20
17
Table 3. Portal and E-mail Group Comparisons
Characteristic Portal E-mail Difference
Gender
Male 36% 43% -7%
Female 64 57 7
(Chi-square=7.16, df=1, p=.007)
Grade point average
3.50 or above 37% 38% -1%
3.25-3.49 21 23 -2
3.00-3.24 21 18 3
2.75-2.99 10 12 -2
2.50-2.74 6 4 2
Below 2.50 5 5 0
(Chi-square=5.45, df=5, p=.363)
Class year
Freshman/sophomore 45% 45% 0%
Upperclass 55 55 0
(Chi-square=0.39, df=1, p=.843) _______
Analysis of survey responses
The next step in the analysis was to determine whether the survey responses regarding various aspects of a
students life on campus varied depending upon whether the request for participation came directly from an e-
mail message or from a notice on a students portal. We turn first to items involving the evaluations of
particular aspects of a students academic and social life. In this part of the questionnaire, students were asked
to give their evaluation (1, 2, 3, 4, 5) on fifteen different dimensions. The actual scale was: Poor 1 2 3 4 5
Excellent.
Table 4. presents the differences in the average evaluation ratings for participants within each of the two
groups and the associated t-values, degrees of freedom and one-sided p-values. Consistent with the previously
stated hypothesis, on every one of these fifteen items, the smaller number of participants in the portal group
gave a higher and more positive evaluation rating than did the larger number of participants in the e-mail
group. For seven of these fifteen items (including the overall evaluation rating), the differences were
significant at the .01 level and, for two other items, the differences were significant at the .05 level.
Given the above results, a subsequent analysis was conducted to determine whether the differences that were
obtained between the portal and the e-mail groups were more a function of gender than a function of the mode
of initial contact. This was necessary since there were a far higher percentage of females in the portal group
than there were in the e-mail group. If females were substantially more positive in their ratings than males,
then the differences could be attributed more to gender differences than to differences in the mode of initial
contact. However, this turned out not to be the case as the subsequent analysis revealed that males and females
varied little in terms of their ratings to these fifteen items in both the e-mail and in the portal groups.
Wiseman/Methodological Innovations Online (2009) 4(2) 12-20
18
Table 4. Mean Differences in Evaluations between Portal and E-mail Groupsa
Mean Evaluation
Item Portal E-mail Diff. t df p-value
Overall evaluation 4.05 3.93 .12 3.27 1728 .001
Overall social environment 3.70 3.58 .12 2.62 1712 .005
Overall academic environment 3.87 3.78 .09 2.21 1714 .014
Helpfulness of academic advisors 3.75 3.53 .22 3.88 1578 .000
Accessibility of academic advisors 3.67 3.50 .17 3.11 1576 .001
Campus spirit 3.08 2.92 .16 2.89 1526 .002
Internship opportunities 4.06 3.91 .15 2.60 1300 .005
Quality of courses 3.98 3.90 .08 2.21 1640 .013
Campus diversity 3.89 3.81 .08 1.72 1603 .043
Quality of students 3.74 3.67 .07 1.60 1711 .052
Helpfulness of internship advisors 3.83 3.74 .09 1.38 1306 .082
Variety of majors 4.13 4.08 .05 1.36 1698 .087
Quality of faculty 3.73 3.69 .04 1.04 1728 .149
Accessibility of internship advisors 3.80 3.74 .06 .93 1371 .178
Variety of course offerings 3.78 3.74 .04 .91 1703 .182__
aScale: Poor 1 2 3 4 5 Excellent.
Students were asked for their opinions concerning the university's food court and its recreational centre. Once
again, a five-point evaluation scale was used. As hypothesised and unlike the previous set of items, there were
no significant differences in the ratings of survey members in the portal and e-mail groups. This can be seen in
Table 5.
Finally, students were asked to indicate the degree to which they believed that five potential problem areas
were present at their university. As shown in Table 6., on only one of these potential problem areas, academic
dishonesty, was there a significant difference at the .05 level in the average ratings of the portal respondents
and the e-mail group respondents. Here, those in the portal group were significantly more likely to believe that
academic dishonesty was a problem at the university. A possible explanation for this, in part, may be due to
the fact that female students, who were overrepresented in the portal group, were also significantly more
likely than their male counterparts to say that academic dishonesty was a problem on the campus. The next
two largest differences were for alcohol abuse and drug abuse with portal members, once again, believing that
these were more a problem than those in the e-mail group, but the differences that were found were not large
enough to be statistically significant.
Wiseman/Methodological Innovations Online (2009) 4(2) 12-20
19
Table 5. Mean Differences for Food Court and Recreation Centre Items between Portal and E-mail
Groups
Mean Evaluationa
Item Portal E-mail Diff. t df p-value
Food Courta
Quality of service 3.55 3.58 -.03 -.68 1432 .499
Variety of food offerings 3.38 3.35 .03 .60 1617 .549
Quality of food offerings 3.57 3.56 .01 .23 1617 .820
Value for the money 3.13 3.14 -.01 -.17 1600 .863
Overall level of satisfaction 3.43 3.43 -.00 -.17 1600 .863
Recreation Centre
Overall level of satisfactiona 4.32 4.26 .06 1.56 1625 .120
Equipment is of high qualityb 4.37 4.35 .02 .55 1453 .580
Recreation Centre is important to meb 4.38 4.35 .03 .45 1562 .652
Recreation Centre staff is friendlyb 4.07 4.05 .02 .31 1437 .759
aScale: Very dissatisfied 1 2 3 4 5 Very satisfied.
bScale: Strongly disagree 1 2 3 4 5 Strongly agree.
Table 6. Mean Differences in Perception of Campus Problems between Portal and Email Groups
Mean differencea
Problem area Portal E-mail Diff. t df p-value
Academic dishonesty 2.51 2.40 .11 1.97 1389 .049
Alcohol abuse 3.03 2.95 .08 1.39 1477 .165
Drug abuse 2.67 2.62 .05 1.05 1335 .293
Racism 2.18 2.21 -.03 -.40 1363 .687
Sexual harassment 2.07 2.07 -.00 -.07 1143 .945
a Scale: Not a problem 1 2 3 4 5 Major problem.
Discussion
The results of this experiment revealed that a significantly higher response rate was obtained in the e-mail
group than in the portal group, that portal group sample members gave consistently more positive evaluations
about their academic and social life on campus than did their counterparts in the e-mail group, and that neither
the e-mail nor the portal alternative produced a sample that was representative of the population from which it
was drawn in terms of either gender or GPA. Taken together these three results indicate that the universitys
decision to switch its initial mode of contact from an e-mail message to students customised portal could
have potentially serious data quality implications in future surveys.
It appears that students who are frequent visitors to their portals are the ones who are most engaged with their
university and evaluate their experiences most positively.
Wiseman/Methodological Innovations Online (2009) 4(2) 12-20
20
A sample obtained exclusively from the portal will clearly not provide a sample that is representative of the
student body.
Another area for concern is the large difference in response rates between males and females, both overall and
within each of the two groups. The reason for the concern is that there are likely to be many areas in which
males and females differ in their opinions, although this was not the case in the present study. While the
reasons for these response rate differences are unknown, this particular problem requires future research since
extensive efforts and resources are likely to be required to secure acceptable response rates and representative
samples. Serious consideration must also be given in future studies to conducting follow-up surveys of non-
respondents to determine the nature and extent of any biases that might exist.
In conclusion, there have been numerous studies that have looked at the incremental effects of alternative data
collection strategies in online surveys. Such studies are valuable for helping researchers in their attempts to
obtain samples that are representative of the population. However, this study clearly suggests that online
surveys may have their own unique problems and researchers must be especially careful that the samples that
they obtain are, in fact, representative of their target populations. The findings also lend support to the
arguments made by researchers who have strongly advocated the use of mixed modes of data collection.
References
Cook, C., Heath, F and Thompson, R. (2000) ,,A Meta-Analysis of Response Rates in Web- or Internet-Based
Surveys, Educational and Psychological Measurement 60(6): 821-836.
Deutskens, E., de Ruyter, K., Wetzels, M. and Oosterveld, P. (2004) ,,Response Rate and Response Quality of
Internet-based Surveys: An Experimental Study, Marketing Letters 15(1): 21-36.
Ilieva, J., Baron, S. and Healey, N.M. (2002) ,,Online surveys in marketing research: pros and cons,
International Journal of Market Research 44(3): 361-373.
Kaplowitz, M., Hadlock, T. and Levine, R. (2004) ,,A Comparison of Web and Mail Survey Response Rates,
Public Opinion Quarterly 68(1): 94-101.
Manfreda, K., Zenel, B. and Vehovar, V. (2002) ,,Design of Web Survey Questionnaires: Three Basic
Experiments, Journal of Computer-Mediated Communication 7(3): Article can be found at:
http://jcmc.indiana.edu/vol7/issue3/vehovar.html.
Marcus, B., Bosnjak, M., Lindner, S., Pilischenko, S. and Schutz, A. (2007) ,,Compensating for Low Interest
and Long Surveys: A Field Experiment on Nonresponse in Web Surveys, Social Science Computer Review
25(3): 372-383.
Porter, S. (2004) ,,Raising Response Rates: What Works, New Directions for Institutional Research 121: 5-
21.
Porter, S. and Whitcomb, M. (2003) ,,The Impact of Contact Type on Web Survey Response Rates, Public
Opinion Quarterly 67(4): 579-588.
Porter, S. and Whitcomb, M. (2005) ,,E-Mail Subject Lines and Their Effect on Web Survey Viewing and
Response, Social Science Computer Review 23(3): 380-387.
Porter, S. and Whitcomb, M. (2007) ,,Mixed-Mode Contacts in Web Surveys, Public Opinion Quarterly
71(4): 635-648.
Schaefer, D. and Dillman, D. (1998) ,,Development of a Standard E-Mail Methodology, Public Opinion
Quarterly 62(3): 378-397.
Sheehan, K. (2001) ,,E-mail Survey Response Rates: A Review, Journal of Computer-Mediated-
Communication 6(2): Article can be found at: http://jcmc.indiana.edu/vol6/issue2/sheehan.html.
Taylor, H. (2000) ,,Does internet research "work"? Comparing on-line survey results with telephone surveys,
International Journal of Marketing Research 42(1): 51-63.
Taylor, H., Krane, D. and Thomas, R.K. (2005) ,,Best Foot Forward: Social Desirability in Telephone vs.
Online Surveys, Article can be found at: http://www.publicopinionpros.com/from_field/2005/feb/taylor.asp.
