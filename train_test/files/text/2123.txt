Public Opinion Quarterly, Vol. 74, No. 5, 2010, pp. 934­955
EXAMINING THE RELATIONSHIP BETWEEN
NONRESPONSE PROPENSITY AND DATA QUALITY
IN TWO NATIONAL HOUSEHOLD SURVEYS
SCOTT FRICKER*
ROGER TOURANGEAU
Abstract Important theoretical questions in survey research over the past
50 years have been: How does bringing in late or reluctant respondents af-
fect total survey error? Does the effort and expense of obtaining interviews
from difficult-to-contact or reluctant respondents significantly decrease the
nonresponse error of survey estimates? Or do these late respondents intro-
duce enough measurement error to offset any reductions in nonresponse
bias? This study attempts to address these questions by examining nonre-
sponse and data quality in two national household surveys: the Current
Population Survey (CPS) and the American Time Use Survey (ATUS).
Response propensity models were developed for each survey, and data
quality in each survey was assessed by a variety of indirect indicators
of response error, for example, item-missing-data rates, round value reports,
and interview-reinterview response inconsistencies. The principal analyses
investigated the relationship between response propensity and the data-
quality indicators in each survey, and examined the effects of potential
common causal factors when there was evidence of covariation. Although
the strength of the relationship varied by indicator and survey, data quality
decreased for some indicators as the probability of nonresponse increased.
Therefore, the direct implication for survey managers is that efforts to re-
duce nonresponse can lead to poorer-quality data. Moreover, these effects
remain even after attempts to control for potential common causal factors.
Introduction
Important theoretical questions in survey research over the past 50 years have
been: How does bringing in late or reluctant respondents affect total survey
Scott Fricker is a Research Psychologist at the U.S. Bureau of Labor Statistics, Washington, DC,
USA. Roger Tourangeau is a Research Professor at the Institute for Social Research at the Uni-
versity of Michigan, Ann Arbor, MI, USA, and the Director of the Joint Program in Survey Meth-
odology at the University of Maryland, College Park, MD, USA. *Address correspondence to Scott
Fricker, U.S. Bureau of Labor Statistics, 2 Massachusetts Ave. NE, Room 1950, Washington, DC
20212, USA; e-mail: fricker.scott@bls.gov.
doi: 10.1093/poq/nfq064
Ó The Author 2011. Published by Oxford University Press on behalf of the American Association for Public Opinion Research.
All rights reserved. For permissions, please e-mail: journals.permissions@oup.com
error? Does the effort and expense of obtaining interviews from difficult-to-
contact or reluctant respondents significantly decrease the nonresponse error
of survey estimates? Or do these late respondents introduce enough measure-
ment error to offset any reductions in nonresponse bias?
Evidence from some recent studies suggests that efforts to reduce nonre-
sponse rates have little effect on nonresponse error (Curtin, Presser, and Singer
2000; 2005; Groves, Presser, and Dipko 2004; Keeter et al. 2000; Merkle and
Edelman 2002). For example, Curtin, Presser, and Singer (2000) found negli-
gible differences between monthly estimates of consumer confidence derived
from a full survey dataset and those derived from a dataset in which hard-
to-interview respondents had been removed. Similarly, Keeter et al. (2000)
and Merkle and Edelman (2002) found little correlation between low response
rates and nonresponse bias.
Much less attention, however, has been given to the relationship between
response propensity and survey measurement error. In part, this neglect may
reflect the assumption that the causes of nonresponse and measurement error
are independent. Nonresponse typically is seen as a function of motivational
variables (e.g., interest in the survey topic, time spent away from home),
whereas measurement error is considered primarily a function of cognitive fac-
tors (such as respondent ability, excessive recall demands imposed by the ques-
tions, poor question wording, and so on). This assumption of independent
causal factors may be untenable, however, because the same motivations that
affect participation decisions also may affect performance during the interview.
To the extent that individualsÕ response propensities are positively correlated
with the level of effort that they give during the response process, bringing
reluctant individuals into the respondent pool will increase measurement error
and reduce the quality of estimates (Biemer 2001; Groves 2006).
The few empirical studies that have examined the association between non-
response and data quality suggest that the relationship depends on the statistic of
interest, how measurement error is operationalized, and the type of nonresponse
(noncontact vs. noncooperation). Some studies that have examined indirect
data-quality indicators (e.g., item nonresponse, response completeness) have
found that late responders and initial refusers are more likely than early
responders and those not requiring refusal conversion to skip items; give
shorter, less informative answers to open-ended questions; and provide ``don't
know,'' ``not applicable,'' or ``no opinion'' responses (e.g., Friedman, Clusen,
and Hartzell 2003; Triplett et al. 1996; Willimack, Schuman, and Lepkowski
1995). By contrast, Yan, Tourangeau, and Arens (2004) found that other indirect
indicators of data quality (e.g., acquiescence, nondifferentiation) were unrelated
to response propensity, or were negatively correlated with it; that is, low-
propensity groups produced better data quality than high-propensity groups.
Several studies that have looked at direct estimates of measurement error
(e.g., those based upon discrepancies between survey responses and adminis-
trative records) have found that low-propensity respondents tend to provide
Nonresponse Propensity and Data Quality 935
worse data than high-propensity respondents. For example, Cannell and Fowler
(1963) found that individuals who responded at the end of the survey field
period were 10­15% less accurate in their reports of the number and duration
of their hospital stays than those who responded earlier (see also Kalsbeek et al.
1994). Similarly, Bollinger and David (2001) found that latent ``cooperative''
respondents were less likely than ``uncooperative'' respondents to drop out
of a panel survey and to make reporting errors. More recently, Olson (2006) ex-
amined the separate impact of contact and cooperation propensity on several
variables related to marital dissolution (e.g., time since divorce, length of mar-
riage), separating out the unique contributions of measurement error bias and
nonresponse bias. She found that including reluctant respondents increased mea-
surement error for some estimates, but that bringing in hard-to-contact respond-
ents actually led to decreases in measurement error and overall error (see also
Voigt et al. 2005). For most of the estimates in her study, however, the resulting
changes in measurement error were nonmonotonic across propensity strata and
were very small relative to the size of the estimates. A much stronger link
between response propensity and reporting errors was found in a recent study
by Tourangeau, Groves, and Redline (2010), which examined the role of topic
sensitivity on nonresponse and measurement errors. They found that individuals
who were asked to participate in a survey that raised social desirability issues for
them were both less likely to respond to the survey and more likely to provide
inaccurate answers to the sensitive questions if they did.
The results of these studies suggest that there may be a relationship between
response propensity and data quality, but the nature of that relationship and its
causal mechanisms are not well understood. At the very least, these findings
challenge the traditional assumption that nonresponse and measurement error
are independent. One explanation for covariance between response propensity
and data quality is that the relationship results from a cause (or vector of causes)
common to both (Groves 2006). The identification of appropriate common
causal factors depends in part upon the particular survey protocol and respon-
dent pool, but several candidates seem likely to apply to a broad range of sur-
veys. For example, topic interest is a possibility. Interest in the survey topic may
dispose individuals to agree to a survey request and also stimulate careful pro-
cessing of the survey items. In general, any factor that broadly increases the
motivation to take part in surveys and to respond carefully might lead to a rel-
atively general relationship between response propensities and response quality.
For example, higher levels of social capital could activate stronger norms of
cooperation (producing higher response propensities) and those same norms also
could influence respondentsÕ willingness to engage in more careful processing of
the survey questions. At the other end of the spectrum, busyness or time-stress
could produce a general disinclination both to participate in surveys and to re-
spond accurately if interviewed. Identifying and statistically controlling for such
shared explanatory factors would eliminate the relationship between response
propensity and data quality and provide a means for removing bias.
936 Fricker and Tourangeau
The purpose of this study was to explore the relationship between nonre-
sponse and data quality, and where evidence of covariation emerged, to exam-
ine potential common causal factors underlying the relationship. These issues
were investigated using data from two national household surveys: the Current
PopulationSurvey(CPS)andtheAmericanTimeUseSurvey(ATUS).Datafrom
eachsurveywereusedtodevelopresponse-propensitymodelstopredictthelikeli-
hood of nonresponse. The next section of this article provides an overview of the
CPS and ATUS and briefly describes the process of creating the datasets, propen-
sity models, and data-quality indicators used in this study. The remainder of the
article presents the results of the principal analyses investigating the relationship
between response propensity and data-quality indicators in each survey.
Data and Method
The Current Population Survey: The CPS is the primary labor-force survey in
the United States and is conducted by the U.S. Census Bureau for the Bureau of
Labor Statistics. Each month, the CPS surveys approximately 60,000 house-
holds in 792 sample areas across the country on issues such as employment,
earnings, and hours worked. Demographic and labor-force questions are asked
about the respondent and the other household members. Each CPS housing unit
is sampled on a rotational basis so that any given month includes eight different
rotation groups. Housing units within a given rotation group are sampled for
four consecutive months, are out of the sample for eight months, and then return
to the sample for another four consecutive months. This rotation pattern makes
it possible to match information on housing units monthly across their entire
CPS life cycle. The sampling unit in the CPS is the housing unit (that is, the
residence), not the household (the residents).
The data used to create the CPS propensity scores cover a two-and-a-half-
year period from May 2001 to October 2003, and include housing units that
were eligible for all eight CPS waves. To create a longitudinal data file of
CPS sample units, records from each month were matched based on household
ID, person ID, month-in-sample (MIS), and year. Housing units that were in-
eligible to participate in the CPS in any round by virtue of being vacant, demol-
ished, nonresidential, etc., were excluded, as were units in which all the
previous month's residents had moved and been replaced by an entirely differ-
ent group of residents.
The resulting CPS dataset contained information on 251,000 individuals.
Nonresponse in the CPS is a household-level phenomenon, so a household-
level dataset was created that included basic household information--e.g.,
the number of household members by age group, race, employment status, ed-
ucation level, etc.--as well as information about the main household respon-
dent. The main respondent was the household member who was the most
frequent CPS respondent over the household's eight waves. Over 95 percent
of households had a person who responded to the CPS four or more times,
Nonresponse Propensity and Data Quality 937
and many of them responded to all eight interviews.1 After collapsing to the
household level, the resulting CPS data file had household-level and main re-
spondent information for 97,053 cases.
Nonresponse in the CPS is relatively low compared to other national household
surveys,andtypicallyisworstinround1(whennoncontactishighest)andagainin
round 5 (when the household is returning to the CPS sample for the first time in
eight months). The average response rate for the period covered in our analyses
was 92.4 percent; rounds 1 and 5 had an average response rate of 90.1 and 90.9
percent, respectively, and the remaining rounds had an average response rate of
93.7 percent (American Association for Public Opinion Research [AAPOR], re-
sponse rate 3). Since the vast majority of CPS households respond to all eight
interviews, the CPSpropensity model developed for these analysesuseddata from
the first two waves to predict nonresponse at any wave in months three through
eight.A singleestimate ofoverall nonresponse propensity(notseparatingout non-
contact and noncooperation) was obtained for each household using a logistic re-
gressionmodelpredictingtheprobabilitythattheunitwouldbeanonrespondentin
anyofthelastsixCPSrounds.Predictorsinthismodelincludedlevelofeffort(e.g.,
call attempts) and demographic control variables, as well as variables related to
busyness and social capital constructs. Table 1 lists the variables that reached sig-
nificance in the final model. This model fit the data with a residual chi-square of
36.41 (df ¼ 12, p ¼ .0003), and a Max-rescaled R-squared of 0.1636.
On the basis of their predicted probabilities of nonresponse, households were
divided into propensity quintiles that ranged in average nonresponse propensity
from one percent for the lowest nonresponse propensity group (Group 1) to 30
percent for the highest (Group 5). Each CPS propensity quintile consisted of
approximately 19,400 households.
ThisstudyexaminedfourindicatorsofCPSdataquality:itemnonresponse,round
value reports, classification errors that potentially reflect spurious changes in
respondentsÕ answers between roundsofthe survey (e.g.,changesinreportedrace),
and interview-reinterview response variance.2 Eight CPS demographic items were
selected for inclusion: sex, age, race, ethnicity, educational attainment, homeown-
ership,telephone status,and family income. Theseitemswere chosenbecausethey
were asked of every household member at least once, were variables commonly of
interest to researchers, and offered a range of potential response errors. In addition,
eight of the most frequently asked labor-force items were examined. Data were
1. If two or more people in the household responded an equal number of times, person-level data
were retained for whichever person was the most closely related to the CPS reference person.
2. Each month, CPS attempts to conduct a response error reinterview on a one-percent subsample of
responding households. The reinterviews consist of the entire set of labor-force questions; house-
hold membership is dependently verified, and no reconciliation is conducted. An effort is made to
reinterview the person who responded to the original interview, but interviewers are allowed to
conduct the reinterview with other knowledgeable household members (U.S. Census Bureau
and Bureau of Labor Statistics 2002).
938 Fricker and Tourangeau
Table 1. Significant Predictors of Nonresponse in Waves 3­8 of CPS
Construct Variable Construct Variable
Controls
Respondent age, sex, race, origin
Social Capital
# of Non-family/relatives present
Urbanicity x region Household size
Citizenship
Season of CPS wave 1 interview
HH ownership (own vs. rent)
Level of Effort/Reluctance CPS nonresponse in wave 1 or 2
Racial diversity (county)
# of contact attempts in wave 1
Educational attainment (county)
Family income item nonresponse in wave 1
Median family income (tract)
Busyness Hours worked (wave 1)
Income inequality (county)
Employment status
All working adults in HH work
40þ hours per week?
Marital status
Presence of young child(ren)
Max-rescaled R-squared: 0.1637.
Nonresponse Propensity and Data Quality 939
aggregated across household members, variables, and survey waves to obtain an
overall value for each data-quality indicator for each household.
The percent household item nonresponse was calculated as follows:
PJ INR
¼
P
W
k ¼ 1
ð
P
Pj
i ¼ 1
mijk
mijk
þnijk
Þ, where PJ INR is the percent item nonresponse
for household j, mijk
is the total number of missing responses for person i in
household j in wave k, and nijk
is the total number of non-missing responses
for person i in household j in wave k, summing across all members (Pj
) of house-
hold j for all waves (W) in which the household responded to the CPS. The
percent round value reports and percent between-round classification changes
were calculated in a similar manner, except classification changes were summed
over wave pairs rather than waves.3 The percent of inconsistent responses
between the main CPS and the CPS reinterview was created for the 3,851
households in the dataset that participated in the CPS reinterview program.
Each of these data-quality indicators then was examined to see if it was related
to likelihood of CPS nonresponse. We first analyzed the four indicators across
propensity strata to assess the relative size and direction of the association. We
then explored the extent to which controlling for potential common causal var-
iables affected the association between indicators of data quality and nonre-
sponse propensity. Finally, we repeated these analyses using CPS sample
membersÕ actual response status in rounds three through eight--i.e., whether
they participated in all six rounds or were a nonrespondent in at least one of those
rounds--to examine the relationship between observedCPS nonresponse (rather
than respondentsÕ modeled nonresponse propensity) and CPS data quality.
American Time Use Survey (ATUS): The ATUS is a cross-sectional, computer-
assisted telephone survey that is carried out by the U.S. Census Bureau for the
Bureau of Labor Statistics. Its primary purpose is to provide national estimates of
how Americans spend their time. The ATUS sample is drawn from CPS house-
holds that have completed their eighth CPS interview. A single household mem-
ber from each responding CPS household is randomly selected to participate in
the ATUS interview two months after the eighth CPS interview. The designated
person is assigned a specific reporting day of the week (e.g., Monday); substi-
tutions are not allowed either for the designated ATUS respondent or for the
assigned reporting day. If the interview cannot be completed on the designated
day during the first week of the interviewing period, subsequent interview
attempts are made on the designated day each week for up to eight weeks.
To create the ATUS dataset used in these analyses, records from the January­
December 2003 ATUS public-use files were merged with the ATUS Call History
File. These files contained information about the respondent (e.g., updated de-
mographic and labor-force data), the household (e.g., composition,
3. A classification change indicates that a respondent provided different answers to the same ques-
tion asked in adjacent waves. The variables examined for this indicator were race, educational at-
tainment (restricted to individuals 30 years of age or older), housing tenure, and family income.
940 Fricker and Tourangeau
demographics, weight), the time-use activities of the designated ATUS respon-
dent, the interview process (e.g., interview outcome codes), and ATUS call his-
tories (e.g., outcome codes for individual call attempts). The files were merged
and matched to the CPS file, resulting in a final ATUS data file with 25,778
records.
The dependent variable for the ATUS propensity model was the interview out-
come (i.e., response vs. nonresponse). The respondent, household, and commu-
nity predictors included in the ATUS model were identical to those used in the
CPS analyses, with four exceptions. Three ATUS interview-process variables
were added: the number of call attempts made to ATUS sample members over
the eight-week ATUS fielding period, a variable indicating whether the desig-
nated ATUS respondent was the same person identified as the wave 8 CPS re-
spondent, and the time of day during which the majority of ATUS call attempts
were made. The ATUS model also included an indicator of CPS nonresponse
during rounds 3 through 8.4
As in the CPS analyses, a logistic regression model was used to estimate
a nonresponse propensity score for each ATUS sample member. Table 2 lists
the variables that reached significance in the final ATUS model. The model
fit the data with a residual chi-square of 29.38 (df ¼ 14, p ¼ .0093) and
a Max-rescaled R-squared of 0.3708. We then grouped ATUS respondents into
quintiles based on these propensity scores and ordered them from lowest non-
response propensity (average nonresponse propensity of 10.6 percent in the
lowest group) to highest (average propensity of 54.9 percent), with approxi-
mately 3,275 cases in each group.
For each of the 20,698 individuals in the dataset who participated in ATUS,
we created four data-quality indicators: (1) total number of diary activities
reported; (2) missing diary reports of basic daily activities;5 (3) round values
for activity durations; and (4) item nonresponse on ATUS labor-force ques-
tions. As in the CPS analyses, we began by examining the means for the four
indicators across propensity strata and then assessed the effects of controlling
for potential share explanatory factors. We then conducted parallel analyses to
examine how ATUS data quality varied as a function of nonresponse in the CPS
and ATUS refusal conversion. We also analyzed the association between CPS
4. The use of the overall CPS response-propensity score resulting from multivariate logistic regres-
sion models run on the CPS predictors was evaluated as an alternative to the raw measure of re-
sponse status in CPS waves 3­8, but was found to be less predictive. Therefore, the raw measure was
used instead. This measure of CPS nonresponse in Rounds 3 through 8 replaced the measure of CPS
nonresponse in Rounds 1 or 2.
5. In a given day, most people sleep, eat, and perform personal-care activities (e.g., grooming,
dressing, going to the bathroom). When diaries do not contain one or more of these basic activities
in the 24-hour period, it may be an indication that respondents intentionally omitted some behaviors
or simply did not try to report their activities accurately. We coded the number of times these basic
activities were reported, and flagged cases for which there was no data. A surprisingly large number
of people (31.6 percent of ATUS respondents) failed to report at least one of these activities.
Nonresponse Propensity and Data Quality 941
Table 2. Significant Predictors of ATUS Nonresponse
Construct Variable Construct Variable
Controls
Respondent age, race, origin
Social Capital
# of Non-family/relatives present
Family income
Level of Effort/Reluctance
CPS nonresponse in waves 3­8
Employment status
# of call attempts
ATUS respondent same as CPS
Marital status
Family income item nonresponse
in CPS
Presence of young child(ren)
Busyness
Percent of HH adults who work
Median family income (tract)
Occupation type (executive/professional,
service, support/production,
not in labor force)
Racial diversity (county)
Diversity x region
Max-rescaled R-squared: 0.3708
942 Fricker and Tourangeau
data-quality indicators and ATUS response status to see if poor response quality
on the CPS was associated with ATUS nonresponse.
Results
CPS: Figure 1 presents the relationship between the CPS data-quality indica-
tors and CPS nonresponse propensity. The graph displays five nonresponse-
propensity strata, with likelihood of nonresponse increasing from left to right
along the x-axis. In addition, the figure presents data-quality indicators that have
been standardized into standard-deviation units in order to make it easier to
compare the relative strength of each measureÕs association with propensity.
We ran regression models (regressing the individual indicators on nonresponse
propensity) to obtain slope estimates and significance tests (ANOVA models
also were run to check for nonlinear trends).
There are two main points to take away from this figure. First, the overall
quality of CPS reports appears to decrease across nonresponse-propensity
strata. Taking the mean data-quality score within each strata (by averaging
across the four indicators), we see that there is a monotonic increase in error
as nonresponse propensity rises. Second, the strength of the covariance between
propensity and error is highly dependent upon the type of data-quality indicator.
The relationship is strongest for item nonresponse (ß ¼ .17, p < .001): House-
holds with the highest probability of nonresponse had item-missing rates that
were almost a full standard deviation (or about six percentage points) higher
than households with the lowest nonresponse propensity. Round value reports
also were significantly related to nonresponse propensity, though the strength
of the association was about two-thirds that of item nonresponse (ß ¼ .11,
p < .001). The highest nonresponse-propensity households provided about
10 percent more round value reports than the lowest-propensity households.
In contrast, nonresponse propensity was only weakly associated with the
percent inconsistent reports between the basic CPS and reinterview (ß ¼
.07, p ¼ .021) and in fact was slightly negatively correlated with the measure
of between-wave classification changes (ß ¼ À.05, p < .001).6
Why might item nonresponse and round value reports be related to the level
of nonresponse propensity? One possibility is that this relationship may result
from a shared explanatory factor (or factors). If the shared factors model is cor-
rect, and the model is correctly specified with the appropriate explanatory var-
iable(s), then the relationship between response propensity and data quality will
be eliminated once the shared explanatory factors are statistically controlled.
6. Given the relatively small sample size of the reinterview dataset and the fact that the between-
wave classification change estimate itself likely had significant error (since some ``true'' change
could occur between waves for some of the variables used in this measure), it is not surprising
that these two indicators proved less strongly and consistently related to nonresponse propensity.
Nonresponse Propensity and Data Quality 943
To test the shared explanatory factors hypothesis, we examined several fac-
tors that potentially could contribute to both the likelihood of unit nonresponse
and measurement error. Busyness and social capital, as discussed earlier, are
two possible relatively general common causal candidates, and we included
them in the present analyses. A third possibility is survey burden. In a panel
survey like the CPS, the level of burden respondentsÕ experience in one wave
may affect both their likelihood of response in subsequent waves and their will-
ingness to answer fully and accurately if they do participate. Since we did not
have direct measures of these three factors, we examined a number of indicators
for each construct. Hours worked and commute time served as indicators of
busyness. For social capital, we examined marital status, homeownership,
the presence of children in the household, and educational achievement in
the community. Item burden (i.e., the number of items asked during the first
two CPS waves) served as the measure of survey burden.
We began by looking at the effects of each of these variables individually on
the association between nonresponse propensity and the two indicators of data
quality that showed the strongest association with nonresponse propensity (item
nonresponse and round value reports). If the covariance evident in figure 1 is the
result of one of these common causal variables, then we would expect the co-
variance to diminish or go to zero after controlling for that variable. However,
we found no evidence that busyness, social capital, or survey burden (at least as
operationalized here) had any mediating effect on the relationship between pro-
pensity and data quality. This is illustrated in figure 2 for the item nonresponse
Figure 1. Relationship of CPS Data-quality Indicators (in Standard Devi-
ation Units) to CPS Nonresponse Propensity.
944 Fricker and Tourangeau
measure. To give a better sense of the magnitude of the effects, we present the
raw item-nonresponse rates in figure 2, not the standardized measure. This fig-
ure reveals that the level of reporting error continued to covary with nonre-
sponse propensity even after we took into account measures of busyness
(top-left panel), survey burden (top-right panel), and social capital (bottom
two panels). The shapes of the curves in this figure are essentially the same
as those found in figure 1, and this finding also was true for the other common
causal variables (not presented here) that we examined. The same pattern was
apparent when we examined the effect of common causal variables on the re-
lationship of CPS nonresponse propensity and round value reports. Figure 2
also underscores the practical impact of bringing in difficult-to-contact or re-
luctant respondents. The levels of missingness for most items in the CPS
are quite low--generally less than three percent. But respondents in the
high-nonresponse-propensity group often have item-missing rates that are
two or three times that level.
Figure 2. Effects of Potential Common Cause Variables on the Relation-
ship Between CPS Item Nonresponse and Unit Nonresponse Propensity.
Nonresponse Propensity and Data Quality 945
We next ran a simple regression model that used nonresponse propensity to
predict item nonresponse (or round value reports). We compared the results of
this model to those from a series of models that also included one of the shared
explanatory variables. The results of this analysis confirmed what was evident
in figure 2; that is, controlling for individual common causal variables had little
effect on the size or direction of the relationship between nonresponse propen-
sity and data quality. Moreover, this relationship was evident even when more
complex, multivariate models were run that controlled for multiple common
causal variables simultaneously.
The preceding analyses revealed a positive relationship between CPS non-
response propensity and measurement error, but we can also look to see whether
measurement error varied as a function of actual CPS nonresponse. And, in
fact, it did. The effects of CPS nonresponse mirror those from the nonre-
sponse-propensity analyses. Item nonresponse (ß ¼ .75, p < .001) and to a les-
ser extent round value reports (ß ¼ .10, p < .001) were significantly and
positively related to actual CPS nonresponse in rounds 3 through 8, whereas
changes in classifications between CPS waves (ß ¼ À.17, p < .001) and basic
interview-reinterview response inconsistencies (ß ¼ À.06, p ¼ .244) were neg-
atively related to nonresponse. Moreover, controlling for potential common
causal variables--both individually and in multivariate analyses--had little ef-
fect on the associations between CPS unit nonresponse in waves 3 through 8
and CPS data quality across waves. This finding is illustrated in table 3, which
presents the results of a multivariate regression analyses predicting CPS item
nonresponse from CPS unit nonresponse and a set of shared explanatory var-
iables. It reveals that this association remains when the common causal vari-
ables are statistically controlled, though this model accounted for less variance
than the corresponding models with CPS nonresponse propensities (R-square ¼
0.068 vs. 0.129, respectively).
ATUS: We examined the relationship between CPS data-quality indicators
and ATUS response status to see if the CPS quality measures could be used
as a predictor of ATUS unit nonresponse. Table 4 presents the weighted mean
percents of the CPS data-quality indicators for ATUS respondents and nonres-
pondents, the associated t-values, and zero-order correlations between ATUS
response status and each of the CPS measures. The strongest effect was for CPS
item nonresponse--ATUS nonrespondents had significantly higher CPS item-
missing-data rates than ATUS respondents. ATUS nonrespondents also had
significantly more round values in their CPS answers than ATUS respondents,
but the relative difference between these groups in reporting of round values
was quite small given the large amount of round reporting overall, and the cor-
relation of round reporting with ATUS response status was considerably
smaller than that for item nonresponse. In addition, there was a small, negative
correlation between ATUS response status and CPS between-wave changes in
classification--ATUS nonrespondents had fewer between-wave changes than
946 Fricker and Tourangeau
ATUS respondents. When we examined the small number of ATUS cases that
also had been in the CPS reinterview program, there were no differences be-
tween ATUS respondents and nonrespondents in the level of CPS interview-
reinterview response inconsistencies.
Figure 3 presents the relationship between ATUS data-quality indicators and
ATUS nonresponse propensity. As before, the graph shows nonresponse
Table 3. Multivariate Regression Model Predicting CPS Item
Nonresponse (standardized) from CPS Unit Nonresponse in Waves 3­8
and Potential Common Causal Variables
Effect Estimate t F Sig
Age 0.01 6.00 36.02 <.0001
Education 29.96 <.0001
Less than HS vs. advanced 0.12 8.71 <.0001
HS only vs. advanced 0.09 7.67 <.0001
Some college vs. advanced 0.04 3.26 .0011
BA/BS vs. advanced 0.04 3.51 .0005
CPS nonresponse waves 3­8 5425.18 <.0001
No vs. yes À0.74 À73.66 <.0001
Hours worked À0.00 À2.60 6.75 .0094
Item burden 53.62 <.0001
Low vs. high À0.08 À10.35 <.0001
Medium vs. high À0.06 À5.94 <.0001
Marital status 83.04 <.0001
Unmarried vs. married 0.07 9.11 <.0001
Presence of young child 233.68 <.0001
None vs. older 0.14 15.28 <.0001
Young vs. older À0.08 À6.96 <.0001
R-square ¼ 0.068; n ¼ 97, 053
Table 4. Relation of CPS Data-quality Indicators to ATUS Outcome
CPS Data-
quality
Measure
ATUS
Respondent
(n ¼ 16,372)
ATUS
Nonrespondent
(n ¼ 9,406) t-value p-value
Correlation
with ATUS
Nonresponse
Item-missing
rate
2.3% 4.1% À27.83 0.0001 0.19
Round value
reports
73.0 74.7 À5.62 0.0001 0.08
Change in
classifications
(basic CPS)
7.0 6.8 2.01 0.0445 À0.02
Inconsistent
reports(CPS
reinterview)
12.9 13.1 À0.18 0.8605 0.01
Nonresponse Propensity and Data Quality 947
propensity increasing from left to right along the x-axis and the data-quality
indicators are presented in standard-deviation units. For three of the measures--
activity durations reported as round values, missing activity reports, and labor-
force-item nonresponse--points above the zero-deviation line indicate poorer
data quality; points below the zero-deviation line indicate better data quality.
For the total number of diary activities reported, however, this is reversed--
points above the zero-deviation line indicate that respondents reported more
than the average number of activities; points below the line indicate that they
reported less than the average.
As can be seen in the figure, there is a linear trend between nonresponse
propensities and overall data quality in the ATUS. If we aggregate the standard-
ized scores from the four data-quality indicators within each nonresponse strata
(after flipping the signs for the total activity measure), we see that error
increases with nonresponse propensity (see figure 4). However, the size of this
effect is small--less than 0.2 standard deviations separate the lowest- and high-
est-propensity groups.
This small difference reflects the relatively weak correlations between non-
response propensity and the individual data-quality measures. Although each
is positively (and significantly) related to nonresponse propensity, the only
effect with any practical significance is for the total number of diary activity
reports (ß ¼ .05, p < .001). Respondents in the highest nonresponse-propensity
group reported about three fewer diary activities than respondents in the
lowest-propensity group, which amounts to roughly 15 percent of the typical
number of activities reported (20). When we couple this fact with the finding
Figure 3. Relationship of ATUS Data-quality Indicators (in Standard
Deviation Units) to ATUS Nonresponse Propensity.
948 Fricker and Tourangeau
that respondents in the high nonresponse-propensity group also are more likely
than other sample members to report activities in round time blocks, neglect to
report basic daily activities, and provide incomplete data on ATUS labor-force
items, it raises questions about the impact of including these individuals in
ATUS estimates.
Does the significant relationship between nonresponse and total activity
reports disappear when we control for the effects of potential common causal
factors? We examined the impact of several potential common causal variables
and present some representative results in figure 5. None of the common causal
variables examined (including those not presented here) significantly weakened
the covariance between nonresponse propensities and the ATUS data-quality
indicators. The overall magnitude and direction of the relationship between
nonresponse and total activities was very similar to that shown in figure 3.
Regression analyses that controlled for the common causal variables individ-
ually and then multivariately corroborate these findings; nonresponse continued
to be significantly related to the total number of items reported in the ATUS
time diary even when busyness, social capital, and survey burden variables
were taken into account. Fewer activities were reported by respondents with
high nonresponse propensities and for those without children; the number of
activity reports also was negatively correlated with hours worked and positively
correlated with educational attainment.
We carried out additional analyses that examined the association between
ATUS data quality and other indicators of ATUS response propensity: CPS
unit nonresponse and refusal conversion in the ATUS. To examine the relation-
ship between CPS unit nonresponse and ATUS data quality, we classified
Figure 4. Average ATUS Diary Error (in Standard Deviation Units) by
ATUS Nonresponse Propensity Group.
Nonresponse Propensity and Data Quality 949
ATUS respondents into three groups: those who participated in all eight rounds
of CPS (92.7 percent), those who failed to participate in one CPS round (5.5
percent), and those who failed to participate in two or more rounds of the CPS
(1.8 percent).7 Figure 6 shows the relation of the CPS response-status variable
to the four ATUS data-quality indicators. No difference was found in data qual-
ity between ATUS respondents who participated in every CPS interview and
those who were nonrespondents in a single CPS round. However, ATUS
respondents who failed to participate in two or more rounds of CPS provided
poorer ATUS data--more round duration reports, missed diary activities, more
item nonresponse on ATUS labor-force questions, and fewer diary reports over-
all--than those who always participated in CPS or those who were only
Figure 5. Effects of Potential Common Cause Variables on the Relation-
ship Between the Number of ATUS Diary Reports and ATUS Nonre-
sponse Propensity.
7. All ATUS respondents participated in wave 8 of the CPS, by definition.
950 Fricker and Tourangeau
nonrespondents in one round. Regression analyses run on the individual data-
quality indicators revealed that only labor-force-item nonresponse and total
reported activities were significantly related to CPS unit nonresponse. As be-
fore, data quality and nonresponse covaried even after controlling for potential
common causal variables.
We also examined whether there were differences in data quality between
ATUS respondents who were refusal conversions and those who were not. Ap-
proximately 20 percent of ATUS sample members were flagged as a refusal at
least once during the fielding period, and about five percent of ATUS respond-
ents were successfully converted. Consistent with previous findings, figure 7
shows that data quality was worse for refusal conversion cases than those that
never refused, though regression analyses revealed significant effects only for
the total number of diary activities and missing diary reports measures. Again,
controlling for various potential shared explanatory variables did not elimi-
nate the relationship between refusal conversion status and the data-quality
indicators in ATUS.
Discussion
The purpose of this study was to explore the relationship between response
propensity and survey data quality. There were three main findings from these
analyses. First, data quality decreased as the probability of nonresponse in-
creased. Second, the strength of this relationship varied by data-quality
Figure 6. Relationship of ATUS Data-quality Indicators (in Standard
Deviation Units) to the Amount of CPS Unit Nonresponse.
Nonresponse Propensity and Data Quality 951
indicator and by survey. The effects were stronger in the CPS, where nonre-
sponse propensity was most strongly and positively related to item nonresponse
and round value reports on continuous variables (e.g., hours and earnings). In
ATUS, the relationship of nonresponse propensity to three of the four data-qual-
ity indicators had essentially no practical significance. There was, however,
a moderate, positive, and monotonic association between the total number
of reported diary activities and the likelihood of nonresponse. Third, when data
quality and nonresponse did covary, controlling for potential shared explana-
tory variables related to busyness, social capital, and survey burden did not
weaken the relationship. Data quality continued to decline as nonresponse pro-
pensity rose, though there were main effects for some of the potential common
causal variables.
There were several limitations in our approach to assessing the link between
nonresponse propensity and data quality. Our study was hampered by our use of
relatively indirect indicators of social capital, busyness, survey burden, and data
quality. In the absence of better frame and validation data, we relied on the
indirect indicators that researchers tend to use in these types of analyses,
and brought in additional measures that are less frequently incorporated
(e.g., social-environmental variables in our propensity models, round value
reports as a data-quality measure). In the end, the fit of our nonresponse pro-
pensity models (adjusted R-squared values of 0.164 and 0.371), while not
Figure 7. Relationship Between ATUS Refusal Conversion Status and
ATUS Data-quality Indicators (in Standard Deviation Units).
952 Fricker and Tourangeau
outstanding, was comparable to or exceeded similar models used to examine
these data (e.g., Dixon and Tucker 2000; Groves and Couper 1998; Abraham,
Maitland, and Bianchi 2006). Given our reliance on indirect indicators, the fact
that we observed some clear connections between these variables and response
propensities and reporting errors was encouraging (see, for example, tables 1
and 2).
Still, our approach to modeling response propensities and the unique design
features of both surveys may limit the generalizability of our results. We are in
fact modeling conditional nonresponse or forms of attrition, since we used
responses from the first two CPS waves to predict nonresponse in subsequent
waves and because the ATUS sample units come from households rotating
out of the CPS. Our results therefore may tell us little about predictors of
nonresponse propensity for the initial CPS wave, or more generally for
cross-sectional surveys and random-digit-dialing samples. That said, virtually
everyone participates in the first CPS interview, and most stick with the survey
for all eight rounds.
These analyses have implications for surveys and survey organizations
that strive for the highest response rates possible. Often, extraordinary per-
suasive efforts are made to bring difficult-to-contact or reluctant sample
members into the respondent pool. The assumption is that these efforts
are compensated by reductions in the total mean square error of survey sta-
tistics. Recent work by Curtin, Presser, and Singer (2005), Keeter et al.
(2000), and others casts some doubt on this assumption, at least with respect
to nonresponse error. The present analyses extend this work in two distinct
ways. On the one hand, it demonstrates that bringing in low-propensity
respondents may also introduce statistically significant increases in mea-
surement error. If nonresponse error is not significantly increased by exclud-
ing low-propensity cases (as the studies by Curtin, Presser, and Singer and
Keeter et al. suggest) and these cases also are likely to add measurement
error (as we see here), then survey organizations may more comfortably
divert resources away from recruitment of difficult respondents and focus
instead on other error-reduction techniques. On the other hand, evidence
of covariation between measurement error and nonresponse may call into
question previous investigations of nonresponse bias. The results of this
study suggest that significant measurement error in late/difficult cases
may in fact be concealing nonresponse bias undetected when examining
respondent means (Groves 2006). Such cases may be candidates for methods
that remove measurement error bias from the observations in order to assess
nonresponse bias more accurately (see, e.g., Biemer 2001). If lower-propen-
sity individuals also are more likely to produce noisy data (increasing the
variance of the statistic), then it becomes more difficult to detect whether
these individuals are different from higher-propensity respondents; that
is, it becomes more difficult to determine the effects of excluding these indi-
viduals on nonresponse bias.
Nonresponse Propensity and Data Quality 953
References
Abraham, Katherine G., Aaron Maitland, and Suzanne M. Bianchi. 2006. ``Nonresponse in the
American Time Use Survey: Who Is Missing from the Data and How Much Does It Matter?''
Public Opinion Quarterly 70(5):676­703.
Biemer, Paul P. 2001. ``Nonresponse Bias and Measurement Bias in a Comparison of Face-to-face
and Telephone Interviewing.'' Journal of Official Statistics 17(2):295­320.
Bollinger, Christopher R., and Martin H. David. 2001. ``Estimation with Response Error and Non-
response: Food Stamp Participation in the SIPP.'' Journal of Business and Economic Statistics
19(2):129­41.
Cannell, Charles F., and Floyd J. Fowler. 1963. ``Comparison of a Self-enumerative Procedure and
a Personal Interview: A Validation Study.'' Public Opinion Quarterly 27(2):250­64.
Curtin, Richard, Stanley Presser, and Eleanor Singer. 2000. ``The Effects of Response
Rate Changes on the Index of Consumer Sentiment.'' Public Opinion Quarterly 64(4)
413­28.
------. 2005. ``Changes in Telephone Survey Nonresponse over the Past Quarter Century.'' Public
Opinion Quarterly 69(1):87­98.
Dixon, John, and N. Clyde Tucker. 2000. ``Modeling Household and Interviewer Nonresponse Rates
from Household and Regional Characteristics.'' Paper presented at the International Workshop
on Household Survey Nonresponse, Budapest. Available at http://www.fcsm.gov/committees/
ihsng/buda5.pdf.
Friedman, Ester, Nancy A. Clusen, and Michael Hartzell. 2003. ``Better Late? Characteristics of Late
Respondents to a Health Care Survey.'' Proceedings of the American Statistical Association, Survey
Research Methods Section [CD-ROM]. Alexandria, VA: American Statistical Association.
Groves, Robert M. 2006. ``Nonresponse Rates and Nonresponse Bias in Household Surveys.''
Public Opinion Quarterly 70(5):646­75.
Groves, Robert M., and Mick P. Couper. 1998. Nonresponse in Household Interview Surveys. New
York: John Wiley.
Groves, Robert M., Stanley Presser, and Sarah Dipko. 2004. ``The Role of Topic Interest in Survey
Participation Decisions.'' Public Opinion Quarterly 68(1):2­31.
Kalsbeek, William D., Steven L. Botman, James T. Massey, and Pao-Wen Liu. 1994. ``Cost-
efficiency and the Number of Allowable Call Attempts in the National Health Interview Survey.''
Journal of Official Statistics 10(2):133­52.
Keeter, Scott, Andrew Kohut, Carolyn Miller, Robert Groves, and Stanley Presser. 2000. ``Con-
sequences of Reducing Nonresponse in a Large National Telephone Survey.'' Public Opinion
Quarterly 64(2):125­48.
Merkle, Daniel, and Murray Edelman. 2002. ``Nonresponse in Exit Polls: A Comprehensive Anal-
ysis.'' In Survey Nonresponse, ed. R. Groves, D. Dillman, J. Eltinge, and R. Little. New York:
John Wiley and Sons, 243­58.
Olson, Kristen M. 2006. ``Survey Participation, Nonresponse Bias, Measurement Error Bias, and
Total Bias.'' Public Opinion Quarterly 70(5):737­58.
Tourangeau, Roger, Robert M Groves, and Cleo D. Redline. 2010. ``Sensitive Topics and Reluctant
Respondents: Demonstrating a Link Between Nonresponse Bias and Measurement Error.'' Public
Opinion Quarterly 74(3):413­32.
Triplett, Timothy, Johnny Blair, Teresa Hamilton, and Yun Chiao Kang. 1996. ``Initial Cooperators
vs. Converted Refusers: Are There Response Behavior Differences?'' Proceedings of the Amer-
ican Statistical Association, Survey Research Methods Section [CD-ROM]. Alexandria, VA:
American Statistical Association 1038­41.
U.S. Census Bureau and Bureau of Labor Statistics. 2002. Current Population Survey: Design and
Methodology. Technical Paper 63RV: Washington, DC.
Voigt, Lynda, Denise M. Boudreau, Noel S. Weiss, Kathleen E. Malone, Christopher I. Li, and Janet
R. Daling. 2005. ``Letter to the Editor: RE: ÔStudies with Low Response Proportions May Be Less
954 Fricker and Tourangeau
Biased Than Studies with High Response Proportions.Õ'' American Journal of Epidemiology.
161(4):401­2.
Willimack, Diane K., Howard Schuman, and James M. Lepkowski. 1995. ``Effects of a Prepaid
Nonmonetary Incentive on Response Rates and Response Quality in a Face-to-face Survey.''
Public Opinion Quarterly 59(1):78­92.
Yan, Ting, Roger Tourangeau, and Zac Arens. 2004. ``When Less Is More: Are Reluctant Respond-
ents Poor Reporters?'' Proceedings of the American Statistical Association, Survey Research
Methods Section [CD-ROM]. Alexandria, VA: American Statistical Association: 4632­51.
Nonresponse Propensity and Data Quality 955
