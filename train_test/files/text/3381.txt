Models of human­automation interaction con-
cerned with "who should do what" or "who should be
responsible for what" fail to appreciate how complex
cognitive work is carried out in sociotechnical systems.
Such systems,which have complex problem spaces with
high levels of instability,uncertainty,and unpredictability,
are necessarily self-organizing. In these systems, actors'
behaviors and structures are closely interconnected, or
correlated, and vary continuously in ways that are fit-
ted,or adapted,to the demands of a constantly evolving
work environment.To preserve the capacity of socio-
technical systems for self-organization, and thus their
capacity for dealing with the complexity of their work
environments, the design of human­automation inter-
action must be concerned fundamentally with identi-
fying the limits on "who can do what" and providing
constrained flexibility in the workplace.
Keywords: adaptation,human­automation interaction,
self-organization
David Kaber (2018 [this issue]), in his arti-
cle in this issue of the Journal of Cognitive
Engineering and Decision Making, considers a
variety of issues relating to models of human­
automation interaction in complex systems,
including the need for specifications of what the
human and automation will do, and how, for the
purposes of effective design. In this commen-
tary, I argue that models of human­automation
interaction that focus on "who should do what"
or on "who should be responsible for what" fail
to appreciate how complex cognitive work is
carried out in sociotechnical systems, which
are necessarily self-organizing. To this end, I
discuss some of the essential characteristics of
sociotechnical systems, empirical observations
of how work unfolds in these contexts, and the
kinds of models that are needed for creating
effective designs for self-organizing systems.
Some of the most demanding aspects of per-
forming cognitive work in sociotechnical systems
is that they tend to present very large and complex
problem spaces characterized by high degrees of
instability, uncertainty, and unpredictability (Nai-
kar, 2013; Naikar & Brady, in press). Workers
must contend with dynamic operating conditions,
in which the problems, demands, and pressures
they are faced with change or evolve constantly.
For example, the particular challenges posed by a
wildfire to emergency management workers may
fluctuate widely depending on the current and
anticipated weather conditions or population sizes
and infrastructure in areas to which the fire is
spreading. Furthermore, workers must deal with
considerable uncertainty, especially in relation to
the accuracy or completeness of the information
available to them. In the naval domain, for
instance, imperfections in acoustics sensors for
detecting objects submerged underwater may
mean that the true situation can never be estab-
lished with complete certainty. Finally, adding to
this complexity is the inherent unpredictability of
the work requirements. Workers must contend
with events that have not been--and cannot be--
foreseen or specified fully by analysts or designers
(e.g., Leveson, 1995; Perrow, 1984; Rasmussen,
1968a, 1968b, 1969; Reason, 1990; Vicente,
1999), such as a new kind of military threat (Her-
zog, 2011; Reich, Weinstein, Wild, & Cabanlong,
2010), an unexpected reaction of a patient to an
anesthetic during surgery (Hoppe & Popham,
2007), or an unforeseen chain of supplier col-
lapses in the wake of a natural disaster (Park,
Hong, & Roh, 2013).
731223
EDMXXX10.1177/1555343417731223Journal of Cognitive Engineering and Decision MakingHAI in Self-Organizing Sociotechnical Systems
2017
Address correspondence to Neelam Naikar, Defence
Science and Technology Group, Melbourne, 506
Lorimer St, Fishermans Bend, Victoria 3207, Australia,
neelam.naikar@dst.defence.gov.au.
Special Issue
Human­Automation Interaction in Self-Organizing
Sociotechnical Systems
Neelam Naikar, Defence Science and Technology Group
Journal of Cognitive Engineering and Decision Making
2018, Volume 12, Number 1, March 2018, pp. 62­66
DOI: 10.1177/1555343417731223
Copyright © 2017, Commonwealth of Australia.
HAI in Self-Organizing Sociotechnical Systems 63
These fundamental characteristics of socio-
technical systems make adaptation necessary for
their viability. Workers cannot rely solely on
executing well-established procedures or trained
task sequences or on recognizing suitable solu-
tions from their prior experiences, even if they
have extensive know-how (Rasmussen, 1986;
Rasmussen, Pejtersen, & Goodstein, 1994;
Vicente, 1999). Instead, workers must be capa-
ble of continuously and reliably adapting their
actions to the evolving nature of the problem,
sometimes exercising significant ingenuity or
creativity.
A variety of empirical studies demonstrate
that adaptations in sociotechnical systems can
be observed at the levels of the behaviors of
individual actors and the structures of multiple
actors (Naikar & Brady, in press; Naikar & Elix,
2016). In other words, workers in these systems
function within groups or teams, or within a
social context, and they adapt not only their indi-
vidual behaviors but also their structures, or
organization, in line with the evolving situation
(e.g., Bigley & Roberts, 2001; Bogdanovic,
Perry, Guggenheim, & Manser, 2015; Rochlin,
LaPorte, & Roberts, 1987). For example, in a
field study of a widespread approach to emer-
gency management in the United States (Bigley
& Roberts, 2001), some of the kinds of behav-
ioral adaptations that were observed included
improvisations in the use of tools, such as those
available on a fire truck at the scene of an emer-
gency, and adjustments in standard routines,
such as those for "hose laying" or "ladder throw-
ing." Moreover, sometimes the adaptations
included departures from rules, so that workers
were directly breaching standard operating pro-
cedures. For instance, one procedure prohibits
firefighting teams from approaching a fire from
opposite positions, as one group can push the
fire into another. However, firefighters discussed
a situation in which "opposing hose streams"
was in fact used as the primary tactic.
Field studies of sociotechnical systems also
provide strong evidence of structural adaptation.
As an example, Rochlin et al. (1987) found that
the work organization on naval aircraft carriers
shifts spontaneously between formal and infor-
mal structures. The formal structure, that which
is documented on paper, is rigid, hierarchical,
and centralized, and typically governs opera-
tions on a ship. However, during complex opera-
tions, a different kind of organizational structure
is exhibited. This structure is informal in that it
is not officially documented, and it is flat and
distributed. For instance, faced with significant
time constraints, lower-ranked personnel can
make critical decisions without the approval of
officials with higher rankings. The work organi-
zation on the ship is flexible in that there is no
prespecified plan for when the shifts between
formal and informal structures will occur. More-
over, the informal structure that is adopted
on any one occasion is emergent. That is, there is
no single, fixed mapping between people and
roles. Instead, the mapping changes with the
circumstances.
In this context, the concept of self-organization
is important (Naikar & Brady, in press; Naikar,
Elix, Dâgge, & Caldwell, 2017). This concept can
provide an explanation of how behavioral and
structural adaptations in sociotechnical systems
can be achieved spontaneously, continuously, and
relatively seamlessly, consistent with the observa-
tions of field studies (e.g., Bigley & Roberts,
2001; Bogdanovic et al., 2015; Rochlin et al.,
1987). Specifically, field studies demonstrate that
new organizational structures may be observed in
sociotechnical systems that are not planned a pri-
ori, centrally coordinated, or imposed by external
agents but instead appear to be a spontaneous
response of the system itself to challenges posed
by its environment. The self-organization concept
(e.g., Fuchs, 2004; Haken, 1988; Heylighen, 2001;
Hofkirchner, 1998) suggests that a system's struc-
ture may constrain its response in ways that are
unsuitable or ineffective when particular circum-
stances are encountered (Figure 1). However, in
responding to the local conditions, individual,
interacting actors may engage in spontaneous
behaviors from which novel structures emerge
that enable the system to respond appropriately to
the circumstances. A new structure may be suit-
able for a time, constraining and enabling behav-
iors in ways that are appropriate to the circum-
stances--until the situation changes--and the
spontaneous actions of individual, interacting
actors result in further structural changes.
Both actors' structures and behaviors,
then, may evolve continuously to deal with the
64 March 2018 - Journal of Cognitive Engineering and Decision Making
challenges posed by the environment. More-
over, although this self-organizing process may
not be perfect, it may be relatively seamless,
particularly in well-established systems, and in
any case, it is necessary. Significant events occur
too quickly and conditions are too unstable,
uncertain, and unpredictable for a priori plan-
ning, centralized coordination, or external inter-
vention to provide feasible strategies consis-
tently. Instead, flexibility and adaptation within
the system are required to resolve--in time and
in context of the local operating conditions--the
"proper, immediate balance" between the sys-
tem's safety and productivity goals (Rochlin
et al., 1987, pp. 83­84).
The phenomenon of self-organization has
significant implications for the design of human­
automation interaction in sociotechnical sys-
tems. Designs based on normative or prescrip-
tive models of "who should do what" may
compromise performance by limiting the
capacity of a system for self-organization, and
thus its capacity for dealing with instability,
uncertainty, and unpredictability. In addition,
models of human­automation interaction
focused either at the level of behavior or struc-
ture will not be sufficient. Designs based pre-
dominantly on the behaviors of individual
actors may have unintended, negative conse-
quences for the structural relationships between
these actors, and the converse is also true.
Instead, human­automation interaction must
be conceived in the context of both structure
and behavior--in an integrated manner--and
thus in the context of self-organizing systems.
A suitable starting point for achieving these
design objectives is offered by the diagram
of work organization possibilities (or WOP
diagram), a modeling tool recently proposed as
an extension to the cognitive work analysis
framework, specifically the social organization
and cooperation dimension (Naikar & Elix,
2016). This tool models the structural possibili-
ties of multiple actors and the behavioral oppor-
tunities of individual actors within a single, inte-
grated representation in a way that is consistent
with the phenomenon of self-organization (Fig-
ure 2; Naikar et al., 2017). The set of work orga-
nization possibilities encapsulated in such a
model is independent of specific circumstances,
so that the model is relevant to recurring situa-
tions or novel or unforeseen events. Thus this
tool has the potential to foster the development
of human­automation interaction designs that
do not limit artificially a system's capacity for
self-organization and therefore its capacity for
dealing with a dynamic and ambiguous work
environment.
Specifically, a key design intent is to preserve
the inherent structural possibilities of multiple
actors and behavioral opportunities of individual
actors, so that new or different temporal, spatial,
or functional structures can emerge from the
spontaneous behaviors of interacting actors,
constrained by a set of boundary conditions for
safe and productive performance, consistent
with the cognitive work analysis framework.
Notably, the actors in a WOP diagram may
encompass both human workers and automated
agents, so that the emergent structures may
encompass a mix of human and machine actors.
Ultimately, such a design approach reveals the
limits on "who can do what" rather than specify-
ing "who should do what," which can be con-
ceived only in the context of events that are
known or can be anticipated in detail.
The boundaries or limits on action are estab-
lished by analyzing the organizational con-
straints, or the relationships between the action
possibilities afforded by the work context, or the
environment in which actors are situated, and
the actors themselves (Naikar et al., 2017). As a
result, the WOP diagram simultaneously reveals
the behavioral opportunities of individual actors
and the structural possibilities of multiple actors.
The structural possibilities are interconnected
with actors' behavioral opportunities in a man-
ner that accommodates the observation that, in
Figure 1. Social self-organization. Adapted from
Hofkirchner (1998).
HAI in Self-Organizing Sociotechnical Systems 65
self-organizing systems, the structural forms of
multiple actors emerge from the flexible actions
of individual, interacting actors while at the
same time constraining or enabling their behav-
iors, in ways that are fitted to the circumstances
(cf. Figure 1). Thus, it is proposed that designs
based on this diagram may facilitate the emer-
gence of novel temporal, spatial, or functional
structures from the flexible behaviors of indi-
vidual, interacting actors, such that the system
has greater capacity for adapting to a constantly
evolving work environment.
A particular design goal within this approach
is to facilitate effective interrelationships between
human and machine actors as a central means
for promoting emergence and adaptation of the
system to the work environment. The relational
possibilities of human and machine actors must
be comprehended in view of each actor's full,
although meaningfully constrained, spaces of
opportunities for action. The specific intent is to
provide actors with pathways for interaction that
can preserve each actor's degrees of freedom for
behavior within the specified boundaries of
effective performance, as revealed by means of
a WOP diagram.
Considerable further research, however, is
necessary to realize the ideals of this framework.
First, empirical studies to develop a deeper
appreciation of the nature of self-organization in
sociotechnical systems, especially those with
both human and sophisticated automated agents,
are needed. Second, techniques for analyzing
and modeling self-organizing sociotechnical
systems with both human and machine actors
require attention. Here, it is suggested that the
WOP diagram and, more broadly, the concepts
of cognitive work analysis provide a suitable
starting point, but further research in this area
would be worthwhile. Finally, design strategies
for promoting self-organization in sociotechni-
cal systems require further investigation. Such
investigations should consider means for enrich-
ing the interactions needed between human and
machine actors, which may not be of precisely
the same kinds that are natural among human
actors, if the respective capabilities of those
actors are to be exploited successfully.
This research on human­automation interac-
tion will contribute to the overarching design
goal of creating integrated system designs (Nai-
kar & Elix, 2016) that are compatible with how
complex cognitive work is carried out in socio-
technical systems. Such systems are necessarily
self-organizing, at least during complex, non-
routine operations, so that specifications of
"who should do what" are likely to lead to
uncompromising designs that do not facilitate
and, worse still, deliberately inhibit flexibility in
the workplace, forcing workers to improvise in
an ad hoc manner, which could lead to unsafe or
unproductive outcomes. Instead, actors must be
supported systematically in adapting both their
structures and behaviors, in an integrated manner,
if we are to recognize that self-organization occurs
in sociotechnical systems and that this phenome-
non is important for dealing with a dynamic and
ambiguous work environment. This design goal
requires a change in perspective in which the
emphasis is placed squarely on revealing the lim-
its on "who can do what" and promoting con-
strained flexibility in the workplace.
References
Bigley, G. A., & Roberts, K. H. (2001). The incident command
system: High-reliability organizing for complex and volatile
task environments. Academy of Management Journal, 44,
1281­1299.
Bogdanovic, J., Perry, J., Guggenheim, M., & Manser, T. (2015).
Adaptive coordination in surgical teams: An interview study.
BMC Health Services Research, 15, 128. doi:10.1186/s12913-
015-0792-5
Figure 2. Relationship of the diagram of work
organization possibilities to self-organization (cf.
Figure 1).
66 March 2018 - Journal of Cognitive Engineering and Decision Making
Fuchs, C. (2004). Knowledge management in self-organizing
social systems. Journal of Knowledge Management Practice,
5. Retrieved from http://www.tlainc.com/articl61.htm
Haken, H. (1988). Information and self-organization: A macroscopic
approach to complex systems. Berlin, Germany: Springer.
Herzog, S. (2011). Revisiting the Estonian cyber attacks: Digital
threats and multinational responses. Journal of Strategic Secu-
rity, 4(2), 49­60.
Heylighen, F. (2001). The science of self-organization and adaptiv-
ity. Encyclopedia of Life Support Systems, 5, 253­280.
Hofkirchner, W. (1998). Emergence and the logic of explanation.
An argument for the unity of science. Acta Polytechnica Scan-
dinavica, Mathematics, Computing and Management in Engi-
neering Series, 91, 23­30.
Hoppe, J., & Popham, P. (2007). Complete failure of spinal anaes-
thesia in obstetrics. International Journal of Obstetric Anes-
thesia, 16, 250­255.
Kaber, D. (2018). Issues in human­automation interaction model-
ing: Presumptive aspects of frameworks of types and levels of
automation. Journal of Cognitive Engineering and Decision
Making, 12, 7­24.
Leveson, N. G. (1995). Safeware: System safety and computers.
Reading, MA: Addison-Wesley.
Naikar, N. (2013). Work domain analysis: Concepts, guidelines,
and cases. Boca Raton, FL: CRC Press.
Naikar, N., & Brady, A. (in press). Cognitive systems engineer-
ing: Expertise in sociotechnical systems. In P. Ward, J. M.
Schraagen, J. Gore, & E. Roth (Eds.), The Oxford handbook
of expertise: Research and application. Oxford, UK: Oxford
University Press.
Naikar, N., & Elix, B. (2016). Integrated system design: Promoting
the capacity of sociotechnical systems for adaptation through
extensions of cognitive work analysis. Frontiers in Psychol-
ogy, 7, 962.
Naikar, N., Elix, B., Dâgge, C., & Caldwell, T. (2017). Designing
for self-organisation with the diagram of work organisation
possibilities. In J. Gore & P. Ward (Eds.), Proceedings of the
13th International Conference on Naturalistic Decision Mak-
ing (pp. 159­166). Bath, UK: University of Bath. Retrieved
from https://www.eventsforce.net/uob/media/uploaded/EVUOB/
event_2/GoreWard_NDM13Proceedings_2017.pdf
Park, Y., Hong, P., & Roh, J. J. (2013). Supply chain lessons from
the catastrophic natural disaster in Japan. Business Horizons,
56, 75­85.
Perrow, C. (1984). Normal accidents: Living with high risk tech-
nologies. New York, NY: Basic Books.
Rasmussen, J. (1968a). Characteristics of operator, automatic
equipment and designer in plant automation (Risø-M-808).
Roskilde, Denmark: Risø.
Rasmussen, J. (1968b). On the reliability of process plants and
instrumentation systems (Risø-M-706). Roskilde, Denmark:
Danish Atomic Energy Commission, Research Establishment
Risø.
Rasmussen, J. (1969). Man­machine communication in the light
of accident records (Report No. S-1-69). Roskilde, Denmark:
Danish Atomic Energy Commission, Research Establishment
Risø.
Rasmussen, J. (1986). Information processing and human­machine
interaction: An approach to cognitive engineering. New York,
NY: North-Holland.
Rasmussen, J., Pejtersen, A. M., & Goodstein, L. P. (1994). Cogni-
tive systems engineering. New York, NY: Wiley.
Reason, J. (1990). Human error. Cambridge, UK: Cambridge Uni-
versity Press.
Reich, P. C., Weinstein, S., Wild, C., & Cabanlong, A. S. (2010).
Cyber warfare: A review of theories, law, policies, actual
incidents--and the dilemma of anonymity. European Journal
of Law and Technology, 1(2), 1­58.
Rochlin, G. I., La Porte, T. R., & Roberts, K. H. (1987). The self-
designing high-reliability organization: aircraft carrier flight
operations at sea. Naval War College Review, 40(4), 76­90.
Vicente, K. J. (1999). Cognitive work analysis: Toward safe, pro-
ductive, and healthy computer-based work. Mahwah, NJ: Law-
rence Erlbaum.
Neelam Naikar leads the Centre for Cognitive Work
and Safety Analysis at the Defence Science and
Technology Group in Melbourne, Australia. Her
primary research interests are in methods for work
analysis and system design.
