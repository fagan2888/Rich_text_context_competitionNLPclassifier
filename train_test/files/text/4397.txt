Research and Politics
July-September 2016: 1
­9
© The Author(s) 2016
DOI: 10.1177/2053168016665856
rap.sagepub.com
Creative Commons Non Commercial CC-BY-NC: This article is distributed under the terms of the Creative Commons
Attribution-NonCommercial 3.0 License (http://www.creativecommons.org/licenses/by-nc/3.0/) which permits non-commercial use,
reproduction and distribution of the work without further permission provided the original work is attributed as specified on the SAGE and Open
Access pages (https://us.sagepub.com/en-us/nam/open-access-at-sage).
Introduction
Many academic papers (and especially the first few arti-
cles on a topic) describe relationships that turn out to be
illusory upon closer examination (Ioannidis, 2005).
Additionally, the typical published estimate is probably of
larger magnitude than the true relationship (Ioannidis,
2008). Recent large-scale attempts to replicate social sci-
entific findings have discovered that many of these find-
ings become substantially smaller and more uncertain than
initially indicated (Boekel et al., 2015; Camerer et al.,
2016; Hartshorne and Schachner, 2012; Ioannidis et al.,
2014; Klein et al., 2014; Maniadis et al., 2014; Open
Science Collaboration, 2015); the "replication crisis" has
plagued fields in the hard sciences as well (e.g. Begley and
Ellis, 2012; Prinz et al., 2011; Steward et al., 2012).
Replicability problems are exacerbated by researcher
behaviors like p-hacking (analyzing the same data in mul-
tiple ways but only reporting the most statistically signifi-
cant findings).1 But even if behaviors like this were
eliminated, the problems would continue to exist because
the publication process privileges statistically significant
results (Brodeur et al., 2016; Coursol and Wagner, 1986;
Gerber et al., 2001; Gerber and Malhotra, 2008a,b; Sterling
et al., 1995), including by influencing authors' decision to
write up and publicize their findings (Franco et al., 2014).
When null findings are not published, they cannot place
anomalously large and statistically significant results into
their proper context; such anomalous results can attract a
great deal of scientific interest because of their novelty and
counterintuitiveness.2 These problems are often collec-
tively referred to as publication bias (Scargle, 2000).
Although the publication bias created by "misunderstand-
ing or misuse of statistical inference is only one cause of
the `reproducibility crisis' ... to our community, it is an
important one" (Wasserstein and Lazar, 2016: 2).3
While much of the previous work in this area focuses on
establishing that publication bias is real and pervasive in
disciplines that use statistical evidence (e.g. by using "cali-
per tests" of published p-values, as in Gerber and Malhotra
(2008a) and Brodeur et al. (2016)), our paper seeks to
Measuring the effects of publication
bias in political science
Justin Esarey and Ahra Wu
Abstract
Prior research finds that statistically significant results are overrepresented in scientific publications. If significant results
are consistently favored in the review process, published results could systematically overstate the magnitude of their
findings even under ideal conditions. In this paper, we measure the impact of this publication bias on political science
using a new data set of published quantitative results. Although any measurement of publication bias depends on the
prior distribution of empirical relationships, we determine that published estimates in political science are on average
substantially larger than their true value under a variety of reasonable choices for this prior. We also find that many
published estimates have a false positive probability substantially greater than the conventional  = 0.05 threshold for
statistical significance if the prior probability of a null relationship exceeds 50%. Finally, although the proportion of
published false positives would be reduced if significance tests used a smaller , this change would not solve the problem
of upward bias in the magnitude of published results.
Keywords
Hypothesis testing, publication bias, statistical significance
Department of Political Science, Rice University, USA
Corresponding author:
Justin Esarey, Assistant Professor of Political Science, Rice University,
105 Herzstein Hall MS-24, 6100 Main Street Houston, TX 77005, USA.
Email: justin@justinesarey.com
665856
RAP0010.1177/2053168016665856Research & PoliticsEsarey and Wu
research-article2016
Research Article
2 Research and Politics 
determine how publication bias has affected the accumu-
lated body of knowledge in political science. We measure
the impact of publication bias on political science using a
new data set of published quantitative results. Although any
measurement of publication bias depends on the prior dis-
tribution of empirical relationships, we estimate that pub-
lished results in political science are distorted to a
substantively meaningful degree under a variety of reason-
able choices for this prior.
We come to three conclusions. First, published estimates
of relationships in political science are on average substan-
tially larger than their true value. The exact degree of
upward bias depends on the choice of prior, but at the high
end we estimate that the true value of published relation-
ships is on average 40% smaller than their published value.
More optimistic priors yield a lower average bias, but still
find that at least 14% of results are biased upward by 10%
or more. Second, we find that many published results have
a false positive probability substantially greater than the
conventional  = 0.05 threshold for statistical significance
if the prior probability of a null relationship exceeds 50%.
These two findings are quantitatively and qualitatively sim-
ilar to results uncovered by the large scale replication stud-
ies noted above, suggesting that publication bias can
explain much of the "replication crisis" these studies have
observed.4 Finally, we find that both the upward bias in
magnitude and the probability of being a false positive is
smaller for results with p-values further from the threshold
for significance. Our last finding suggests that requiring a
more stringent statistical significance test (with a smaller )
for publication might be effective at combating publication
bias (Johnson, 2013). Unfortunately, although the propor-
tion of published false positives would be reduced by this
strategy (Bayarri et al., 2016; Goodman, 2001), we find
that such a reform would not solve the problem of upward
bias in published results: published results near the new
threshold of significance would still be (on average) sub-
stantially biased upward.
Measurement strategy
Trying to measure the degree of upward bias in an estimate

 of some parameter , or the prevalence of false positives
(statistically significant estimates of 
 when the null
 = 0 is true), is tricky. Any measurement depends on an
assumption about the true value of  (or a probability distri-
bution of beliefs about its value, f ( )
 ). For example, con-
sider the distribution of statistically significant estimates

 associated with a true value of . Publication bias
implies that E t t
  
| , 0.05






 
 , where t = /
   is
the t-statistic, 

 
is the estimated standard error of 
, and
t
d
f
,0.05






 is the critical t value for a two-tailed significance
test under a null hypothesis  = 0 setting  = Pr(significant |
 = 0) = 0.05 with d f
many degrees of freedom. For a
fixed and known , we could calculate the degree of publi-
cation bias as
E t t
d
d
f
f
  
 
 

 


| ,
=
| , ,
,0.05






-
-








-
-




 


 
 

  








+
-


















d
d d
f
| , ,









- 

(1)
where   
=
,0.05
t
d
f







 
(that is, the smallest statistically
significant 
 ) and  is the t probability density function.
That is, we define bias as the difference between the
expected value of statistically significant estimates and the
true value of the estimand.5
However, in a published work,  is unknown. We must
therefore calculate
E E t t
E E t t
d
f
d
f
  
 
-






















| ,
= | ,
,0.05
,0.

0
05
,0.05
= | ,




















-

















 
E t t
d
f 



-








  
f d
( )



(2)
under some reasonable assumptions about our prior beliefs
about , f 
( ). This estimate of publication bias will obvi-
ously be a function of our choice of f 
( ), and conse-
quently it is advisable to estimate publication bias under a
variety of choices for f 
( ) to ensure robust results.
We estimate the degree of expected publication bias in the
political science literature as a proportion of the published
result, E E t t
d
f
    
-
( ) ( )






















sign pub
| , /
,0.05

 .
The sign 
( ) term means that we measure the degree of bias
in the direction of the true  (that is, as a function of the dis-
tance of the relationship from zero); this allows us to meas-
ure the degree to which the average published result
exaggerates the true magnitude of a relationship.6 The pub-
lished estimate 
pub
informs our assumption about the prior
f ( )
 to recognize that each project comes out of a different
family of projects pertaining to different subfields and topics
whose magnitudes are difficult to compare across families.
We consider two classes of f ( )
 :
Esarey and Wu 3
(a) aspike-and-slabdistributionwithaspikeat  = 0
and a uniform slab between -






3 ,3
 
 
pub pub
;
and
(b) a spike-and-normal distribution with a spike at
 = 0 added to a normal distribution with stan-
dard deviation equal to 
pub
.
The first distribution represents a 33% probability prior
belief that a non-zero  
pub
 , while the second repre-
sents a  68% probability prior belief that a non-zero

 
pub
; our results are robust to other reasonable
choices for the boundaries of the spike-and-slab prior and
the standard deviation of the normal prior.7 We systemati-
cally vary the height of the spike, Pr( = 0
 ), to determine
how different expectations for the baseline rate of null rela-
tionships changes our view of the published literature.
Finally, we repeat our analysis with no spike at  = 0, to
recognize the possibility that a point null hypothesis is
never true in real data (Gelman, 2011).
We use our prior belief density f ( )
 to determine the
relationship between true relationships and observed esti-
mates using simulation. To do this, we generate 100,000
draws 
 from f ( )
 for each published study. For each
draw of 
 , we simulate a sample estimate    

s
= + ,
 

d f
( ) where 

 
is the published standard error of

pub
and  is the t-density with d f
degrees of freedom
equivalent to the published study.8 We determine which of
these results is statistically significant by comparing
ts s
= /
  
  
to the t
d
f
,0.05






 two-tailed  = 0.05 critical
value from a t-density with the appropriate degrees of free-
dom. Finally, we calculate   
s
-
( ) ( )






sign for each of the
statistically significant draws. The average of this quantity is
our estimate of E E t t
d
f
   
-
( ) ( )






















sign | ,
,0.05
 .

We then divide this by the absolute value of the published
result, 
pub
, to calculate percentage bias.
Data set
We estimate the effect of publication bias on the literature in
political science using a new data set of quantitative work
recently published in prominent, general interest journals.
Our data set is composed of 314 quantitative articles pub-
lished in the American Political Science Review (APSR: 139
articles in volumes 102­107, from 2008­2013) and the
American Journal of Political Science (AJPS: 175 articles in
volumes 54­57, from 2010­2013).9 To simplify the analy-
sis, we analyze only articles with continuous and unbounded
dependent variables. Among the 173 articles with continu-
ous and unbounded dependent variables,10 6 articles have at
least one missing value regarding their estimates or sample
sizes, which are necessary for our analysis. Therefore, we
remove these 6 articles from our analysis. Consequently, we
are left with 167 quantitative articles published in the APSR
(70 articles) and the AJPS (97 articles). Finally, 25 studies
out of these 167 quantitative articles (or 15% of that num-
ber) report statistically insignificant results as their main
relationship under a two-tailed test with  = 0.05, although
17 of the 25 studies are statistically significant if using a
one-tailed test with  = 0.05.11 We omit these studies from
our analysis as their interpretation is unclear in the context
of assessing publication bias when using an  = 0.05 two-
tailed significance test, leaving 142 studies for analysis. The
consequence of omitting statistically insignificant results is
that our estimates are upper bounds on the degree of publi-
cation bias in the literature: the more likely it is that statisti-
cally insignificant results will be published, the smaller that
publication bias will be.
A complete list of the rules we used to identify and code
observations in our data set is provided in Appendix 2; we
summarize the procedure here.12 Each observation of the col-
lected data set represents one article and contains the article's
main finding (viz., an estimated marginal effect, 
pub
).
Defining the main finding of an article can be complicated,
as many articles present multiple results.13 We code the main
finding in the following way. First, if there is any expression
such as "the key independent variable" or "the main finding
of this paper," we consider that relationship the main finding.
If there is no such explicit phrasing, we consider the finding
that is emphasized in the abstract or in the conclusion of a
paper as the main finding. If there are multiple hypotheses
that receive almost equal attention, we record the informa-
tion of the first hypothesis ("H1" or "the first hypothesis").
Results
The result of applying this technique to the published (and
statistically significant) marginal effects estimates in our
data set reveals a substantial tendency toward upward bias
in magnitude, as illustrated in Table 1. As the table shows,
if we have a baseline expectation that only 10% of our
hypotheses correctly predict a relationship a priori, then
over 50% of published findings are expected to be at least
10% larger in magnitude than the true relationship. The
typical published result in this scenario is on average at
least 29% larger than the true relationship. Even if there are
no relationships that are exactly zero under a normal prior
density (with standard deviation equal to 
pub
), over 40%
of published results have 10% upward bias in magnitude.
In general, the magnitude of the bias problem scales posi-
tively with the assumed underlying proportion of null
results in the population of research ideas ( Pr  = 0
( ) ).
The implication of the analysis is that a substantial por-
tion of published results overestimate the true size of the
relationship being studied because statistical significance
tests are used to screen results for publication. Biases that
are large enough to be substantively meaningful are not
4 Research and Politics 
uncommon; if our assumptions about f ( )
 are a good
representation of the background rate of null relation-
ships, we would expect many empirical findings (perhaps
even a majority) to exaggerate the size of the true relation-
ships that they measure. Moreover, the high end of our
estimates (viz., that the true value of published relation-
ships is on average 40% smaller than their published
value) matches recent empirical estimates by large-scale
replication projects. For example, the Open Science
Collaboration (OSC) estimated that originally published
effect sizes in psychology are on average around 50%
smaller than effect sizes in replication studies of the same
relationship (Open Science Collaboration, 2015: aac4716-
3­aac-4716-5). A similar replication of 18 studies in
experimental economics found that replication effect sizes
were on average only 65.9% of the size of the original
estimate, a reduction of about 44% (Camerer et al., 2016:
1434).14 Most of our estimates, however, show smaller
mean bias; this suggests that either (a) the proportion of
null results in the population of studies is higher than we
contemplated, or (b) other factors (opportunistic model
selection by the original researchers, a bias against suc-
cess among the researchers performing the replication,
and/or many alternative possibilities) may be working in
concert with publication bias to explain prior empirical
results.
Not all publications are equally susceptible to bias, as
seen in Figure 1. The figure shows that individual results
vary greatly in terms of expected publication bias, regard-
less of the prior probability of a null effect Pr  = 0
( ).
Indeed, Figure 1(d) shows substantial bias, and substantial
Table 1. Expected bias in a sample of published marginal effects from APSR and AJPS.
Assumed population Pr( 0)
  Spike-and-slab prior Spike-and-normal prior
Mean % bias % of estimates with
bias  10%
Mean % bias % of estimates with
bias  10%
10% 29.8 87.3 40.5 52.8
20% 18.9 78.2 29.2 47.9
50% 8.90 41.5 17.8 44.3
100% 4.73 14.1 12.4 42.3
The table shows the estimated prevalence of upward bias in estimate magnitude in a sample of 167 articles from the American Political Science Review
and the American Journal of Political Science; the sample size is 142 after 25 statistically insignificant results are excluded. We generate 100,000 draws

 from   

U -






3 ,3
^ ^
pub pub
or  

 0, pub
( ) with probability (1 )
- p and  = 0 with probability p for each published study; the assumed
value of p is listed in column 1. For each draw of 
, we simulate a sample estimate    

s
= + ,  

d
f
( ), where 

 
is the published standard
error of 
pub
and  is the t-density with d
f
degrees of freedom equivalent to the published study. We determine which of these estimates is
statistically significant by comparing t
s s
= /
  
  
to the t
d
f
,0.05
( )
 critical value for an  = 0.05 test (two-tailed) from a t-density with degrees of
freedom equivalent to the published study. Finally, we calculate    
-
( ) ( )






sign
pub
/ for each of the statistically significant draws. Columns 2
and 4 list the mean value of these replicates, our estimate of E E t t
d
f
    
-
( ) ( )
















( )
sign
pub
| , /
,0.05

 , across all 142 results for the prior
distribution indicated in the column heading. Columns 3 and 5 lists the corresponding proportion of estimates that are greater than or equal to 10%.
variation among published results, for the normal prior even
with no spike at  = 0 . Importantly, the expected bias is
strongly associated with the published p-value of the result:
smaller p-values are associated with smaller expected bias.
Our finding underscores a point made in the American
Statistical Association's statement on p-values: "the wide-
spread use of `statistical significance' (generally interpreted
as `p 0.05
 ') as a license for making a claim of a scientific
finding (or implied truth) leads to considerable distortion of
the scientific process" (Wasserstein and Lazar, 2016: 9).
Calculating susceptibility to false positives
Statistical significance testing is designed to lower the risk of
concluding that a relationship exists when the evidence could
be consistent with no relationship at all. However, it is well
established (though perhaps not widely understood) that sta-
tistical significance testing is often insufficient to reduce the
chance of a false positive to an acceptable level when the
prior probability of studying a null relationship is very high
(Bayarri et al., 2016; Goodman, 2001; Nuzzo, 2014;
Siegfried, 2010). A key factor is the prior probability that the
null hypothesis is true (i.e., the a priori expectation that the
relationship being studied does not actually exist). That is:
Pr |
Pr( | )Pr( )

 
=
( ) =
= =
0
0 0
statistically significant
stat. sig
D
D
D = = = +
 - =
Pr( | )Pr( )
Pr( | )( Pr( ))
stat. sig
stat. sig
 
 
0 0
0 1 0
(3)
Esarey and Wu 5
We can use this formula to calculate this probability for the
observations in our data set; this is similar to a calculation
that Goodman (2001) and Bayarri et al. (2016) performed
using Bayes'factors and to a closely related formula offered
by Maniadis et al. (2014). To establish a lower bound for
Pr  = 0 |stat.sig.
( ), we set Pr stat. sig.| 0 = 1
 
( ) to
maximize the denominator of equation (3). We then set the
prior probability Pr  = 0
( ) to a fixed value and calculated
Pr  = 0 |stat.sig.
( ) for a range of Pr (stat.sig.|  = 0) 
[0,0.05]. The results for four different values of Pr  = 0
( )
are shown in Figure 2; the histogram in this figure indi-
cates the distribution of p-values (i.e. the value of
Pr stat. sig.| = 0

( )) in our data set.
As the figure shows, not all published work has an equal
expected probability of being a false positive. Results that
are close to the boundary of statistical significance (with
p  0.05) have the greatest expected probability of being a
false positive. Results that are further from this boundary
(e.g. where p 0.01
 ) are at substantially lower risk of
being false positives. This finding is consistent with the
prior work of the Open Science Collaboration (2015:
aac4716-5), whose replications of notable findings in psy-
chology discovered that "a negative correlation of replica-
tion success with the original study p value indicates that
the initial strength of evidence is predictive of reproducibil-
ity." Camerer et al. (2016: 1435) find the same relationship
Figure 1. Histogram of expected bias calculations from APSR and AJPS. (a) Spike-and-slab prior, Pr( 0) =10%
  . (b) Spike-and-
normal prior, Pr( 0) =10%
  . (c) Uniform prior, Pr( 0) =100%
  . (d) Normal prior, Pr( 0) =100%
  . Each histogram shows
the proportion of articles in a sample of 167 articles from the American Political Science Review and the American Journal of Political
Science corresponding to a degree of expected bias; the sample size is 142 after 25 statistically insignificant results are excluded.
Expected bias E E t t
d
f
    
-
( ) ( )






















sign
pub
| , /
,0.05

 is calculated using the prior density indicated in the sub-figure's
caption and the procedure described in Table 1. The color of the bar indicates a published result p -value in the range listed by the
sub-figure's legend.
6 Research and Politics 
between p-values and replicability. The finding is consist-
ent with the calculations of Goodman (2001) and Bayarri
et al. (2016), who shows that lower p-values are associated
with greater reductions in the posterior probability of the
null hypothesis (relative to its prior probability).
Figure 2 indicates that our concern about the likelihood
of a false positive should be geometrically related to our
prior belief about Pr  = 0
( ) and almost linearly related to
a result's p-value. When Pr  = 0 0.5
( ) , the probability of a
false positive never exceeds 5% in our calculation. However,
if Pr  = 0 = 0.75
( ) , we calculate that 10.6% of the pub-
lished results in our data have Pr .
 = 0 | 10%
stat. sig.
( )
When Pr  = 0 = 0.9
( ) , 25.4% of the published results in
our data set have Pr  = 0 | 10%
stat. sig.
( ) . Over 40% of
these results have have Pr  = 0 | 10%
stat. sig.
( ) if
Pr  = 0 = 0.95
( ) . Our finding may explain why so many
results fail to replicate. For example, the Open Science
Collaboration was able to successfully replicate only 39 of
100 relationships from the psychology literature that it
tested in its study (Open Science Collaboration, 2015: aac-
4716-5). A survey of researchers in psychology and allied
fields by Hartshorne and Schachner (2012: 3) found that
only 49% of attempted replications were able to fully repli-
cate a study's original findings. In economics, Camerer
et al. (2016) were able to successfully replicate only 11 of
the 18 studies they examined, a 61% success rate. Even in
medicine, a recent study by Prinz et al. (2011) found that
their laboratory was only able to completely replicate
between 20% and 25% of the published work examined.
Conclusions and implications
The problem of publication bias has been studied for years
and permeates all scientific disciplines that use statistical
evidence (Rosenthal, 1979; Sterling et al., 1995). Interest in
the problem has been reignited by effort to replicate results
in multiple disciplines that have met with a surprisingly
high rate of failure (Boekel et al., 2015; Camerer et al.,
2016; Hartshorne and Schachner, 2012; Ioannidis et al.,
2014; Klein et al., 2014; Maniadis et al., 2014; Open
Science Collaboration, 2015). Prior work has established
that statistically significant results are favored in political
science (e.g. Gerber and Malhotra, 2008a), but to what
extent does it distort substantive knowledge in the
Figure 2. Expected lower bound false positive probability calculations. The figure shows the relationship between the probability
that the null hypothesis is true given a statistically significant result Pr null|st. sig.
( )
( ) as a function of the probability of obtaining
a statistically significant result when the null hypothesis is true Pr st. sig.|null
( )
( ) that is implied by equation (3). To establish a
lower bound for Pr null|st. sig.
( ), we set Pr stat. sig.| 0 =1
 
( ) in equation (3). We set Pr null
( ) to several alternative values,
as indicated in the figure's legend. The histogram shows the proportion of p -values in bins of width 0.005 for 142 published and
statistically significant results in our data set.
Esarey and Wu 7
discipline? Are our findings contaminated by results that
are biased upward in magnitude? Are false positive find-
ings published too often in that literature? The answers
depend on the unknown prior distribution of true relation-
ships. But we find evidence for both problems in the pub-
lished political science literature, and the problems are
large enough to be qualitatively meaningful under a wide
variety of different prior distributions. If these problems
exist, they occur because statistically significant results are
favored in the publication process: smaller values in an
estimate's sampling distribution are disproportionately
ignored and null relationships are still likely to be published
(Brodeur et al., 2016; Coursol and Wagner, 1986; Gerber
et al., 2001; Gerber and Malhotra, 2008a,b; Sterling et al.,
1995). We believe that our paper complements the findings
of large-scale replication projects by placing them into a
clearer theoretical context: under reasonable assumptions
for the prior distribution of effects f ( )
 , the results of
these studies are what we should expect given (a) the exist-
ence of a publication process that favors statistically sig-
nificant results and (b) the distribution of published results
in the literature. In short, our findings suggest that publica-
tion bias is a reasonable explanation for at least part of the
"replication crisis."
Based on our evidence, results with smaller p-values are
less affected by publication bias because they are further
from the  = 0.05 threshold. These results are also at lesser
risk of being a false positive (Bayarri et al., 2016; Goodman,
2001). However, using a decreased threshold for statistical
significance (i.e. only publishing results that can pass a sig-
nificance test with  lower than 0.05), as suggested by
Johnson (2013), simply recreates the problem for results
near the new threshold. Consider the simulations of Table 1
for the spike-and-slab prior when Pr  = 0 = 90%
( ) : using
a significance threshold of  = 0.01 results in 66.9% of esti-
mates exceeding 10% magnitude in bias (compared to
87.3% of estimates using the  = 0.05 threshold). When
Pr  = 0 = 0%
( ) under the same prior, using a significance
threshold of  = 0.01 results in 20.4% of estimates exceed-
ing 10% magnitude in bias (compared to 14.1% of esti-
mates using the  = 0.05 threshold).
The empirical "credibility revolution" in economics
and political science has rightfully made us ask harder
questions of the quality of our research designs on a
paper-to-paper basis (Angrist and Pischke, 2010). But as
long as statistically significant results are privileged in the
publication process, even researchers who do everything
right from a causal identification perspective could still
produce a literature with results that are (on average) biased
upward and overpopulated with false positives. Just as
the credibility revolution has made us more skeptical of
some research designs, we believe that our findings (and
the larger universe of findings concerning replicability)
demand increased skepticism of novel results. This is par-
ticularly true if the result is only marginally statistically
significant, because marginally significant results are at
increased risk of being false positives. Consequently, it
may be prudent to place less importance on the novelty
and originality of a scholar's output in evaluating his or
her contribution to the discipline-- recognizing, of course,
that these are still important and valuable qualities!-- and
more importance on work that checks the robustness of
existing findings, including replication studies. We should
also be careful about allowing the initial discovery of a
new phenomenon to shape our research agenda before the
phenomenon is thoroughly replicated. In the event that the
discovery is a false positive, researchers seeking to apply
the findings to other areas will necessarily be building
their work on a null finding, thereby raising the overall
prior probability of null hypotheses (viz. Pr( = 0)
 ) in the
population and making the overall problem of publication
bias even worse. We think that these changes constitute a
substantial revision to the status quo, but one that is
important to safeguarding the reliability of the findings
that we communicate to each other, to our students, and to
the larger world.
Acknowledgements
We thank Ashley Leeds, Will H. Moore, Cliff Morgan, Ric Stoll,
our anonymous reviewers, and participants in our sessions at the
2013 Annual Meeting of the American Political Science
Association and the 2013 Annual Meeting of the Society for
Political Methodology for their helpful comments and
suggestions.
Funding
This research received no specific grant from any funding agency
in the public, commercial, or not-for-profit sectors.
Supplementary material
The replication files are available at: https://dataverse.harvard.
edu/dataverse/researchandpolitics. The supplementary files are
available at: http://rap.sagepub.com/content/3/3
Notes
1. The term `p-hacking' is taken from Simonsohn et al.
(2014).
2. This phenomenon is often referred to as "the file drawer
problem" (Rosenthal, 1979): all of the null results that could
contextualize new findings remain unpublished (in the
file drawers of scientists). A recent study by Franco et al.
(2014) highlights the acuteness of this problem in the social
sciences.
 3. Wasserstein borrows Peng's (2015) phrasing here.
4. An alternative explanation for widespread replication fail-
ures, that replicators' protocols are insufficiently faithful to
the original studies and the methodology is biased against
finding successful replications, is advanced by Gilbert et al.
(2016) in specific response to the large scale replication
of results in psychology undertaken by the Open Science
8 Research and Politics 
Collaboration (2015). This alternative explanation has been
itself criticized by the authors of the OSC study (Anderson
et al., 2016; Lakens, 2016) as well as Srivastava (2016).
 5. Note that E t t
d
f
  
| ,
,0.05














-

 is different than

  
-














E t t
d
f
| ,
,0.05
 ; bias in one does not necessarily
imply bias in the other. We study 
E t t
d
f
  
| ,
,0.05














-

because our understanding of the meaning of "publication
bias" is persistent divergence between the mean of published
(statistically significant) estimates 
 and the true value of .We
thus fix the true value of  and examine the distribution of asso-
ciated estimates 
 . We explore this issue further in Appendix
1, including estimating   
pub pub
-














E t t
d
f
| ,
,0.05

 for
published findings 
pub
.
 6. We set sign  = 0 =1
( ) .
 7. For example, if we change the uniform slab to range between
-






10 ,10
 
 
pub pub
with a total Pr( 0)
  of 10%, we
estimate a mean bias of 24.4% in our sample of published
results. We also estimate that 50.7% of published results in
our data set have a false positive probability above 10%. These
magnitudes are lower than, but broadly comparable to those we
show in Table 1 under the assumption that the slab in f ( )

ranges between -






3 ,3
 
 
pub pub
. When the normal prior is
set to a standard deviation of 10 
pub
and Pr( 0) =10%
  ,
we estimate a mean bias of 24.2% and that 50.7% of results
have a false positive probability greater than 10%.
 8. The degrees of freedom is set at n k
- , where k is the number of
estimated coefficients, whenever possible. If k was not reported
in the study, we set the degrees of freedom at n - 2, the smallest
plausible value (one slope coefficient plus the intercept).
 9. We collect information of articles from the AJPS for shorter
time period than that of the APSR because, on average, there
are more quantitative articles published in a single issue of
the AJPS than in that of the APSR. We want to balance the
number of articles collected from both journals, while col-
lecting information of recently published articles. Focusing
on the articles published in the APSR from 2008 to 2013
also allows us to contribute to the literature by introducing
a new data set, given that the time period of the Gerber and
Malhotra (2008a) data set is from 1995 to 2007.
10. A total of 138 articles in our data set have discrete depend-
ent variables and three articles have continuous but bounded
dependent variables.
11. A total of 33 studies (out of 314) in our data set specify a
one-tailed test. This information is available in the "tails"
variable in our replication dataset.
12. The complete set of coding sheets we used for all the articles
we examined are available in an Appendix 2.
13. We chose to focus on one main finding per article so that
our results would equally weight each paper's contribution
to publication bias in the literature. Thus, to be precise, we
estimate the publication bias of primary findings of papers
in the discipline rather than for all published findings.
Theoretically, we would expect within-article clustering of
publication bias for multiple results presented in the same
article for a variety of reasons: the results come from the
same data set or model, are written by the same authors using
the same research practices, and so on. Presenting one find-
ing per article allows us to neglect this complication.
14. Interestingly, the Klein et al. (2014) replication study of
16 relationships in psychology finds an equal number of
studies whose median replication effect size is larger and
smaller than the original estimate (see Table 2); this may be
because the authors deliberately chose some findings known
to be robustly replicable in addition to newer findings with
unknown replicability.
Carnegie Corporation of New York Grant
The open access article processing charge (APC) for this article
was waived due to a grant awarded to Research & Politics from
Carnegie Corporation of New York under its "Bridging the Gap"
initiative. The statements made and views expressed are solely the
responsibility of the authors.
References
Anderson CJ, Bahnik S, Barnett-Cowan M, et al. (2016) Response
to "Comment on estimating the reproducibility of psycho-
logical science". Science 351(6277): 1037­1037.
Angrist JD and Pischke JS (2010) The credibility revolution in
empirical economics: How better research design is tak-
ing the con out of econometrics. The Journal of Economic
Perspectives 24(2): 3­30.
Bayarri M, Benjamin DJ, Berger JO, et al. (2016) Rejection odds
and rejection ratios: A proposal for statistical practice in test-
ing hypotheses. Journal of Mathematical Psychology
Begley C and Ellis LM (2012) Raise standards for preclinical can-
cer research. Nature 483: 531­533.
Boekel W, Wagenmakers EJ, Belay L, et al. (2015) A purely con-
firmatory replication study of structural brain-behavior cor-
relations. Cortex 66: 115­133.
Brodeur A, Le M, Sangnier M, et al. strike back. American
Economic Journal: Applied Economics 8(1): 1­32.
Camerer CF, Dreber A, Forsell E, et al. (2016) Evaluating rep-
licability of laboratory experiments in economics. Science
351(6280): 1433­1436.
Coursol A and Wagner EE (1986) Effect of positive findings on
submission and acceptance rates: A note on meta-analysis
bias. Professional Psychology: Research and Practice 17:
136­137.
Franco A, Malhotra N and Simonovits G (2014) Publication bias
in the social sciences: Unlocking the file drawer. Science
345(6203): 1502­1505.
Gelman A (2011) So-called Bayesian hypothesis testing is just
as bad as regular hypothesis testing. Available at: http://
andrewgelman.com/2011/04/02/so-called_bayes/ (accessed
30 August 2016).
Gerber A and Malhotra N (2008a) Do statistical reporting stand-
ards affect what is published? Publication bias in two lead-
ing political science journals. Quarterly Journal of Political
Science 3(3): 313­326.
Esarey and Wu 9
Gerber AS and Malhotra N (2008b) Sociological methods & pub-
lication bias in empirical sociological research: Do arbitrary
significance levels distort published results? Sociological
Methods & Research 37(3): 3­30.
Gerber AS, Green DP and Nickerson D (2001) Testing for
publication bias in political science. Political Analysis:
385­392.
Gilbert DT, Gary K, Pettigrew S, et al. (2016) Comment on
`Estimating the reproducibility of psychological science'.
Science 351(6277): 1037.
Goodman SN (2001) Of p-values and Bayes: A modest proposal.
Epidemiology 12(3): 295­297.
Hartshorne JK and Schachner A (2012) Tracking replicability as
a method of post-publication open evaluation. Frontiers in
Computational Neuroscience 6(8): 1­14.
Ioannidis JP, Munafo MR, Fusar-Poli P, et al. (2014) Publication
and other reporting biases in cognitive sciences: Detection,
prevalence, and prevention. Trends in Cognitive Sciences
18(5): 235­241.
Ioannidis JPA (2005) Why most published research findings are
false. PLoS Medicine 2: 696­701.
Ioannidis JPA (2008) Why most discovered true associations are
inflated. Epidemiology 19: 640­648.
Johnson VE (2013) Revised standards for statistical evidence.
Proceedings of the National Academy of Sciences 110(48):
19313­19317.
Klein RA, Ratliff KA, Vianello M, et al. (2014) Investi-
gating variation in replicability. Social Psychology 45(3):
142­152.
Lakens D (2016) The statistical conclusions in Gilbert (2016) are
completely invalid. Available at: http://daniellakens.blogs-
pot.com/2016/03/the-statistical-conclusions-in-gilbert.html
(accessed 30 August 2016).
Maniadis Z, Tufano F and List JA (2014) One swallow doesn't
make a summer: New evidence on anchoring effects.
American Economic Review 104(1): 277­290.
Nuzzo R (2014) Statistical errors. Nature 506: 150­152.
Open Science Collaboration (2015) Estimating the reproducibility
of psychological science. Science 349(6251): 943.
Peng R (2015) The reproducibility crisis in science: A statistical
counterattack. Significance 12(3): 30­32.
Prinz F, Schlange T and Khusru A (2011) Believe it or not: How
much can we rely on published data on potential drug tar-
gets? Nature Reviews Drug Discovery 10: 712.
Rosenthal R (1979) The file drawer problem and tolerance for null
results. Psychological Bulletin 86: 638­641.
Scargle JD (2000) Publication bias: The "file-drawer" problem in sci-
entific inference. Journal of Scientific Exploration 14: 91­106.
Siegfried T (2010) Odds are, it's wrong. Science News 177(7): 26.
Simonsohn U, Nelson LD and Simmons JP (2014) P-curve: A
key to the file drawer. Journal of Experimental Psychology:
General 143(2): 534­547.
Srivastava S (2016) Evaluating a new critique of the reproduc-
ibility project. Available at: https://hardsci.wordpress.
com/2016/03/03/evaluating-a-new-critique-of-the-repro-
ducibility-project/ (accessed 30 August 2016).
Sterling T, Rosenbaum WL and Winkam JJ (1995) Publication
decisions revisited: The effect of the outcome of statistical
tests on the decision to publish and vice versa. The American
Statistician 49: 108­112.
Steward O, Popovich PG, Dietrich W, et al. (2012) Replication and
reproducibility in spinal cord injury research. Experimental
Neurology 233: 597­605.
Wasserstein RL and Lazar NA (2016) The ASA's statement on
p-values: Context, process, and purpose. The American
Statistician.
