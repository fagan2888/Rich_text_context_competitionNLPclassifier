SAGE Open
October-December 2014: 1
­13
© The Author(s) 2014
DOI: 10.1177/2158244014559019
sgo.sagepub.com
Creative Commons CC BY: This article is distributed under the terms of the Creative Commons Attribution 3.0 License
(http://www.creativecommons.org/licenses/by/3.0/) which permits any use, reproduction and distribution of the work without further
permission provided the original work is attributed as specified on the SAGE and Open Access page (http://www.uk.sagepub.com/aboutus/openaccess.htm).
Article
Online education continues to grow. Over the past decade,
increasing numbers of students in the United States have
enrolled in online college courses. In 2011, approximately
6,700,000, or 32%, of all higher education students in the
United States had enrolled in one or more online courses
(Allen & Seaman, 2013). The literature suggests that online
education will continue to expand throughout the upcoming
decade, although not without concerns. Even though higher
education administrators and faculty seemingly have
become more accepting of online courses as an extension to
the physical campus (Bowen, 2013), chief academic offi-
cers of universities perceive that the majority of university
faculty members remain skeptical of teaching online (Allen
& Seaman, 2013). Many faculty have concerns about the
integrity and quality of online courses (Jaggars & Bailey,
2010), although the literature suggests that learning out-
comes in hybrid courses are the same and in some respects
superior to learning outcomes in face-to-face courses
(Means, Toyama, Murphy, Bakia, & Jones, 2010). More
research of online discussions across the various disciplines
would be helpful those interested in applying asynchronous
discussions in their courses. In this research, an instructor
of a hybrid course applied an empirical method to assess
evidence of learning in the course's online discussions.
Faculty who want to substantiate the learning in their
courses' asynchronous discussions may want to consider
applying similar methods described in this study, and con-
sider publishing the results.
Research on the use of asynchronous discussions used in
courses for learning is relatively new, considering that the
Internet has been available in schools for only a few decades.
In 2003, approximately 93% of the public school classrooms
in the United States had Internet access, which is in stark
contrast with less than 5% of classrooms having Internet
access in 1993 (Parsad, Jones, & Greene, 2005). Over the
past decade, there has been an increase in the use of asyn-
chronous discussions as an instructional strategy in face-
to-face courses as well as in hybrid and online courses. More
recently, strategies such as "flipping a classroom" have
increasingly involved the use of online discussions for learn-
ing in face-to-face courses to extend instruction outside the
physical classroom in preparation for learning inside the
classroom (Bergmann & Sams, 2012).
In this study, content analysis was applied to examine the
discussion transcripts for learning with respect to a concep-
tual framework developed from a 4-year study of computer
conferencing (Garrison, Anderson, & Archer, 2001). The
goal was to gain a broader, deeper understanding of the lev-
els of learning in the discussions. An assumption of this
study is that online discussions are essential elements of
559019
SGOXXX10.1177/2158244014559019SAGE OpenRodriguez
research-article2014
1California State University, Sacramento, USA
Corresponding Author:
Mark A. Rodriguez, California State University, Sacramento, 6000 J Street,
Sacramento, CA 95819, USA.
Email: rodrigue@csus.edu
Content Analysis as a Method to Assess
Online Discussions for Learning
Mark A. Rodriguez1
Abstract
One of the challenges for instructors in online education is to create learning opportunities through online text-based
discourse. The goal of this study was to examine the use of content analysis to better understand graduate students'
learning in online discussions. The discussion transcripts of a hybrid graduate course were analyzed to determine levels
and frequencies of learning or cognitive presence. The analysis of the online discussions was guided by social constructivist
rationale, and includes descriptive statistics and inter-rater reliability measurements. Findings show that cognitive presence
levels were concentrated in categories marked for exploration and integration. Non-cognitive messages resulted in the
highest frequencies of discussion messages, demonstrating indications of social presence and teaching presence. Training
was found to be a key factor in resolving coding inconsistencies to improve the reliability of the content analysis. The
processes of content analysis applied in this study to evaluate learning in online discussions provided useful information for
the development and study of online discussions for learning.
Keywords
cognitive presence, asynchronous discussions, content analysis
2 SAGE Open
online learning. Throughout this article, the terms online dis-
cussions or asynchronous discussion are used interchange-
ably and are intended to refer to time-delayed discussions,
not synchronous real-time discussions.
Literature Review
Learning theories provide a basis for understanding how
learning occurs in online discussions. Principles of learning
theories are used to explain or support teaching in meeting
the learning needs of students (Borich & Tombari, 1997;
Dillenbourg, 1999; Hofer, 2001). Particularly relevant to
online learning are the principles of constructivist learning
theories, which include collaboration, interaction, and the
use of language (Bruner, 1986; John-Steiner & Mahn, 1996;
Vygotsky, 1980). In turn, these constructivist principles fun-
damentally characterize how learning occurs in asynchro-
nous discussions (Lebow, 1993; McLoughlin & Oliver,
1998; Swan, 2005; Yuan & Kim, 2014) and serve as the basis
underlying the approach to this study in examining the dis-
cussion transcripts of graduate students for evidence of
learning.
Choosing discussion strategies is an important step in the
planning of online learning. A review of literature identifies
factors to consider in planning for online discussions, which
may help planning for the discussions.
Online Discussion Strategies
Asynchronous discussions are typically an important part of
learning in online or hybrid courses. Discussions offer a way
for students to learn and articulate their understanding of
learning through interactions with each other and the instruc-
tor (Parker & Hess, 2001). Darabi, Liang, Suryavanshi, and
Yurekli (2013), in their meta-analysis of 80 studies of online
discussions, found that "learners responded better to strate-
gic and productive discussion than when they were asked to
elaborate on a topic" (p. 239). Although the purpose of this
study does not involve an in-depth analysis of the various
online discussion strategies, it does give an overview of
strategies and factors that are relevant in planning. Further
studies that describe the processes of choosing and applying
text-based discussion strategies would be helpful.
The Socratic method of discussion has traditionally been
recognized as an effective way to help students learn
beyond memorizing and regurgitating facts (Hansen, 1988).
Socratic discussion methods employ thought-provoking
questions which are evident in online discussion strategies
such as in online debates and case studies (Yang, Newby, &
Bill, 2005). Literature circles are another Socratic method
of discussion that has been used effectively to support
learning in both online and face-to-face discussions.
Literature circles begin with a discussion of a text, and are
guided by open-ended questions that are designed to pro-
mote deeper thinking and discussion of the text. Having
literature circles online provides for added reflection oppor-
tunities for students in reading the transcripts of the discus-
sions, which supports broader and deeper perspectives and
understanding of the discussion topic (Larson, 2009). The
think-pair-share discussion method has been structured
effectively for use in online discussions (Johnson &Aragon,
2003). In this strategy, students first think independently
about a problem or a question, are then paired or grouped to
discuss their thoughts, ending with the whole class sharing
and discussing the topic. The think-pair-share discussion
strategy builds a strong foundation for communicating
ideas with a range of fellow students and the instructor
(King, 1993).
In a study by R. S. Anderson, Goode, Mitchell, and
Thompson (2013), they examined a group of doctoral stu-
dents' perceptions of four different online constructivist dis-
cussion strategies, including problem-based learning (PBL),
discussion web strategy, 3-2-1 strategy, and case study. Each
of the strategies was said to reflect social constructivist learn-
ing because of the interaction involved in the discussions and
the development of personal meanings associated with the
discussions. In the PBL discussion strategy, the students dis-
cussed self-directed problem-solving objectives. In the dis-
cussion web strategy, the students identified and described
the pros and cons of a particular topic, justifying their posi-
tion. In the 3-2-1 online discussion strategy, the students read
a text, selected three key points from the text, provided two
supportive explanations, and then posed one question related
to the reading. In the case study strategy, the students ana-
lyzed real-life situations, read case information, made reflec-
tive connections, stated opinions, and asked and responded
to questions. Students'perceptions of these discussion strate-
gies were collected through surveys and interviews, and
were analyzed, coded, and categorized by four coders based
on a process of inductive reasoning. The results showed that
the students believed all of the discussion strategies to be
effective for particular types of discussions.
Factors in Planning Online Discussions
Consideration of the practical aspects or factors of online
discussions can be helpful in planning for the use of them in
online learning. Among the factors to consider are time, stu-
dent attitudes, and technology.
Asynchronous discussions require reading, writing, and
comprehension tasks that arise from students needing to
develop, post, and read text-based communication. This can
result in more time required of the students to complete
online discussions compared with the amount of time typi-
cally spent in face-to-face class discussions (Meyer, 2003).
Instructors would be wise to consider the coordination needs
in scheduling the time for students to read and post to the
discussions. Reading, writing, and the increased amount of
time that students spend in text-based discussions can pro-
vide for more focused and deeper learning through the
Rodriguez 3
increased opportunities to read, reflect, and respond
(Benbunan-Fich & Hiltz, 1999; Bliuc, Ellis, Goodyear, &
Piggott, 2011; Maor, 2003).
Asynchronous text-based discussions may have a positive
impact on student attitudes toward discussions by lessening
any difficulties arising from students who are reluctant to
participate, feel confronted in the discussions, or are not
given enough time to voice their opinions. The atmosphere in
asynchronous discussions is seen by some students to be
easier to handle, less stressful, equitable, and less likely to be
dominated by a few individuals (Wang & Woo, 2007).
However, there are disadvantages of text-based discussions
that students may perceive negatively. Text by itself does not
communicate the nuances of tone of voice nor does it replace
visual cues of face-to-face discussions. The use of emoticons
in online discussions can suggest tone of voice that may clar-
ify the meaning behind the text (Tiene, 2000). More studies
are needed to understand the effect and practice of using
emoticons in online academic discussions.
Students need to possess functional technical skills to par-
ticipate in asynchronous discussions. The necessary level of
technical difficulty is not extremely high for most students,
but a lack of familiarity with the online discussion features
may cause frustration among some students. Frustration with
technical difficulties may affect the quality of participation
in the discussions (Davidson-Shivers, Muilenburg, & Tanner,
2001). The knowledge and skills necessary for students to
participate in asynchronous discussions can be effectively
addressed in instruction.
Reviewing various discussion strategies and considering
factors that can affect asynchronous discussions will aid in
planning. Analyzing the discussions is an additional task that
can provide information for reflecting and revising the over-
all discussion strategy in efforts to understand and improve
the online discussion's effectiveness for learning.
Methods to Analyze Online Discussions
There are many analysis methods reported in the literature
that have been used to assess learning in online discussions,
few have been applied extensively. Among the analysis
methods, two methods appeared to have been applied in sev-
eral other studies, and were considered for this study, the
Interaction Analysis Model (Lucas, Gunawardena, &
Moreira, 2014) and the Community of Inquiry (COI) frame-
work (Garrison & Arbaugh, 2007).
A study that used the Interaction Analysis Model from
Gunawardena, Lowe, and Anderson (1997) described an
analysis of an online debate. Gunawardena et al. character-
ized online debate as a " . . . co-creation of knowledge and
negotiation of meaning" (p. 406). The Interaction Analysis
Model consisted of five categories or phases, including (a)
sharing or comparing information, (b) discovery and explo-
ration, (c) negotiating of meaning or co-construction of
knowledge, (d) testing and modification or synthesis, and (e)
agreeing or application. From the coding and analysis, based
on these five categories, much of the participants' discus-
sions were determined to be characteristic of two categories,
exploration or discovery, and negotiation of meaning. The
Interaction Analysis Model categories have some similarities
of categories with Garrison's COI framework's practical
inquiry model. However, the COI framework, which was
chosen for this study, appeared more broad-based, including
categories of teaching presence, social presence, and cogni-
tive presence.
Theoretical Rationale
The theoretical framework of this study is based on the COI
framework (Garrison et al., 2001), which is grounded in con-
structivist learning theory that considers collaboration,
reflection, and critical analysis as essential to learning. The
COI framework is comprised of three core elements: social
presence, teaching presence, and cognitive presence. Social
presence, teaching presence, and cognitive presence are all
intertwined. "Cognitive presence is defined as the extent to
which learners are able to construct and confirm meaning
through sustained reflection and discourse" (p. 11), and can
be used as a " . . . means to assess the nature and quality of
critical, reflective discourse that occurs within the text based
educational environment" (p. 7).
The element of cognitive presence was selected as a pri-
mary focus for this study based on reviews of research stud-
ies that employed content analysis to assess online
discussions. Many of the content analysis studies that were
reviewed were identified in articles from De Wever,
Schellens, Valcke, and Van Keer (2006) and Rourke,
Anderson, Garrison, and Archer (2001). Content analysis is a
qualitative research method that has been used fairly exten-
sively, and its selection followed a "directed approach,"
where the analysis method is based on methods identified in
the literature (Hsieh & Shannon, 2005). In research pertain-
ing to the content analysis of asynchronous discussions,
research literature highlights the importance of reporting the
basis of its theoretical foundation including information
regarding the unit of analysis and the reliability of the study
(De Wever et al., 2006). The theoretical rationale, unit of
analysis, and reliability of analysis are all reported in this
study.
In developing the COI framework, a practical inquiry
model was developed. The practical inquiry model was
designed to be applied to analyze transcripts of online dis-
cussions for cognitive presence. The practical inquiry model
consists of four phases grounded in perception, deliberation,
conception, and action. Each phase reflects a process leading
to problem resolution beginning with identifying or under-
standing the problem, termed the triggering event. The sec-
ond phase involves exploration of the topic. In the third
phase, integration, possible resolutions or solutions or con-
clusions are identified, and the fourth phase, resolution, is
4 SAGE Open
where the solutions or conclusions are selected and applied.
These four phases or categories (triggering event, explora-
tion, integration, and resolution) represent a process of
evolving learning in asynchronous discussions that can be
supported and analyzed. The process to identify evidence of
each of the four phases is an interpretive process. To guide
the process of identifying cognitive presence in online dis-
cussions, categories of descriptors, indicators, sociocogni-
tive processes, and examples act as guidelines to facilitate
content analysis coding of the discussion transcripts
(Garrison et al., 2001). According to Garrison et al. (2001), it
is important to note that the practical inquiry model indica-
tors should "not be seen as immutable" (p. 9), meaning that
other studies using the practical inquiry model may find a
need to refine or revise the criteria to meet specific analysis
needs, as was the case for this study.
Method
In this study, an empirical method, content analysis, was
chosen to assess cognitive presence of three asynchronous
discussions of graduate students in one of their courses.
"Content analysis is a research technique for making repli-
cable and valid inferences from texts (or other meaningful
matter) to the contexts of their use" (Krippendorff, 2012, p.
24). Content analysis enables a process to systematically
examine the quality of learning in online discussions
(Gunawardena et al., 1997). Although the use of content
analysis dates back to the 1940s and 1950s, it was not until
the 1980s and 1990s that it began to be more frequently
applied to study learning in asynchronous discussions.
Henri (1992) described computer conferencing as a "gold
mine of information" (p. 118) that would provide research-
ers a rich resource to analyze and advance online learning.
Use of content analysis to assess online discussions has
increased over the past 20 years, just as Henri had pre-
dicted, but concerns about lack of uniformity and disclo-
sure of the analysis methods have arisen (De Wever et al.,
2006; Rourke et al., 2001).
Issues in comparing content analysis studies of online dis-
cussions have arisen due to a lack of consistency in the dif-
ferent analysis instruments used (Rourke &Anderson, 2004).
"This lack of replication (i.e., of successful applications of
other researchers' coding schemes) should be regarded as a
serious problem" (Rourke et al., 2001, p. 6). Consequently,
research literature has stressed the need for more studies to
employ similar instruments (T. Anderson, 2005; De Wever
et al., 2006), which in turn should increase the reliability and
validity of these types of studies (Stacey & Gerbic, 2003).
The importance of building on previous research influenced
choosing the method and the coding instrument of this study.
Repeating research designs helps establish the reliability of
the results, which can be obtained from repeated use of the
same instrument. Further information regarding this practi-
cal inquiry model, which has been applied in other content
analysis studies of online discussions, can be found in stud-
ies from Akyol and Garrison (2011), de Leng, Dolmans,
Jöbsis, Muijtjens, and van der Vleuten (2009), De Wever
et al. (2006), Fahy (2005), and others.
Analysis of the results of this study is aimed to (a) aid the
researcher in the planning of future asynchronous discus-
sions in the graduate program, (b) provide information for
instructors who are interested in studying the learning effec-
tiveness of their asynchronous discussions, and (c) apply les-
sons learned from this study to further studies involving the
use of content analysis as a method to evaluate online discus-
sions. One goal is to continue using content analysis to better
understand the use of this method to assess online learning
strategies. Analysis of other discussions from the same grad-
uate cohort of this study is planned.
Research Questions
Research Question 1: What were the levels of cognitive
and non-cognitive presence in a cohort of graduate stu-
dents' online discussion transcripts from one of their col-
lege courses?
Research Question 2: Considering that the use of roles in
online discussion was the main topic in the first online
discussion in the course, what were the students' percep-
tions at the end of the semester regarding the use of roles
in online discussions?
In the "Discussion" section of this study, the researcher
reflects on the use of content analysis and the practical
inquiry model as a means to assess the learning effectiveness
of online discussions.
Participants and Context
In this study, three online discussions of a hybrid graduate
course, the "Fundamentals of Online Pedagogy," were exam-
ined for cognitive presence. The researcher of this study was
the instructor of the course, which was primarily online, but
included three face-to-face class sessions at the beginning,
the middle, and the end of the semester. The course is part of
a master's program designed to be completed entirely
through a hybrid delivery system over four consecutive
semesters. All of the courses in the students' graduate pro-
gram typically met face-to-face only 2 to 3 times each semes-
ter. The course examined for this study was situated in the
first semester of the students' graduate program. There were
a total of 19 students (10 females, 9 males) participating in
the course, although only 15 students finished the course. A
random sample size of students (N = 15) was chosen for the
analysis. Three separate discussions were analyzed out of
eight total online discussions in the course. Each of these
discussions included three discussion groups, except the first
discussion which had five groups, and each of the discus-
sions occurred over a week time period. The reasoning
Rodriguez 5
behind selecting and analyzing the first three online discus-
sions of the course was to allow for the opportunity to make
observations of the students' learning progression in the
course from the beginning of the semester. Studies of the
other discussions in the same course and same students will
be based on the results of this study.
Online Discussion Strategy
The online discussion strategy used in each of the three dis-
cussions of the study was identical, consisting of a format
similar to the 3-2-1 discussion strategy, which included read-
ing a text, finding key points in the text, providing supportive
explanations, and posing questions. The instructions for the
discussion included several items:
·
· Students were divided into groups for the online
discussions.
·
· Each discussion required reading of an initial journal
article.
·
· Groups were to identify and discuss three main points
from the article with a goal of discussing and identify-
ing related topics to broaden and deepen students'
understanding of the topic.
·
· Students were then to search for another article based
on the three main points that had been identified and
discussed; read the new article; and then describe, dis-
cuss, and synthesize the concepts from the two
articles.
·
· To conclude the discussions, summaries were posed
synthesizing each group's discussions.
·
· Role assignments were assigned to each member of
the discussion group, which included monitor, encour-
ager, facilitator, quality assurance checker, and
summarizer.
Data Collection
The analysis of the online discussions was unobtrusive,
occurring after the students had already completed the course
and been given their final grades. Technology was used to
collect, process, and organize the data for analysis.
Blackboard, the University's learning management system,
was used to archive the discussion transcripts. Learning
management systems, such as Blackboard, support the use of
content analysis to evaluate online discussions with their
capacities to store online discussion transcripts that are eas-
ily accessible for later analysis. The discussion transcripts
for this study were exported from Blackboard, and then
imported into HyperRESEARCH, a qualitative software pro-
gram that was used to organize and code the discussion
transcripts.
A secondary data source included student survey data
which were collected at the end of the semester. The survey
inquired of the students about their perceptions regarding the
use of roles in online asynchronous discussions, and was the
subject of Discussion 1, which was based on the reading of
an article on the use of roles in online discussions from De
Wever, Van Keer, Schellens, and Valcke (2010).
Coding
Two coders, the researcher, who was the instructor of the
course, and another university instructor coded the three sets
of discussion transcripts. Training was provided to the coders
to aid them in coding the discussion transcripts. The training
involved the researcher/instructor of the course describing
and demonstrating to the second coder how to apply the cod-
ing instrument (Table 1) to the discussion transcripts.
Included in the training were discussions between the two
coders regarding the instructions for Discussion 1 that had
been given to the students who participated in the discussion.
The training occurred during 1 week, totaling approximately
3 to 4 hr. Discussion transcripts from Discussion 1 were used
in the training to demonstrate the coding. Following the
training, the coders worked independently in coding the units
of analysis.
The units of analysis consisted of each message thread
from each of the discussion participants in the three discus-
sions. Clear demarcation at the beginning of each message
identified for the coders where to begin and end each coding
effort. Each of the messages was analyzed by the two coders
and classified according to the indicator representing its level
of cognitive presence (see Table 1), which was based on a
modified version of the practical inquiry model (Garrison
et al., 2001). During the coding, if the coders determined that
more than one indicator of cognitive presence was evident in
a message thread, the indicator selected was to be based on a
preponderance of evidence in the message. Training for the
coders was considered a critically important factor that could
affect the reliability of the content analysis applied in this
study.
Variables
The variables related to cognitive presence were defined
based on four categories of Garrison's practical inquiry
model (Garrison et al., 2001), including triggering events,
exploration, integration, and resolution. Each broad category
contained a sub-level of indicators, which were used as the
basis to code each discussion message. Although the sub-
level indicators were drawn from Garrison's practical inquiry
model, they were modified to facilitate the coders in identi-
fying evidence in the online discussion transcripts of this
study. Categorization of the sub-level indicators provided
deeper insight into the cognitive and non-cognitive presence
displayed by the students in the discussions. Table 1 shows
the sub-level indicators for the broad categories of cognitive
and non-cognitive presence, including the percentages of
occurrence for the three discussions. The percentages of
6 SAGE Open
messages identified for each indicator reflected in Table 1
represent an agreed-on coding of each message based on dis-
cussion and reconciliation of messages between the two cod-
ers after all of the discussion messages had been coded.
The discussion messages were separated for each cogni-
tive and non-cognitive broad category by group for each of
the three discussions (see Tables 2, 3, and 4). These three
tables show the number of messages coded for each broad
category, including the triggering event, exploration, integra-
tion, resolution, and non-cognitive. Inter-rater reliability of
the coding was calculated for Discussions 2 and 3 with the
use of Cohen's kappa statistical measurements, including
percentages of agreement between coders (see Table 5).
Potential for Coding Error and Bias
The researcher of this study was the instructor of the course
with 20 years of experience in the field of educational technol-
ogy. Steps were taken to reduce any ambiguity and bias in the
coding process: (a) Student names were removed from the dis-
cussion transcripts in preparation for the coding. This elimi-
nated coder bias toward any student names. (b) Training and
practice coding of Discussion 1 was provided to emphasize
procedures for the coding based strictly on the criteria estab-
lished for each indicator (see Table 1). Training was consid-
ered essential to reduce ambiguity in the judgment process for
coding each message. (c) Demarcation of the unit of analysis
Table 1. Messages Coded for Cognitive Presence by Indicators by Discussion.
Cognitive presence Indicators D1 (%) D2 (%) D3 (%)
1. Triggering a. Recognizes or identifies problems, concepts, or issues 0.45 10.5 5.5
b. Describes only the assigned reading 0.45 1.0 2.7
Sub-total 0.9 11.5 8.2
2. Exploration a. Adds to established points but does not systematically defend/justify/develop. 11.5 4.5 4.5
b. Presents relevant background information related to discussion topic. 2.4 13.0 2.1
c. Adds suggestions about discussion topic. 3.0 2.0 4.7
d. Asks questions seeking specialized information. 4.1 14.0 4.1
e. Offers opinions. 9.4 7.5 5.2
Sub-total 30.5 41.0 20.6
3. Integration a. Explores potential solutions, applications, or conclusions. 1.7 3.5 3.1
b. Draws conclusions or summarizes discussion. 3.0 5.5 6.2
c. 
Reference to previous message followed by substantiated agreement, for
example, "I agree because . . ."
1.3 6.0 8.2
d. Substantiated building on, adding to others' ideas. 1.3 1.0 4.8
e. 
Synthesis: Connecting ideas. Integrating information from various sources--
Textbook, articles, and personal experience.
9.0 1.0 5.8
f. Providing rational, justifications. 1.7 1.0 4.1
Sub-total 18.0 18.0 32.3
4. Resolution a. Applying, testing, defending, or critiquing solutions or conclusions. 0.0 0.0 0.7
b. Suggests applications or action to take. 0.9 2.0 2.1
c. Commits to solutions or conclusions. 2.1 0.0 0.7
Sub-total 3.0 2.0 3.4
5. Non-cognitive a. Clarifying discussion procedures. 32.6 11.0 19.6
b. Encouraging. 13.3 15.5 15.8
c. Not coded, off topic. 1.7 1.0 0.0
Sub-total 47.6 27.5 35.5
Total 100 100 100
Table 2. Total Messages by Cognitive and Non-Cognitive Levels by Group.
Discussion 1 Group 1 Group 2 Group 3 Group 4 Group 5 Total %
Triggering event 1 0 1 0 0 2 0.9
Exploration 12 36 9 10 4 71 30.5
Integration 6 5 5 8 18 42 18.0
Resolution 0 2 1 2 2 7 3.0
Non-cognitive 23 26 16 29 17 111 47.6
Total 42 69 32 49 41 233 100
Rodriguez 7
of the coding was discussed between the coders to eliminate
any errors from confusion in the unit of data to code. The unit
of analysis for coding consisted of the message as opposed to
sentences or paragraphs. However, there was the possibility of
messages containing evidence of more than one indicator,
which was addressed in this study by coding the messages
based on the preponderance of evidence in the message.
Results
A total of 724 messages were posted in the three discussions.
Discussion 1 included a total of 233 messages, Discussion 2
contained 200 messages, and Discussion 3 had 291 messages.
For all of the three discussions, the average percentage of
messages coded for each cognitive presence category resulted
in 6.8% triggering events, 29.4% exploration, 23.8% integra-
tion, and 2.9% resolution. In addition, messages coded as
non-cognitive accounted for 37.1% of the total messages.
Three categories, non-cognitive, exploration, and integra-
tion, accounted for roughly 90% of the total messages in the
three discussions. The lowest percentage of responses was in
the categories of resolution (2.9%) and the triggering event
category (6.8%), which were both considerably lower than
any of the other three categories. The messages categorized
as triggering events primarily consisted of descriptions of the
articles assigned by the instructor for the discussions. The
discussion messages categorized as resolution primarily con-
sisted of student summaries of the discussions with little
back-and-forth discussion among the students about the
summaries. In online discussions, students appear to need
more direct guidance to increase discussion that would be
characteristic of resolution.
Messages coded as exploration (29.4%) and integration
(23.8%) accounted for the highest frequencies of cognitive
presence in this study. High frequencies of exploration and
integration messages have been similarly reported in the
findings of other studies that also relied on the primary
inquiry model and content analysis to evaluate learning in
the discussions (Garrison et al., 2001; Liu & Yang, 2012).
The non-cognitive category accounted for the overall highest
frequency (37.1%) of messages, which was 7.7% higher than
the next highest category of exploration (29.4%). Students
who did not follow the instructions in Discussion 1 appeared
to directly influence the higher frequency of messages coded
as non-cognitive (47.1%), which was substantially higher
than non-cognitive messages in Discussions 2 (27.5%) and 3
(35.5%). Discussions 2 and 3 did not have as many messages
asking questions about the instructions and procedures of the
online discussion. A solution to reduce potential non-cogni-
tive messages is to teach the students about the procedures
and processes prior to the first discussion. In a fully online
course, this could be accomplished through a separate dis-
cussion thread that focused exclusively on protocols of the
discussions. In a hybrid course, this could be addressed in the
first face-to-face class.
Table 3. Total Messages by Cognitive and Non-Cognitive Levels by Group.
Discussion 2 Group 1 Group 2 Group 3 Total %
Triggering event 14 7 2 23 11.5
Exploration 35 27 20 82 41.0
Integration 4 13 19 36 18.0
Resolution 0 3 1 4 2.0
Non-cognitive 28 23 4 55 27.5
Total 81 73 46 200 100
Table 4. Total Messages by Cognitive and Non-Cognitive Levels by Group.
Discussion 3 Group 1 Group 2 Group 3 Total %
Triggering event 7 15 2 24 8.2
Exploration 29 17 14 60 20.6
Integration 24 31 39 94 32.3
Resolution 4 1 5 10 3.4
Non-cognitive 54 33 16 103 35.5
Total 118 97 76 291 100
Table 5. Inter-Rater Reliability and Percent Agreement.
% agreement Cohen's kappa
Discussion 2 Group 1 64.5 .49
Discussion 2 Group 2 88.6 .85
Discussion 2 Group 3 61.4 .47
Discussion 3 Group 1 60.9 .45
Discussion 3 Group 2 57.8 .44
Discussion 3 Group 3 70.4 .59
8 SAGE Open
Displayed in Tables 2, 3, and 4 are the total number of
messages for each of the three discussions which are listed
by group showing the total percentages of cognitive presence
(triggering event, exploration, integration, and resolution)
and non-cognitive presence. There were five groups partici-
pating in Discussion 1, three groups in Discussion 2, and
three groups in Discussion 3.Apattern appeared to be emerg-
ing from the overall frequency of messages coded for the
different categories of cognitive and non-cognitive presence.
Messages coded as non-cognitive or exploration were usu-
ally the most frequent type of messages identified in each of
the three discussions. Messages coded as resolution or as
triggering events were identified as the least frequent mes-
sages in all three discussions. Messages coded as integration
were usually identified as the third most frequently occurring
messages in the three discussions, except in Discussion 3
where they were second highest, only 3.2% lower than total
non-cognitive messages.
Discussion 1 Cognitive and Non-Cognitive
Presence
Discussion 1 had the highest percentage of messages coded
as non-cognitive, 47.6% (see Table 2) of the messages, com-
pared with an average of 37.1% of non-cognitive messages
for all three discussions. Table 1 shows that most of the mes-
sages classified as non-cognitive were coded for clarifying
discussion procedures. Messages coded as exploration were
slightly higher at 30.5%, when compared with an average of
29.4% for all three discussions. Integration messages in
Discussion 1 were lower at 18%, similar to Discussion 2's,
but considerably lower than Discussions 3's percentage of
messages categorized for integration at 32.3%. Few mes-
sages (0.9%) were coded as triggering events. However,
some of the messages that were coded as exploration and
integration contained evidence characteristic of triggering
event criteria, but were coded as exploration or integration
due to the preponderance of evidence in the message.Another
factor affecting the low frequency of triggering events in
Discussion 1 was partially a result of the students being
introduced to the topic of Discussion 1 during the first face-
to-face class meeting. Students having the opportunity to talk
to each other about Discussion 1's reading assignment in a
face-to-face environment eliminated some of the online dis-
cussion that likely would have occurred.
Discussion 2 Cognitive and Non-Cognitive
Presence
In Discussion 2 (see Table 3), the data show an increase of
10.6% in messages coded as triggering events, up 11.5%
compared with Discussion 1 at 0.9%, while the average for
triggering events for all three discussions was at 6.8%. The
content of the triggering event messages in Discussion 2
clearly showed that the students were able to adequately
identify and describe the topic of the discussion, which was
based on an article focused on social presence in asynchro-
nous discussions.
Also in Discussion 2, students were much more active
exploring and discussing additional perspectives related to
the topic of the assigned reading. The increased discussion
activity resulted in the highest percentage of messages coded
for exploration with 41%, compared with the 29.4% explora-
tion average of all three discussions. Furthermore, data for
Discussion 2, as seen in Table 1, show that 14% of the stu-
dents' messages focused on asking questions seeking spe-
cialized information. Discussion 2 also showed a 20.1%
decrease of messages coded as non-cognitive compared with
Discussion 1, due to fewer messages coded for clarifying
discussion procedures. It appeared that the students had
become more comfortable with the procedures for the online
discussion. Also, in Discussion 2, messages identified as
resolution remained low at 2% of the total messages, which
consisted of students reporting a summary that synthesized
the overall discussion, although there continued to be limited
discussion among the students regarding the summary.
Discussion 3 Cognitive and Non-Cognitive
Presence
Discussion 3 had the highest frequency of messages coded
for integration at 32.3%, compared with an average of 23.8%
for all three discussions. Integration reflects a higher level of
thinking skills reflecting students' ability to synthesize the
discussion concepts and form summary conclusions. In
Discussion 3, there was a higher percent of non-cognitive
messages at 35.4%, compared with 27.5% in Discussion 2.
The increase of non-cognitive messages in Discussion 3
from Discussion 2 was due to a moderate increase in mes-
sages that were seeking clarification (see Table 1). Discussion
3 was focused on "concept mapping," which was for some
students a difficult concept to read about and discuss and
then apply. In the course, there was an assignment that
required the creation of concept maps. Some confusion arose
among the students between the assignments requiring the
actual building of a concept map with the online discussion
about concept mapping. Conversely, the increase in integra-
tion messages appeared to be a result of more discussion
requiring the forming of solutions of how to develop concept
maps that would be applied.
Reliability
Inter-rater reliability identifies the extent of agreement
among the coders and takes into account any agreement
occurring by chance. Information of the inter-rater reliability
provides an indication of the reproducibility and stability of
a study, and is considered an essential element of content
analysis (De Wever et al., 2006; Lombard, Snyder-Duch, &
Bracken, 2002).
Rodriguez 9
There are differences in interpretations regarding accept-
able levels of inter-rater reliability coefficients. According to
Landis and Koch (1977), kappa coefficients of "0-.20 are
slight, .21-.40 fair, .41-.60 moderate, .61-.80 substantial, and
.81-1.00 perfect" (p. 165). However, literature also shows the
existence of differing interpretations regarding coefficient
levels, including interpretations saying that coefficients
below .8 give rise to concern (Lombard et al., 2002). In a
similar study, Garrison et al. (2001) assessed cognitive pres-
ence in two online discussions and calculated kappa coeffi-
cients at levels of .35, .49, and .74, and .45, .65, and .84. The
increasingly higher coefficients, from .35 to .75, and from
.45 to 84, were thought to be a result of increased training for
the coders. Training appeared to be an influencing factor.
In content analysis, latent content is often considered a
challenging area of concern. The latent nature of discussions
involves potential reliability issues related to the coders'" . . .
interpretations of the meaning of the content" (Potter &
Levine-Donnerstein, 1999, p. 259). To alleviate any issues of
reliability associated with latent content, training proves to
be a key factor. Higher coefficients are thought to be more
achievable as a result of coders having more training with the
processes related to coding (Riffe, Lacy, & Fico, 1998).
In Table 5, the kappa coefficients and percentages are dis-
played for Discussions 2 and 3. Discussion 1 is not displayed
as it was used in the training process for the two coders. There
were three separate group discussions in Discussion 2 and
three group discussions for Discussion 3. Inter-rater reliabil-
ity coefficients were calculated for each group discussion.
Most of the coefficients were in the moderate range from .41
to .60. However, for Discussion 2, Group 2, an inter-rater reli-
ability of .85 was calculated, which is a substantially higher
kappa coefficient than all of the other discussions. The coders
periodically compared and discussed their coding of
Discussion 1. For Discussions 2 and 3, the coders compared
and discussed their coding only after the entire coding was
completed, and did not periodically compare coding results
during the coding of the discussion transcripts.
The training that was provided to the coders of this study
included (a) discussions of the coding instrument and the
indicators, (b) discussions on how to apply and record the
coding, (c) a review of the instructions and procedures for
the discussions, and (d) practice coding of Discussion 1,
including discussing the differences in the coding results and
arriving at an agreement to resolve differences.
The low reliability coefficients were a result of differing
interpretations between the two coders regarding the content
of the reading assignments. These differences were exam-
ined, discussed, and resolved after the coding of Discussions
2 and 3. The sub-level indicators appeared to be a source of
the problem in applying and coding the discussion messages.
In discussing the differences of the coding, each message
was considered in relation to the sub-level indicator and in
relation to the broader category (triggering event, explora-
tion, integration, and resolution).
Descriptions of the type of content in a message would
meet specific categorization criteria based on the sub-level
indicators of each broad category. Messages coded as trig-
gering event were to basically describe the initial article.
Differing scores arose from the coder not knowing the differ-
ence between new information from a new article from infor-
mation that was derived from the assigned article of the
discussion. Thorough review of the assigned article was
needed.
Sub-level indictors for integration and exploration were
the areas where many other scores differed. Examination of
differences of integration and exploration made apparent a
need to further refine the sub-level indicators along with an
examination of the prompts or instructions for the discus-
sion. Each of the messages coded differently in these two
areas were discussed in relation to the articles associated
with the discussions until an agreement was made.
As a result, specific type of training was also identified
that would need to focus on the subject matter of the topics
that were the focus of each online discussion. Overall, the
process of evaluating the coding provided insight into refine-
ments or changes to the discussion strategy that may have a
positive impact on the use of online discussions for
learning.
Survey Results
Among the learning goals for the students in the course
involved in this study was to develop knowledge of online
discussion strategies. A survey was given to the students at
the end of the semester to examine their perceptions of a dis-
cussion strategy that was the main topic of Discussion 1,
which was focused on the use of roles in online discussion.
The survey results collected at the end of the semester
showed that all of the students, except one, believed that the
use of roles in online discussions would result in generating
greater participation from all the members of the discussions,
which is a contributing factor for quality and productivity in
online discussions. The student who did not wholeheartedly
support the use of roles as essential to online discussions
believed that roles required too much structure in an online
discussion, which could inhibit free-flowing discourse.
Second, the role of summarizer was identified by a majority
of students as the most important and difficult role in an
online discussion. Students also indicated in the survey that
they had difficulty with summarizing skills, although they did
believe that summarizing discussions would help foster
deeper understanding of the topics being discussed. In addi-
tion, students were evenly divided in their beliefs about rotat-
ing roles in discussions. Half of the students believed that
maintaining the same role was more beneficial to the discus-
sions than changing roles from discussion to discussion.
Maintaining the same role was believed to help the students
master the role, which would lead to higher quality discus-
sions. The other half believed that changing roles from
10 SAGE Open
discussion to discussion would help learning new skills from
roles that may not have been chosen. Most students believed
that their choice of roles would be influenced by their comfort
level, although the students also indicated that there was
value in being forced out of their comfort zone. Overall, there
was a strong indication that the students favored the use of
roles as an important online discussion strategy for learning.
Discussion
Examining the three discussions through the research frame-
work of this study provided deeper insights into the impact
of online discussion strategies in learning. Reviewing the lit-
erature for online discussion strategies revealed a need for
more perspectives of research-based reviews of online dis-
cussion strategies. For example, Darabi et al. (2013), in their
meta-analysis of online discussion strategies, could only
locate eight articles that met their rigorous research criteria.
In determining a theoretical rationale to frame this study of
online discussions, also revealed was a dearth of frameworks
that had been applied extensively in research. However, the
practical inquiry of Garrison et al. (2001) that was applied in
this study appeared to be flexible and adaptable enough to
serve as a basis to analyze many types of online discussions.
Applying the practical inquiry framework to analyze the
three online discussions of this study made evident the need
for careful instructional planning of the online discussion
prompts with consideration of triggering events, exploration,
integration, and resolution.
Overall, Tables 2, 3, and 4 show strong evidence of cogni-
tive presence in the three online discussions. The distribution
of discussion frequencies was acceptable with a substantial
volume of the discussion posts evident of exploration and
integration. The small percentage of triggering event mes-
sages was acceptable with 8% to 10% of the messages ade-
quately covering the necessary information essential to the
discussions. Students appeared to progress fairly well
through the discussions, first identifying the topic of discus-
sion (triggering event), expanding with ideas for further dis-
cussion (exploration), and then discussing and integrating
new ideas. Strengthening the discussions strategy could be
accomplished with more direction in applying solutions or
conclusions to real or virtual situations. In the original
instructions for the online discussions given to the students,
there were not any directions instructing the students to apply
their conclusions to real-life or virtual settings, although
there were a few students who were K-12 teachers that did
discuss applications of the conclusion to their real-life class-
room situations.
Non-Cognitive Presence
The percentages of non-cognitive messages appeared rather
high in the first discussion, went down in the second discus-
sion, and then increased slightly in the third discussion.
Online discussions are sometimes filled with non-cognitive
off-task messages, which was not a problem in this study.
Non-cognitive messages that seek clarifications of the dis-
cussion instructions can help facilitate the discussion, and
can be supported in various ways depending on whether the
course is a hybrid or fully online course. Clarification of
instructions can be taught in face-to-face class sessions, or
separate threads dedicated to clarifying instructions for the
discussion would help. "Teaching presence," which is a
major concept of the COI (Garrison et al., 2001), can be ele-
vated in online discussions through separate discussion
threads led by the instructor of the course. Students appear to
expect the instructor to have strong teaching presence in
online discussions.
The messages coded as encouraging are characteristic of
social presence, also a major concept of the COI model.
Messages coded as encouraging were moderately evident
throughout the three discussions. Encouraging messages can
increase social presence in online discussions, which pro-
motes positive outcomes. Students in this study were influ-
enced by the use of encourager roles, which was used in all
three discussions. The use of roles as a discussion strategy
has been shown to have a positive impact on student interac-
tion in asynchronous discussions (De Wever et al., 2010;
Yeh, 2010), which was confirmed by this study.
Reflections on Content Analysis and the Practical
Inquiry Model
T. Anderson (2005) suggested that there must be an easier
way to support and confirm acceptance of online learning
other than through content analysis, and that few of the con-
tent analysis methods for online discussions have been used
repeatedly enough, creating a problem comparing studies.
Although difficulties do arise in applying content analysis to
study online discussions, for those desiring to improve their
online discussions, the benefits of applying content analysis
outweigh the difficulties in its use.
Major difficulties in using content analysis include (a)
modifying the indicators' criteria, (b) developing training
for the coders, (c) training coders, and (d) the time it takes
to complete these tasks. Content analysis is a tedious pro-
cess to apply in the analysis of online discussion tran-
scripts. Thorough training is required for the coders to
establish acceptable reliability measurements. There has
been some difficulty in training others to discern the dif-
ferences between phases of the practical inquiry model.
The integration " . . . phase is the most difficult to detect
from a teaching or research perspective" (Garrison et al.,
2001, p. 10). The inter-rater reliability of this study was
affected by issues that typically affect content analysis
studies, relating to the coders being knowledgeable of the
subject matter in the study, and crafting measurability
instruments that address this issue (Potter & Levine-
Donnerstein, 1999).
Rodriguez 11
Limitations
Limitations of this study include a small sample size involv-
ing 15 graduate students from an educational technology
master's program in their first semester of the program. The
results may be generalizable to analogous populations using
comparable online discussion strategies. However, the
design of coder training and the development of criteria for
coding indicators may possibly affect the results. The pri-
mary inquiry model, which includes the categories of trig-
gering events, exploration, integration, and resolution,
appears to provide a strong framework to serve as a base for
the development of the coding criteria. Other factors that
may affect generalizability include variables related to stu-
dent backgrounds (e.g., student context factors including
academic, language, social, and socioeconomic factors).
These factors should be considered in developing and exam-
ining online learning (Swan, 2001).
Other limitations include the following: The study only
examined one element, cognitive presence, from the COI
model (Garrison et al., 2001), excluding elements of teach-
ing presence and social presence. Although there were no
plans at the onset of the study to examine teaching and social
presence, there is some indication of their presence in the
non-cognitive messages. Social presence appears to be evi-
dent in the messages containing "encouraging" comments,
and teaching presence is potentially present in the messages
containing comments related to instructional procedures. It is
worthwhile to examine these two areas more closely in future
studies.
Conclusion
The research literature on content analysis of online discus-
sions calls repeatedly in its conclusions for the need of stud-
ies that contain all of the following: (a) a systematic
coherence between theory and analysis categories, (b) clear
choice of the unit of analysis, and (c) information about the
inter-rater reliability procedures (De Wever et al., 2006).
These are the essential requirements needed for content anal-
ysis studies that provide the means for professional compari-
sons that may contribute to improved conditions in online
teaching and learning. In addition to meeting the above three
requirements necessary to compare studies applying content
analysis to online discussions, there are other concerns to
address in the process of applying content analysis to study
online discussions.
There are several important steps in applying methods of
content analysis to analyze learning in online discussions.
These steps are not meant to be comprehensive but are les-
sons learned from this study:
1. Use a software program such as HyperResearch or
Atlas.ti, which is very helpful in sorting, analyzing,
and reporting data.
2. Use an existing theoretical rationale or research
design such as the COI (Garrison et al., 2001).
Although there is pressure to develop an individual-
ized instrument, researchers and practitioners can
benefit from repetitive studies that thoroughly vet
research designs.
3. Examine and understand the online discussion strate-
gies in meeting learning objectives associated with
the online discussions.
4. Modify the criteria (indicators) of the coding instru-
ment to align with the identification of the content of
the online discussion messages in relation to the
learning objectives.
5. Select coders who are knowledgeable with the sub-
ject matter of the online discussions.
6. Plan thorough training for the coders.
7. Assess training of coders for inter-rater reliability
measurements that are at least .8.
8. In analyzing the online discussions, analyze for cog-
nitive presence, teaching presence, and social
presence.
This study used an existing model for content analysis to
analyze online discussions of a course previously taught. The
purpose was to improve the use of online discussions to sup-
port learning in hybrid courses in education. The study con-
firms the value by applying the framework practical inquiry
model (Garrison et al., 2001) and further identifies important
steps or lessons learned in applying content analysis to assess
learning of online discussions. Finally, it was the experience
of the researcher of this study that the process of applying
content analysis to examine a course's online discussions
will provide additional perspectives on development and use
of online discussions for learning.
Declaration of Conflicting Interests
The author(s) declared no potential conflicts of interest with respect
to the research, authorship, and/or publication of this article.
Funding
The author(s) received no financial support for the research and/or
authorship of this article.
References
Akyol, Z., & Garrison, D. R. (2011). Understanding cognitive pres-
ence in an online and blended Community of Inquiry: Assessing
outcomes and processes for deep approaches to learning. British
Journal of Educational Technology, 42, 233-250.
Allen, I. E., & Seaman, J. (2013). Changing course: Ten years of
tracking online education in the United States. Newburyport,
MA: Sloan Consortium.
Anderson, R. S., Goode, G. S., Mitchell, J. S., & Thompson, R. F.
(2013). Four online discussion strategies: Perceptions of seven
doctoral students. Journal of Literacy and Technology, 14(2),
115-160.
12 SAGE Open
Anderson, T. (2005). Online education innovation: Going boldly
where others fear to thread. In G. Kearsley (Ed.), Online learn-
ing: Personal reflections on the transformation of education
(pp. 1-11). Englewood Cliffs, NJ: Educational Technology
Publications.
Benbunan-Fich, R., & Hiltz, S. R. (1999). Impacts of asynchro-
nous learning networks on individual and group problem solv-
ing: A field experiment. Group Decision and Negotiation, 8,
409-426.
Bergmann, J., & Sams, A. (2012). Flip your classroom: Reach every
student in every class every day. Eugene, OR: International
Society for Technology in Education.
Bliuc, A. M., Ellis, R. A., Goodyear, P., & Piggott, L. (2011). A
blended learning Approach to teaching foreign policy: Student
experiences of learning through face-to-face and online dis-
cussion and their relationship to academic performance.
Computers & Education, 56, 856-864.
Borich, G. D., & Tombari, M. L. (1997). Educational psychology:
A contemporary approach. New York, NY: Longman.
Bowen, W. G. (2013, March 25). Walk deliberately, don't run,
toward online education. The Chronicle of Higher Education.
Retrieved from http://chronicle.com/article/Walk-Deliberately-
Dont-Run/138109/
Bruner, J. (1986). Actual minds, possible worlds. Cambridge, MA:
Harvard University Press.
Darabi, A., Liang, X., Suryavanshi, R., & Yurekli, H. (2013).
Effectiveness of online discussion strategies: A meta-Analysis.
American Journal of Distance Education, 27, 228-241.
Davidson-Shivers, G. V., Muilenburg, L. Y., & Tanner, E. J. (2001).
How do students participate in synchronous and asynchro-
nous online discussions? Journal of Educational Computing
Research, 25, 351-366.
de Leng, B. A., Dolmans, D. H., Jöbsis, R., Muijtjens, A. M., & van
der Vleuten, C. P. (2009). Exploration of an e-learning model
to foster critical thinking on basic science concepts during
work placements. Computers & Education, 53, 1-13.
De Wever, B., Schellens, T., Valcke, M., & Van Keer, H. (2006).
Content analysis schemes to analyze transcripts of online
asynchronous discussion groups: A review. Computers &
Education, 46, 6-28.
De Wever, B., Van Keer, H. V., Schellens, T., & Valcke, M. (2010).
Roles as a structuring tool in online discussion groups: The dif-
ferential impact of different roles on social knowledge con-
struction. Computers in Human Behavior, 26, 516-523.
Dillenbourg, P. (1999). What do you mean by collaborative learn-
ing? In P. Dillenbourg (Ed.), Advances in Learning and
Instruction: Collaborative-learning--Cognitive and compu-
tational approaches (pp. 1-19). Bingley, UK: Emerald Group
Publishing.
Fahy, P. (2005). Two methods for assessing critical thinking in
computer-mediated communications (CMC) transcripts.
International Journal of Instructional Technology and
Distance Learning, 2(3), 13-28.
Garrison, D. R., Anderson, T., & Archer, W. (2001). Critical
thinking, cognitive presence, and computer conferencing in
distance education. American Journal of Distance Education,
15, 7-23.
Garrison, D. R., & Arbaugh, J. B. (2007). Researching the
Community of Inquiry framework: Review, issues, and future
directions. The Internet and Higher Education, 10, 157-172.
Gunawardena, C. N., Lowe, C. A., & Anderson, T. (1997). Analysis
of a global online debate and the development of an interaction
analysis model for examining social construction of knowledge
in computer conferencing. Journal of Educational Computing
Research, 17, 397-431.
Hansen, D. T. (1988). Was Socrates a "Socratic teacher."
Educational Theory, 38, 213-224.
Henri, F. (1992). Computer conferencing and content analysis. In
A. R. Kaye (Ed.), Collaborative learning through computer
conferencing: The Najaden papers (pp. 117-136). Berlin,
Germany: Springer.
Hofer, B. K. (2001). Personal epistemology research: Implications
for learning and teaching. Educational Psychology Review, 13,
353-383.
Hsieh, H. F., & Shannon, S. E. (2005). Three approaches to quali-
tative content analysis. Qualitative Health Research, 15,
1277-1288.
Jaggars, S. S., & Bailey, T. (2010). Effectiveness of fully online
courses for college students: Response to a Department
of Education meta-analysis. Available from http://ccrc.
tc.columbia.edu/
Johnson, S. D., & Aragon, S. R. (2003). An instructional strategy
framework for online learning environments. New Directions
for Adult and Continuing Education, 2003(100), 31-43.
John-Steiner, V., & Mahn, H. (1996). Sociocultural approaches
to learning and development: A Vygotskian framework.
Educational Psychologist, 31, 191-206.
King, A. (1993). From sage on the stage to guide on the side.
College Teaching, 41, 30-35.
Krippendorff, K. (2012). Content analysis: An introduction to its
methodology. Beverly Hills, CA: SAGE.
Landis, J. R., & Koch, G. G. (1977). The measurement of observer
agreement for categorical data. Biometrics, 33, 159-174.
Larson, L. (2009). Reader response meets the new literacies:
Empowering readers in online learning communities. The
Reading Teacher, 62, 638-648.
Lebow, D. (1993). Constructivist values for systems design: Five
principles toward a new mindset. Educational Technology
Research & Development, 41, 4-16.
Liu, C. J., & Yang, S. C. (2012). Applying the practical inquiry
model to investigate the quality of students' online discourse in
an information ethics course based on Bloom's teaching goal
and Bird's 3C model. Computers & Education, 59, 466-480.
Lombard, M., Snyder-Duch, J., & Bracken, C. C. (2002). Content
analysis in mass communication: Assessment and reporting of
intercoder reliability. Human Communication Research, 28,
587-604.
Lucas, M., Gunawardena, C., & Moreira, A. (2014). Assessing
social construction of knowledge online: A critique of the
interaction analysis model. Computers in Human Behavior, 30,
574-582.
Maor, D. (2003). The teacher's role in developing interaction and
reflection in an online learning community. Educational Media
International, 40, 127-138.
McLoughlin, C., & Oliver, R. (1998). Maximising the language
and learning link in computer learning environments. British
Journal of Educational Technology, 29, 125-136.
Means, B., Toyama, Y., Murphy, R., Bakia, M., & Jones, K.
(2010). Evaluation of evidence-based practices in online
learning: A meta-analysis and review of online learning
Rodriguez 13
studies. Washington, DC: U.S. Department of Education,
Office of Planning, Evaluation, and Policy Development.
Meyer, K. A. (2003). Face-to-face versus threaded discussions: The
role of time and higher-order thinking. Journal of Asynchronous
Learning Networks, 7(3), 55-65.
Parker, W. C., & Hess, D. (2001). Teaching with and for discussion.
Teaching and Teacher Education, 17, 273-289.
Parsad, B., Jones, J., & Greene, B. (2005). Internet access in
US public schools and classrooms: 1994-2003 (ED TAB,
NCES 2005-015). Washington, DC: U.S. Department of
Education.
Potter, W., & Levine-Donnerstein, D. (1999). Rethinking valid-
ity and reliability in content analysis. Journal of Applied
Communication Research, 27, 258-284.
Riffe, D., Lacy, S., & Fico, F. G. (1998). Analyzing media messages:
Using quantitative content analysis in research. Mahwah, NJ:
Lawrence Erlbaum.
Rourke, L., & Anderson, T. (2004). Validity in quantitative content
analysis. Educational Technology Research & Development,
52, 5-18.
Rourke, L., Anderson, T., Garrison, D. R., & Archer, W. (2001).
Methodological issues in the content analysis of computer
conference transcripts. International Journal of Artificial
Intelligence in Education, 12, 8-22.
Stacey, E., & Gerbic, P. (2003). Investigating the impact of com-
puter conferencing: Content analysis as a manageable research
tool. In G. Crisp, D. Thiele, I. Scholten, S. Barker & J. Baron
(Eds.), Interact, Integrate, Impact. Proceedings of the 20th
Annual Conference of the Australasian Society for Computers
in Learning in Tertiary Education (ASCILITE) (pp. 495-503).
Adelaide: Australasian Society for Computers in Learning in
Tertiary Education.
Swan, K. (2001). Virtual interaction: Design factors affecting stu-
dent satisfaction and perceived learning in asynchronous online
courses. Distance Education, 22, 306-331.
Swan, K. (2005). A constructivist model for thinking about learn-
ing online. In J. Bourne & J. C. Moore (Eds.), Elements of
quality online education: Engaging communities (pp. 13-30).
Needham, MA: Sloan-C.
Tiene, D. (2000). Online discussions: A survey of advantages and
disadvantages compared to face-to-face discussions. Journal of
Educational Multimedia and Hypermedia, 9, 369-382.
Vygotsky, L. S. (1980). Mind in society: The development of higher
psychological processes. Boston, MA: Harvard University Press.
Wang, Q., & Woo, H. L. (2007). Comparing asynchronous online
discussions and face-to-face discussions in a classroom setting.
British Journal of Educational Technology, 38, 272-286.
Yang, Y. T. C., Newby, T. J., & Bill, R. L. (2005). Using Socratic
questioning to promote critical thinking skills through asyn-
chronous discussion forums in distance learning environments.
The American Journal of Distance Education, 19, 163-181.
Yeh, Y. C. (2010). Analyzing online behaviors, roles, and learning
communities via online discussions. Journal of Educational
Technology & Society, 13(1), 140-151.
Yuan, J., & Kim, C. (2014). Guidelines for facilitating the devel-
opment of learning communities in online courses. Journal
of Computer Assisted Learning, 30, 220-232. doi:10.1111/
jcal.12042
Author Biography
Mark Rodriguez is a professor of Education. He is currently the
Administrator in Charge of Academic Technology & Creative
Services at California State University, Sacramento.
