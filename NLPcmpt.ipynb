{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gensim\n",
    "import scipy\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#simple test\n",
    "sentences = [['first', 'sentence'], ['second', 'sentence']]\n",
    "# train word2vec on the two sentences\n",
    "model = gensim.models.Word2Vec(sentences, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-3.4526961e-03,  2.3118081e-03,  2.4384053e-03,  4.4007921e-03,\n",
       "        1.2382763e-03, -2.9070906e-03,  1.1367354e-03, -4.5268903e-03,\n",
       "       -2.3104530e-03, -1.5687657e-03,  1.6342090e-04,  7.7269861e-04,\n",
       "        2.2091225e-03, -4.4522155e-03, -4.4334903e-03, -1.4050103e-04,\n",
       "       -3.5099732e-03,  4.9146661e-03,  8.1566646e-04,  4.4399332e-03,\n",
       "        1.1467126e-03, -3.0689740e-03,  3.8246163e-03, -2.3882238e-03,\n",
       "        2.0566764e-03, -7.1487098e-04, -4.9565434e-03,  3.3371127e-03,\n",
       "        1.0608492e-03,  1.7947876e-03,  3.3255599e-03, -1.6293864e-03,\n",
       "        2.0660569e-03, -2.5953236e-04, -4.9334881e-04, -1.7169239e-03,\n",
       "        3.7111726e-03,  2.0328916e-03,  1.7572351e-03,  1.1983340e-04,\n",
       "       -4.9522228e-04,  3.0089790e-04,  3.7768958e-03, -4.4228062e-03,\n",
       "        4.3594092e-03,  2.5589243e-03,  4.4387323e-03, -4.0921452e-03,\n",
       "        1.3489388e-04,  3.9382940e-03, -1.5831060e-03, -4.1275551e-03,\n",
       "       -3.1102083e-03,  1.7296202e-03, -2.6884533e-03,  3.9885524e-03,\n",
       "        4.5165536e-04, -2.2935809e-03, -4.1952822e-03,  9.2845649e-04,\n",
       "        2.4093136e-03, -3.9069676e-03,  1.9768234e-03,  4.7426568e-03,\n",
       "       -3.7285283e-03,  1.6855864e-03, -2.6640610e-03, -1.7159664e-03,\n",
       "        1.7620644e-03, -1.6219402e-03, -7.8148622e-04, -5.3391117e-04,\n",
       "        3.8633640e-03,  4.2938809e-03, -4.1795490e-04, -2.3427848e-03,\n",
       "        1.3750264e-03,  3.3660172e-03, -3.3690315e-03, -8.9105561e-05,\n",
       "        3.5553388e-04, -3.9886660e-03, -5.0855469e-04, -2.9322358e-03,\n",
       "        4.0281024e-03, -4.4495729e-03,  4.0955599e-03, -3.0797245e-03,\n",
       "       -1.9338024e-04, -9.9289254e-04,  2.6498062e-03, -4.5212391e-03,\n",
       "        1.1242189e-04,  6.3340936e-04,  4.8658107e-03, -2.3896410e-03,\n",
       "        6.4127991e-04, -2.6959321e-03,  1.6484503e-03, -4.1903909e-03],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model['first']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sentence(filepath):  # 问题抽取\n",
    "    final_list = []\n",
    "    for line in open(filepath, 'r'):\n",
    "        line_list = line.strip().split('\\n')\n",
    "        final_list += line_list[1:-1]\n",
    "    return final_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#open('Users/xiaojing/Documents/Jingxiao/Applied_Data_Science/NLP_cmpt/train_test/files/text/100.txt', 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(90244, 328380)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#class MySentences(object):\n",
    "#    def __init__(self, dirname):\n",
    "#        self.dirname = dirname\n",
    "# \n",
    "#    def __iter__(self):\n",
    "#        for fname in os.listdir(self.dirname):\n",
    "#            for line in open(os.path.join(self.dirname, fname)):\n",
    "#                yield line.split()\n",
    "\n",
    "#train_file = open('/train_test/file/text/1001.txt','r')\n",
    "#sentences = MySentences(train_file) # a memory-friendly iterator\n",
    "sentences = open('1001-Copy1.txt','r').readlines()\n",
    "model = gensim.models.Word2Vec(sentences)\n",
    "model.train(sentences, total_examples=len(sentences), epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.similar_by_word('network')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-10-25 18:49:42,609 : INFO : Done reading data file\n",
      "2018-10-25 18:49:42,610 : INFO : collecting all words and their counts\n",
      "2018-10-25 18:49:42,611 : WARNING : Each 'sentences' item should be a list of words (usually unicode strings). First item here is instead plain <class 'str'>.\n",
      "2018-10-25 18:49:42,612 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2018-10-25 18:49:42,617 : INFO : PROGRESS: at sentence #10000, processed 10000 words, keeping 76 word types\n",
      "2018-10-25 18:49:42,621 : INFO : PROGRESS: at sentence #20000, processed 20000 words, keeping 79 word types\n",
      "2018-10-25 18:49:42,629 : INFO : PROGRESS: at sentence #30000, processed 30000 words, keeping 81 word types\n",
      "2018-10-25 18:49:42,630 : INFO : collected 81 word types from a corpus of 30763 raw words and 30763 sentences\n",
      "2018-10-25 18:49:42,631 : INFO : Loading a fresh vocabulary\n",
      "2018-10-25 18:49:42,632 : INFO : effective_min_count=5 retains 74 unique words (91% of original 81, drops 7)\n",
      "2018-10-25 18:49:42,633 : INFO : effective_min_count=5 leaves 30751 word corpus (99% of original 30763, drops 12)\n",
      "2018-10-25 18:49:42,634 : INFO : deleting the raw counts dictionary of 81 items\n",
      "2018-10-25 18:49:42,639 : INFO : sample=0.001 downsamples 49 most-common words\n",
      "2018-10-25 18:49:42,641 : INFO : downsampling leaves estimated 8270 word corpus (26.9% of prior 30751)\n",
      "2018-10-25 18:49:42,644 : INFO : estimated required memory for 74 words and 60 dimensions: 72520 bytes\n",
      "2018-10-25 18:49:42,645 : INFO : resetting layer weights\n",
      "2018-10-25 18:49:42,651 : INFO : training model with 3 workers on 74 vocabulary and 60 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2018-10-25 18:49:42,693 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-25 18:49:42,695 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-25 18:49:42,696 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-25 18:49:42,696 : INFO : EPOCH - 1 : training on 30763 raw words (8239 effective words) took 0.0s, 247007 effective words/s\n",
      "2018-10-25 18:49:42,730 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-25 18:49:42,733 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-25 18:49:42,734 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-25 18:49:42,734 : INFO : EPOCH - 2 : training on 30763 raw words (8136 effective words) took 0.0s, 268546 effective words/s\n",
      "2018-10-25 18:49:42,769 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-25 18:49:42,771 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-25 18:49:42,772 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-25 18:49:42,772 : INFO : EPOCH - 3 : training on 30763 raw words (8371 effective words) took 0.0s, 291672 effective words/s\n",
      "2018-10-25 18:49:42,809 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-25 18:49:42,810 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-25 18:49:42,814 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-25 18:49:42,814 : INFO : EPOCH - 4 : training on 30763 raw words (8325 effective words) took 0.0s, 279260 effective words/s\n",
      "2018-10-25 18:49:42,846 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-25 18:49:42,848 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-25 18:49:42,850 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-25 18:49:42,851 : INFO : EPOCH - 5 : training on 30763 raw words (8271 effective words) took 0.0s, 289764 effective words/s\n",
      "2018-10-25 18:49:42,851 : INFO : training on a 153815 raw words (41342 effective words) took 0.2s, 208498 effective words/s\n",
      "2018-10-25 18:49:42,852 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-10-25 18:49:42,852 : WARNING : Effective 'alpha' higher than previous training cycles\n",
      "2018-10-25 18:49:42,853 : INFO : training model with 3 workers on 74 vocabulary and 60 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2018-10-25 18:49:42,891 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-25 18:49:42,892 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-25 18:49:42,893 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-25 18:49:42,894 : INFO : EPOCH - 1 : training on 30763 raw words (8258 effective words) took 0.0s, 249379 effective words/s\n",
      "2018-10-25 18:49:42,928 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-25 18:49:42,929 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-25 18:49:42,932 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-25 18:49:42,932 : INFO : EPOCH - 2 : training on 30763 raw words (8270 effective words) took 0.0s, 265217 effective words/s\n",
      "2018-10-25 18:49:42,966 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-25 18:49:42,967 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-25 18:49:42,968 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-25 18:49:42,968 : INFO : EPOCH - 3 : training on 30763 raw words (8298 effective words) took 0.0s, 293001 effective words/s\n",
      "2018-10-25 18:49:43,002 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-25 18:49:43,004 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-25 18:49:43,006 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-25 18:49:43,007 : INFO : EPOCH - 4 : training on 30763 raw words (8276 effective words) took 0.0s, 268477 effective words/s\n",
      "2018-10-25 18:49:43,044 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-25 18:49:43,045 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-25 18:49:43,048 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-25 18:49:43,049 : INFO : EPOCH - 5 : training on 30763 raw words (8200 effective words) took 0.0s, 245291 effective words/s\n",
      "2018-10-25 18:49:43,092 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-25 18:49:43,094 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-25 18:49:43,098 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-25 18:49:43,099 : INFO : EPOCH - 6 : training on 30763 raw words (8218 effective words) took 0.0s, 188543 effective words/s\n",
      "2018-10-25 18:49:43,145 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-25 18:49:43,146 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-25 18:49:43,148 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-25 18:49:43,149 : INFO : EPOCH - 7 : training on 30763 raw words (8378 effective words) took 0.0s, 212106 effective words/s\n",
      "2018-10-25 18:49:43,182 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-25 18:49:43,183 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-25 18:49:43,186 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-25 18:49:43,187 : INFO : EPOCH - 8 : training on 30763 raw words (8384 effective words) took 0.0s, 273587 effective words/s\n",
      "2018-10-25 18:49:43,221 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-25 18:49:43,222 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-25 18:49:43,224 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-25 18:49:43,225 : INFO : EPOCH - 9 : training on 30763 raw words (8207 effective words) took 0.0s, 335191 effective words/s\n",
      "2018-10-25 18:49:43,269 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-25 18:49:43,271 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-25 18:49:43,273 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-25 18:49:43,273 : INFO : EPOCH - 10 : training on 30763 raw words (8230 effective words) took 0.0s, 202334 effective words/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-10-25 18:49:43,275 : INFO : training on a 307630 raw words (82719 effective words) took 0.4s, 196545 effective words/s\n",
      "2018-10-25 18:49:43,276 : INFO : saving Word2Vec object under ./data/atec_nlp_sim_train.w2v, separately None\n",
      "2018-10-25 18:49:43,279 : INFO : not storing attribute vectors_norm\n",
      "2018-10-25 18:49:43,281 : INFO : not storing attribute cum_table\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data/atec_nlp_sim_train.w2v'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/gensim/utils.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, fname_or_handle, separately, sep_limit, ignore, pickle_protocol)\u001b[0m\n\u001b[1;32m    681\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 682\u001b[0;31m             \u001b[0m_pickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname_or_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    683\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"saved %s object\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: file must have a 'write' attribute",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-81-ac3c77dcde6d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWord2Vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./data/atec_nlp_sim_train.w2v\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1248\u001b[0m         \u001b[0;31m# don't bother storing the cached normalized vectors, recalculable table\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1249\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ignore'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ignore'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'vectors_norm'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'cum_table'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1250\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWord2Vec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1252\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_latest_training_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, fname_or_handle, **kwargs)\u001b[0m\n\u001b[1;32m    619\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    620\u001b[0m         \"\"\"\n\u001b[0;32m--> 621\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseAny2VecModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname_or_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    622\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    623\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/gensim/utils.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, fname_or_handle, separately, sep_limit, ignore, pickle_protocol)\u001b[0m\n\u001b[1;32m    683\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"saved %s object\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    684\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# `fname_or_handle` does not have write attribute\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 685\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_smart_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname_or_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseparately\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep_limit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    686\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/gensim/utils.py\u001b[0m in \u001b[0;36m_smart_save\u001b[0;34m(self, fname, separately, sep_limit, ignore, pickle_protocol)\u001b[0m\n\u001b[1;32m    539\u001b[0m                                        compress, subname)\n\u001b[1;32m    540\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mpickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0;31m# restore attribs handled specially\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/gensim/utils.py\u001b[0m in \u001b[0;36mpickle\u001b[0;34m(obj, fname, protocol)\u001b[0m\n\u001b[1;32m   1338\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1339\u001b[0m     \"\"\"\n\u001b[0;32m-> 1340\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0msmart_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfout\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# 'b' for binary, needed on Windows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1341\u001b[0m         \u001b[0m_pickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/smart_open/smart_open_lib.py\u001b[0m in \u001b[0;36msmart_open\u001b[0;34m(uri, mode, **kw)\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mode should be a string'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m     \u001b[0mfobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_shortcut_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/smart_open/smart_open_lib.py\u001b[0m in \u001b[0;36m_shortcut_open\u001b[0;34m(uri, mode, **kw)\u001b[0m\n\u001b[1;32m    299\u001b[0m     \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPY3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 301\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_uri\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muri_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffering\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuffering\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mopen_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    302\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mopen_kwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_uri\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muri_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffering\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuffering\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/atec_nlp_sim_train.w2v'"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# @Author  : Junru_Lu\n",
    "# @File    : train.py\n",
    "# @Software: PyCharm\n",
    "# @Environment : Python 3.6+\n",
    "# @Reference : https://github.com/likejazz/Siamese-LSTM\n",
    "\n",
    "# 基础包\n",
    "import gensim\n",
    "import logging\n",
    "\n",
    "'''\n",
    "本配置文件用于根据新语料训练词向量\n",
    "'''\n",
    "\n",
    "# ------------------预加载------------------ #\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "\n",
    "# -----------------基础函数------------------ #\n",
    "\n",
    "def extract_questions(filepath):  # 问题抽取\n",
    "    final_list = []\n",
    "    for line in open(filepath, 'r'):\n",
    "        line_list = line.strip()\n",
    "        final_list += line_list[1:-1]\n",
    "    return final_list\n",
    "\n",
    "\n",
    "# -----------------主函数----------------- #\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    documents = list(extract_questions(\"1001-Copy1.txt\"))  # 问题list\n",
    "    logging.info(\"Done reading data file\")\n",
    "    model = gensim.models.Word2Vec(documents, size=60)\n",
    "    model.train(documents, total_examples=len(documents), epochs=10)\n",
    "    model.save(\"./train-test/data/atec_nlp_sim_train.w2v\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
